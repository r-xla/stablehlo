<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>StableHLO Specification • stablehlo</title><!-- mathjax math --><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    chtml: {
      fontURL: "https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/output/chtml/fonts/woff-v2"
    }
  };
</script><script src="lightswitch.js"></script><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="StableHLO Specification"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">stablehlo</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/r-xla/stablehlo/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch"><li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>StableHLO Specification</h1>
      <small class="dont-index">Source: <a href="https://github.com/r-xla/stablehlo/blob/feat-convert/SPEC.md" class="external-link"><code>SPEC.md</code></a></small>
    </div>

<div id="stablehlo-specification" class="section level1">

<p>StableHLO is an operation set for high-level operations (HLO) in machine learning (ML) models. StableHLO works as a portability layer between different ML frameworks and ML compilers: ML frameworks that produce StableHLO programs are compatible with ML compilers that consume StableHLO programs.</p>
<p>Our goal is to simplify and accelerate ML development by creating more interoperability between various ML frameworks (such as TensorFlow, JAX and PyTorch) and ML compilers (such as XLA and IREE). Towards that end, this document provides a specification for the StableHLO programming language.</p>
<p>This specification contains three major sections. First, the <a href="#programs">Programs</a> section describes the structure of StableHLO programs which consist of StableHLO functions which themselves consist of StableHLO ops. Within that structure, the <a href="#ops">Ops</a> section specifies the semantics of individual ops. The <a href="#execution">Execution</a> section provides semantics for all these ops executing together within a program. Finally, the <a href="#notation">Notation</a> section discusses the notation used throughout the specification.</p>
<p>To view the spec from a previous release of StableHLO, open the repo at the <a href="https://github.com/openxla/stablehlo/tags" class="external-link">tagged release</a> of interest. For example, the <a href="https://github.com/openxla/stablehlo/blob/v0.19.0/docs/spec.md" class="external-link">StableHLO v0.19.0 Spec</a>. To view changes that occurred at each minor version bump of StableHLO, refer to the version log in <a href="https://github.com/openxla/stablehlo/blob/main/stablehlo/dialect/VhloDialect.td" class="external-link">VhloDialect.td</a>.</p>
<div class="section level2">
<h2 id="programs">Programs<a class="anchor" aria-label="anchor" href="#programs"></a></h2>
<pre class="ebnf"><code>Program ::= {Func}</code></pre>
<p><strong>StableHLO programs</strong> consist of an arbitrary number of StableHLO functions. Below is an example program with a function <code>@main</code> which has 3 inputs (<code>%image</code>, <code>%weights</code> and <code>%bias</code>) and 1 output. The body of the function has 6 ops.</p>
<pre class="mlir"><code>func.func @main(
  %image: tensor&lt;28x28xf32&gt;,
  %weights: tensor&lt;784x10xf32&gt;,
  %bias: tensor&lt;1x10xf32&gt;
) -&gt; tensor&lt;1x10xf32&gt; {
  %0 = "stablehlo.reshape"(%image) : (tensor&lt;28x28xf32&gt;) -&gt; tensor&lt;1x784xf32&gt;
  %1 = "stablehlo.dot"(%0, %weights) : (tensor&lt;1x784xf32&gt;, tensor&lt;784x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;
  %2 = "stablehlo.add"(%1, %bias) : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;
  %3 = "stablehlo.constant"() {value = dense&lt;0.0&gt; : tensor&lt;1x10xf32&gt;} : () -&gt; tensor&lt;1x10xf32&gt;
  %4 = "stablehlo.maximum"(%2, %3) : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;
  "func.return"(%4): (tensor&lt;1x10xf32&gt;) -&gt; ()
}</code></pre>
<div class="section level3">
<h3 id="functions">Functions<a class="anchor" aria-label="anchor" href="#functions"></a></h3>
<pre class="ebnf"><code>Func        ::= 'func' '.' 'func' FuncId FuncInputs FuncOutputs '{' FuncBody '}'
FuncInputs  ::= '(' [FuncInput {',' FuncInput}] `)`
FuncInput   ::= ValueId ':' ValueType
FuncOutputs ::= ['-&gt;' FuncOutput, {',' FuncOutput}]
FuncOutput  ::= ValueType
FuncBody    ::= {Op}</code></pre>
<p><strong>StableHLO functions</strong> (which are also called <strong>named functions</strong>) have an identifier, inputs/outputs and a body. In the future, we are planning to introduce additional metadata for functions to achieve better compatibility with HLO (<a href="https://github.com/openxla/stablehlo/issues/425" class="external-link">#425</a>, <a href="https://github.com/openxla/stablehlo/issues/626" class="external-link">#626</a>, <a href="https://github.com/openxla/stablehlo/issues/740" class="external-link">#740</a>, <a href="https://github.com/openxla/stablehlo/issues/744" class="external-link">#744</a>).</p>
</div>
<div class="section level3">
<h3 id="identifiers">Identifiers<a class="anchor" aria-label="anchor" href="#identifiers"></a></h3>
<pre class="ebnf"><code>FuncId  ::= '@' letter {letter | digit}
ValueId ::= '%' digit {digit}
          | '%' letter {letter | digit}
letter  ::= 'a' | ... | 'z' | 'A' | ... | 'Z' | '_'
digit   ::= '0' | ... | '9'</code></pre>
<p><strong>StableHLO identifiers</strong> are similar to identifiers in many programming languages, with two peculiarities: 1) all identifiers have sigils which distinguish different kinds of identifiers, 2) value identifiers can be completely numeric to simplify generation of StableHLO programs.</p>
</div>
<div class="section level3">
<h3 id="types">Types<a class="anchor" aria-label="anchor" href="#types"></a></h3>
<pre class="ebnf"><code>Type         ::= ValueType | NonValueType
ValueType    ::= TensorType | QuantizedTensorType | TokenType | TupleType
NonValueType ::= TensorElementType | QuantizedTensorElementType | FunctionType | StringType</code></pre>
<p><strong>StableHLO types</strong> are categorized into <strong>value types</strong> (which are also called <strong>first-class types</strong>) which represent StableHLO values and <strong>non-value types</strong> which describe other program elements. StableHLO types are similar to types in many programming languages, with the main peculiarity being StableHLO’s domain-specific nature which results in some unusual outcomes (e.g. scalar types are not value types).</p>
<pre class="ebnf"><code>TensorType ::= 'tensor' '&lt;' Shape TensorElementType '&gt;'
Shape ::= {DimensionSize 'x'}
DimensionSize ::= digit {digit} | '?'</code></pre>
<p><strong>Tensor types</strong> represent tensors, i.e. multidimensional arrays. They have a <strong>shape</strong> and an <strong>element type</strong>, where a shape represents non-negative or unknown <strong>dimension sizes</strong> in the ascending order of the corresponding <strong>dimensions</strong> (which are also called <strong>axes</strong>) numbered from <code>0</code> to <code>R-1</code>. The number of dimensions <code>R</code> is called <strong>rank</strong>. For example, <code>tensor&lt;2x3xf32&gt;</code> is a tensor type with shape <code>2x3</code> and element type <code>f32</code>. It has two dimensions (or, in other words, two axes) - 0th dimension and 1st dimension - whose sizes are 2 and 3. Its rank is 2.</p>
<p>Shapes can be partially or completely unknown (dynamic), e.g. <code>tensor&lt;?x2xf64&gt;</code> is partially unknown and <code>tensor&lt;?x?xf64&gt;</code> is completely unknown. Dynamic dimension sizes are represented using a <code>?</code>. Shapes cannot be unranked.</p>
<p>In the future, we are planning to explore extending tensor types beyond dimension sizes and element types, for example, to include layouts (<a href="https://github.com/openxla/stablehlo/issues/629" class="external-link">#629</a>) and sparsity (<a href="https://github.com/openxla/stablehlo/issues/1078" class="external-link">#1078</a>).</p>
<pre class="ebnf"><code>QuantizedTensorType ::= 'tensor' '&lt;' Shape QuantizedTensorElementType '&gt;'
QuantizedTensorElementType ::= '!quant.uniform' '&lt;'
                  QuantizationStorageType
                  ['&lt;' QuantizationStorageMin ':' QuantizationStorageMax '&gt;']
                  ':' QuantizationExpressedType
                  [':' QuantizationDimension]
                  ',' QuantizationParameters '&gt;'
QuantizationStorageType ::= IntegerType
QuantizationStorageMin ::= IntegerLiteral
QuantizationStorageMax ::= IntegerLiteral
QuantizationExpressedType ::= FloatType
QuantizationDimension ::= IntegerLiteral
QuantizationParameters ::= QuantizationParameter
                         | '{' QuantizationParameter {',' QuantizationParameter} '}'
QuantizationParameter ::= QuantizationScale [':' QuantizationZeroPoint]
QuantizationScale ::= FloatLiteral
QuantizationZeroPoint ::= IntegerLiteral</code></pre>
<table class="table"><colgroup><col width="26%"><col width="45%"><col width="28%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>storage_type</code></td>
<td>integer type</td>
<td>(C1-C3), (C8)</td>
</tr><tr class="even"><td><code>storage_min</code></td>
<td>integer constant</td>
<td>(C1), (C3), (C7)</td>
</tr><tr class="odd"><td><code>storage_max</code></td>
<td>integer constant</td>
<td>(C2), (C3), (C7)</td>
</tr><tr class="even"><td><code>expressed_type</code></td>
<td>floating-point type</td>
<td>(C4)</td>
</tr><tr class="odd"><td><code>quantization_dimension</code></td>
<td>optional integer constant</td>
<td>(C10-C12)</td>
</tr><tr class="even"><td><code>scales</code></td>
<td>variadic number of floating-point constants</td>
<td>(C4-C6), (C9), (C10), (C13)</td>
</tr><tr class="odd"><td><code>zero_points</code></td>
<td>variadic number of integer constants</td>
<td>(C7-C9)</td>
</tr></tbody></table><p><strong>Quantized element types</strong> represent integer values of a <strong>storage type</strong> in the range from <code>storage_min</code> to <code>storage_max</code> (inclusive) that correspond to floating-point values of an <strong>expressed type</strong>. For a given integer value <code>i</code>, the corresponding floating-point value <code>f</code> can be computed as <code>f = (i - zero_point) * scale</code>, where <code>scale</code> and <code>zero_point</code> are called <strong>quantization parameters</strong>. The <code>storage_min</code> and <code>storage_max</code> are optional in the grammar, but have default values of <code>min_value(storage_type)</code> and <code>max_value(storage_type)</code> respectively. Quantized element types have the following constraints:</p>
<ul><li>(C1) <code>type(storage_min) = storage_type</code>.</li>
<li>(C2) <code>type(storage_max) = storage_type</code>.</li>
<li>(C3) <code>min_value(storage_type) &lt;= storage_min &lt; storage_max &lt;= max_value(storage_type)</code>.</li>
<li>(C4) <code>type(scales...) = expressed_type</code>.</li>
<li>(C5) <code>0 &lt; scales</code>.</li>
<li>(C6) <code>is_finite(scales...)</code>.</li>
<li>(C7) <code>storage_min &lt;= zero_points &lt;= storage_max</code>.</li>
<li>(C8) <code>type(zero_points...) = storage_type</code>.</li>
<li>(C9) <code>size(scales) = size(zero_points)</code>.</li>
<li>(C10) If <code>is_empty(quantization_dimension)</code>, then <code>size(scales) = 1</code>.</li>
<li>(C11) <code>0 &lt;= quantization_dimension</code>.</li>
</ul><p>At the moment, <code>QuantizationScale</code> is a floating-point constant, but there is strong interest in integer-based scales, represented with multipliers and shifts. We are planning to explore this in the near future (<a href="https://github.com/openxla/stablehlo/issues/1404" class="external-link">#1404</a>).</p>
<p>There is an ongoing discussion on the semantics of <code>QuantizationZeroPoint</code>, including the type, the values and whether there can be just one or potentially multiple zero points in a quantized tensor type. Based on the results of this discussion, the specification around zero points may change in the future (<a href="https://github.com/openxla/stablehlo/issues/1405" class="external-link">#1405</a>).</p>
<p>Another ongoing discussion involves the semantics of <code>QuantizationStorageMin</code> and <code>QuantizationStorageMax</code> to determine whether any constraints should be imposed on these values and on the values of quantized tensors (<a href="https://github.com/openxla/stablehlo/issues/1406" class="external-link">#1406</a>).</p>
<p>Finally, we are planning to explore representing unknown scales and zero points, similarly to how we are planning to explore representing unknown dimension sizes (<a href="https://github.com/openxla/stablehlo/issues/1407" class="external-link">#1407</a>).</p>
<p><strong>Quantized tensor types</strong> represent tensors with quantized elements. These tensors are exactly the same as regular tensors, except that their elements have quantized element types, instead of regular element types.</p>
<p>In quantized tensors, quantization can be <strong>per-tensor</strong>, meaning, having one <code>scale</code> and <code>zero_point</code> for the entire tensor or can be <strong>per-axis</strong>, meaning, having multiple <code>scales</code> and <code>zero_points</code>, one pair per slice of a particular dimension <code>quantization_dimension</code>. More formally, in a tensor <code>t</code> with per-axis quantization, there are <code>dim(t, quantization_dimension)</code> slices of the <code>quantization_dimension</code>: <code>t[:, ..., 0, ..., :], t[:, ..., 1, ..., :]</code>, etc. All elements in the <code>i</code>th slice use <code>scales[i]</code> and <code>zero_points[i]</code> as their quantization parameters. Quantized tensor types have the following constraints:</p>
<ul><li>For per-tensor quantization:
<ul><li>No additional constraints.</li>
</ul></li>
<li>For per-axis quantization:
<ul><li>(C12) <code>quantization_dimension &lt; rank(self)</code>.</li>
<li>(C13) <code>dim(self, quantization_dimension) = size(scales)</code>.</li>
</ul></li>
</ul><pre class="ebnf"><code>TokenType ::= 'token'</code></pre>
<p><strong>Token types</strong> represent tokens, i.e. opaque values produced and consumed by some operations. Tokens are used for imposing execution order on operations as described in the <a href="#execution">Execution</a> section.</p>
<pre class="ebnf"><code>TupleType ::= 'tuple' '&lt;' TupleElementTypes '&gt;'
TupleElementTypes ::= [ValueType {',' ValueType}]</code></pre>
<p><strong>Tuple types</strong> represent tuples, i.e. heterogeneous lists. Tuples are a legacy feature which only exists for compatibility with HLO. In HLO, tuples are used to represent variadic inputs and outputs. In StableHLO, variadic inputs and outputs are supported natively, and the only use of tuples in StableHLO is to comprehensively represent HLO ABI where e.g. <code>T</code>, <code>tuple&lt;T&gt;</code> and <code>tuple&lt;tuple&lt;T&gt;&gt;</code> may be materially different depending on a particular implementation. In the future, we are planning to make changes to HLO ABI which may allow us to remove tuple types from StableHLO (<a href="https://github.com/openxla/stablehlo/issues/598" class="external-link">#598</a>).</p>
<pre class="ebnf"><code>TensorElementType ::= BooleanType | IntegerType | FloatType | ComplexType
BooleanType ::= 'i1'
IntegerType ::= SignedIntegerType | UnsignedIntegerType
SignedIntegerType ::= 'si2' | 'si4' | 'si8' | 'si16' | 'si32' | 'si64'
UnsignedIntegerType ::= 'ui2' | 'ui4' | 'ui8' | 'ui16' | 'ui32' | 'ui64'
FloatType ::= 'f4E2M1FN' | 'f6E2M3FN' | 'f6E3M2FN' | 'f8E3M4' | 'f8E4M3'
            | 'f8E4M3FN' | 'f8E4M3FNUZ' | 'f8E4M3B11FNUZ' | 'f8E5M2'
            | 'f8E5M2FNUZ' | 'f8E8M0FNU' | 'bf16' | 'f16' | 'f32' | 'f64'
TensorFloat32 ::= 'tf32'
ComplexType ::= 'complex' '&lt;' ComplexElementType '&gt;'
ComplexElementType ::= 'f32' | 'f64'</code></pre>
<p><strong>Element types</strong> represent elements of tensor types. Unlike in many programming languages, these types are not first class in StableHLO. This means that StableHLO programs cannot directly represent values of these types (as a result, it is idiomatic to represent scalar values of type <code>T</code> with 0-dimensional tensor values of type <code>tensor&lt;T&gt;</code>).</p>
<ul><li>
<strong>Boolean type</strong> represents boolean values <code>true</code> and <code>false</code>.</li>
<li>
<strong>Integer types</strong> can be either signed (<code>si</code>) or unsigned (<code>ui</code>) and have one of the supported bit widths (<code>2</code>, <code>4</code>, <code>8</code>, <code>16</code>, <code>32</code> or <code>64</code>). Signed <code>siN</code> types represent integer values from <code>-2^(N-1)</code> to <code>2^(N-1)-1</code> inclusive, and unsigned <code>uiN</code> types represent integer values from <code>0</code> to <code>2^N-1</code> inclusive.</li>
<li>
<strong>Floating-point types</strong> can be one of the following:
<ul><li>
<code>f8E3M4</code>, <code>f8E4M3</code> and <code>f8E5M2</code> 8-bit floating point numbers following IEEE-754 conventions.</li>
<li>
<code>f8E4M3FN</code> and <code>f8E5M2</code> types corresponding to respectively the <code>E4M3</code> and <code>E5M2</code> encodings of the FP8 format described in <a href="https://arxiv.org/abs/2209.05433" class="external-link">FP8 Formats for Deep Learning</a>.</li>
<li>
<code>f8E4M3FNUZ</code> and <code>f8E5M2FNUZ</code> types corresponding to the <code>E4M3</code> and <code>E5M2</code> encodings of the FP8 formats described in <a href="https://arxiv.org/abs/2206.02915" class="external-link">8-bit Numerical Formats for Deep Neural Networks</a>.</li>
<li>
<code>f8E4M3B11FNUZ</code> type corresponding to the <code>E4M3</code> encoding of the FP8 formats described in <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf" class="external-link">Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks</a>.</li>
<li>
<code>bf16</code> type corresponding to the <code>bfloat16</code> format described in <a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus" class="external-link">BFloat16: The secret to high performance on Cloud TPUs</a>.</li>
<li>
<code>f16</code>, <code>f32</code> and <code>f64</code> types corresponding to respectively <code>binary16</code> (“half precision”), <code>binary32</code> (“single precision”) and <code>binary64</code> (“double precision”) formats described in <a href="https://ieeexplore.ieee.org/document/8766229" class="external-link">the IEEE 754 standard</a>.</li>
<li>
<code>tf32</code> type corresponds to the <a href="https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/" class="external-link">TensorFloat32 format</a> and has limited support in StableHLO.</li>
<li>
<code>f4E2M1FN</code>, <code>f6E2M3FN</code>, <code>f6E3M2FN</code> and <code>f8E8M0FNU</code> MX (microscaling) types described in <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf" class="external-link">OCP Microscaling Formats Specification</a>.</li>
</ul></li>
<li>
<strong>Complex types</strong> represent complex values that have a <strong>real part</strong> and an <strong>imaginary part</strong> of the same <strong>element type</strong>. Supported complex types are <code>complex&lt;f32&gt;</code> (both parts are of type <code>f32</code>) and <code>complex&lt;f64&gt;</code> (both parts are of type <code>f64</code>).</li>
</ul><pre class="ebnf"><code>FunctionType ::= '(' InputTypes ')' '-&gt;' '(' OutputTypes ')'
InputTypes ::= [ValueType {',' ValueType}]
OutputTypes ::= [ValueType {',' ValueType}]</code></pre>
<p><strong>Function types</strong> represent both named and anonymous functions. They have input types (the list of types on the left-hand side of <code>-&gt;</code>) and output types (the list of types on the right-hand side of <code>-&gt;</code>). In many programming languages, function types are first class, but not in StableHLO.</p>
<pre class="ebnf"><code>StringType ::= 'string'</code></pre>
<p><strong>String type</strong> represents sequences of bytes. Unlike in many programming languages, string type is not first class in StableHLO and is only used to specify static metadata for program elements.</p>
</div>
<div class="section level3">
<h3 id="operations">Operations<a class="anchor" aria-label="anchor" href="#operations"></a></h3>
<p><strong>StableHLO operations</strong> (which are also called <strong>ops</strong>) represent a closed set of high-level operations in machine learning models. As discussed above, StableHLO syntax is heavily inspired by MLIR, which is not necessarily the most ergonomic alternative, but is arguably the best fit for StableHLO’s goal of creating more interoperability between ML frameworks and ML compilers.</p>
<pre class="ebnf"><code>Op            ::= [OpOutputs] OpName OpInputs ':' OpSignature
OpName        ::= '"' 'stablehlo' '.' OpMnemonic '"'
OpMnemonic    ::= 'abs' | 'add' | ...</code></pre>
<p><strong>StableHLO operations</strong> (which are also called <strong>ops</strong>) have a name, inputs/outputs and a signature. The name consists of the <code>stablehlo.</code> prefix and a <strong>mnemonic</strong> which uniquely identifies one of the supported ops. See below for a comprehensive list of all supported ops.</p>
<pre class="ebnf"><code>OpInputs        ::= OpInputValues OpInputFuncs OpInputAttrs
OpInputValues   ::= '(' [OpInputValue {',' OpInputValue}] ')'
OpInputValue    ::= ValueId
OpInputFuncs    ::= ['(' OpInputFunc {',' OpInputFunc} ')']
OpInputAttrs    ::= ['{' OpInputAttr {',' OpInputAttr} '}']
OpOutputs       ::= [OpOutput {',' OpOutput} '=']
OpOutput        ::= ValueId</code></pre>
<p>Ops consume <strong>inputs</strong> and produce <strong>outputs</strong>. Inputs are categorized into input values (computed during execution), input functions (provided statically, because in StableHLO functions are not first-class values) and input attributes (also provided statically). The kind of inputs and outputs consumed and produced by an op depends on its mnemonic. For example, the <code>add</code> op consumes 2 input values and produces 1 output value. In comparison, the <code>select_and_scatter</code> op consumes 3 input values, 2 input functions and 3 input attributes.</p>
<pre class="ebnf"><code>OpInputFunc ::= '{' Unused FuncInputs ':' FuncBody '}'
Unused      ::= '^' digit {digit}
              | '^' letter {letter | digit}</code></pre>
<p><strong>Input functions</strong> (which are also called <strong>anonymous functions</strong>) are very similar to named functions except that: 1) they don’t have an identifier (hence the name “anonymous”), 2) they don’t declare output types (output types are inferred from the <code>return</code> op within the function).</p>
<p>The syntax for input functions includes a currently unused part (see the <code>Unused</code> production above) which is there for compatibility with MLIR. In MLIR, there is a more general concept of “regions” which can have multiple “blocks” of ops connected together via jump ops. These blocks have ids which correspond to the <code>Unused</code> production, so that they can be distinguished from each other. StableHLO doesn’t have jump ops, so the corresponding part of MLIR syntax is unused (but is still there).</p>
<pre class="ebnf"><code>OpInputAttr      ::= OpInputAttrName '=' OpInputAttrValue
OpInputAttrName  ::= letter {letter | digit}
OpInputAttrValue ::= Constant</code></pre>
<p><strong>Input attributes</strong> have a name and a value which is one of the supported constants. They are the primary way to specify static metadata for program elements. For example, the <code>concatenate</code> op uses the attribute <code>dimension</code> to specify the dimension along which its input values are concatenated. Similarly, the <code>slice</code> op uses multiple attributes like <code>start_indices</code> and <code>limit_indices</code> to specify the bounds that are used to slice the input value.</p>
<p>At the moment, StableHLO programs in the wild sometimes contain attributes which are not described in this document. In the future, we are planning to either absorb these attributes into the StableHLO opset or prohibit them from appearing in StableHLO programs. In the meanwhile, here is the list of these attributes:</p>
<ul><li>
<code>layout</code> (<a href="https://github.com/openxla/stablehlo/issues/629" class="external-link">#629</a>).</li>
<li>
<code>mhlo.frontend_attributes</code> (<a href="https://github.com/openxla/stablehlo/issues/628" class="external-link">#628</a>).</li>
<li>
<code>mhlo.sharding</code> (<a href="https://github.com/openxla/stablehlo/issues/619" class="external-link">#619</a>).</li>
<li>
<code>output_operand_aliases</code> (<a href="https://github.com/openxla/stablehlo/issues/740" class="external-link">#740</a>).</li>
<li>Location metadata (<a href="https://github.com/openxla/stablehlo/issues/594" class="external-link">#594</a>).</li>
</ul><pre class="ebnf"><code>OpSignature ::= '(' [ValueType {',' ValueType}] ')' '-&gt;' '(' [ValueType {',' ValueType}] ')'</code></pre>
<p><strong>Op signature</strong> consists of the types of all input values (the list of types on the left-hand side of <code>-&gt;</code>) and the types of all output values (the list of types on the right-hand side of <code>-&gt;</code>). Strictly speaking, input types are redundant, and output types are almost always redundant as well (because for most StableHLO ops, output types can be inferred from inputs). Nonetheless, op signature is deliberately part of StableHLO syntax for compatibility with MLIR.</p>
<p>Below is an example op whose mnemonic is <code>select_and_scatter</code>. It consumes 3 input values (<code>%operand</code>, <code>%source</code> and <code>%init_value</code>), 2 input functions and 3 input attributes (<code>window_dimensions</code>, <code>window_strides</code> and <code>padding</code>). Note how the signature of the op only includes the types of its input values (but not the types of input functions and attributes which are provided inline).</p>
<pre class="mlir"><code>%result = "stablehlo.select_and_scatter"(%operand, %source, %init_value) ({
  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):
    %0 = "stablehlo.compare"(%arg0, %arg1) {
      comparison_direction = #stablehlo&lt;comparison_direction GE&gt;
    } : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i1&gt;
    "stablehlo.return"(%0) : (tensor&lt;i1&gt;) -&gt; ()
}, {
  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):
    %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;
    "stablehlo.return"(%0) : (tensor&lt;i32&gt;) -&gt; ()
}) {
  window_dimensions = dense&lt;[3, 1]&gt; : tensor&lt;2xi64&gt;,
  window_strides = dense&lt;[2, 1]&gt; : tensor&lt;2xi64&gt;,
  padding = dense&lt;[[0, 1], [0, 0]]&gt; : tensor&lt;2x2xi64&gt;
} : (tensor&lt;4x2xi32&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;4x2xi32&gt;</code></pre>
</div>
<div class="section level3">
<h3 id="constants">Constants<a class="anchor" aria-label="anchor" href="#constants"></a></h3>
<pre class="ebnf"><code>Constant ::= BooleanConstant
           | IntegerConstant
           | FloatConstant
           | ComplexConstant
           | TensorConstant
           | QuantizedTensorConstant
           | StringConstant
           | EnumConstant</code></pre>
<p><strong>StableHLO constants</strong> have a literal and a type which together represent a StableHLO value. Generally, the type is part of the constant syntax, except when it’s unambiguous (e.g. a boolean constant unambiguously has type <code>i1</code>, whereas an integer constant can have multiple possible types).</p>
<pre class="ebnf"><code>BooleanConstant ::= BooleanLiteral
BooleanLiteral  ::= 'true' | 'false'</code></pre>
<p><strong>Boolean constants</strong> represent boolean values <code>true</code> and <code>false</code>. Boolean constants have type <code>i1</code>.</p>
<pre class="ebnf"><code>IntegerConstant   ::= IntegerLiteral ':' IntegerType
IntegerLiteral    ::= ['-' | '+'] DecimalDigits
                    | ['-' | '+'] '0x' HexadecimalDigits
DecimalDigits     ::= decimalDigit {decimalDigit}
HexadecimalDigits ::= hexadecimalDigit {hexadecimalDigit}
decimalDigit      ::= '0' | ... | '9'
hexadecimalDigit  ::= decimalDigit | 'a' | ... | 'f' | 'A' | ... | 'F'</code></pre>
<p><strong>Integer constants</strong> represent integer values via strings that use decimal or hexadecimal notation. Other bases, e.g. binary or octal, are not supported. Integer constants have the following constraints:</p>
<ul><li>(C1) <code>is_wellformed(integer_literal, integer_type)</code>.</li>
</ul><pre class="ebnf"><code>FloatConstant  ::= FloatLiteral ':' FloatType
FloatLiteral   ::= SignPart IntegerPart FractionalPart ScientificPart
                 | '0x' [HexadecimalDigits]
SignPart       ::= ['-' | '+']
IntegerPart    ::= DecimalDigits
FractionalPart ::= ['.' [DecimalDigits]]
ScientificPart ::= [('e' | 'E') ['-' | '+'] DecimalDigits]</code></pre>
<p><strong>Floating-point constants</strong> represent floating-point values via strings that use decimal or scientific notation. Additionally, hexadecimal notation can be used to directly specify the underlying bits in the floating-point format of the corresponding type. Floating-point constants have the following constraints:</p>
<ul><li>(C1) If non-hexadecimal notation is used, <code>is_wellformed(float_literal, float_type)</code>.</li>
<li>(C2) If hexadecimal notation is used, <code>size(hexadecimal_digits) = num_bits(float_type) / 4</code>.</li>
</ul><pre class="ebnf"><code>ComplexConstant ::= ComplexLiteral ':' ComplexType
ComplexLiteral  ::= '(' RealPart ',' ImaginaryPart ')'
RealPart        ::= FloatLiteral
ImaginaryPart   ::= FloatLiteral</code></pre>
<p><strong>Complex constants</strong> represent complex values using lists of a real part (comes first) and an imaginary part (comes second). For example, <code>(1.0, 0.0) : complex&lt;f32&gt;</code> represents <code>1.0 + 0.0i</code>, and <code>(0.0, 1.0) : complex&lt;f32&gt;</code> represents <code>0.0 + 1.0i</code>. The order in which these parts are then stored in memory is implementation-defined. Complex constants have the following constraints:</p>
<ul><li>(C1) <code>is_wellformed(real_part, complex_element_type(complex_type))</code>.</li>
<li>(C2) <code>is_wellformed(imaginary_part, complex_element_type(complex_type))</code>.</li>
</ul><pre class="ebnf"><code>TensorConstant ::= TensorLiteral ':' TensorType
TensorLiteral  ::= 'dense' '&lt;' (DenseLiteral | ElementLiteral) '&gt;'
DenseLiteral   ::= DenseDimension | DenseElements
DenseDimension ::= '[' [DenseLiteral {',' DenseLiteral}] ']'
DenseElements  ::= [ElementLiteral {',' ElementLiteral}]
ElementLiteral ::= BooleanLiteral | IntegerLiteral | FloatLiteral | ComplexLiteral</code></pre>
<p><strong>Tensor constants</strong> represent tensor values using nested lists specified via NumPy notation. For example, <code>dense&lt;[[1, 2, 3], [4, 5, 6]]&gt; : tensor&lt;2x3xi32&gt;</code> represents a tensor value with the following mapping from indices to elements: <code>{0, 0} =&gt; 1</code>, <code>{0, 1} =&gt; 2</code>, <code>{0, 2} =&gt; 3</code>, <code>{1, 0} =&gt; 4</code>, <code>{1, 1} =&gt; 5</code>, <code>{1, 2} =&gt; 6</code>. The order in which these elements are then stored in memory is implementation-defined. Tensor constants have the following constraints:</p>
<ul><li>(C1) <code>has_syntax(tensor_literal, element_type(tensor_type))</code>, where:
<ul><li>
<code>has_syntax(element_literal: Syntax, element_type: Type) =   is_wellformed(element_literal, type)</code>.</li>
<li>
<code>has_syntax(tensor_literal: List, element_type: Type) =   has_syntax(tensor_literal..., element_type)</code>.</li>
</ul></li>
<li>(C2) <code>has_shape(tensor_literal, shape(tensor_type))</code>, where:
<ul><li>
<code>has_shape(element_literal: Syntax, []) = true</code>.</li>
<li>
<code>has_shape(tensor_literal: List, shape: List) =   size(tensor_literal) = shape[0] and   has_shape(tensor_literal..., shape[1:])</code>.</li>
<li>otherwise, <code>false</code>.</li>
</ul></li>
</ul><pre class="ebnf"><code>QuantizedTensorConstant ::= QuantizedTensorLiteral ':' QuantizedTensorType
QuantizedTensorLiteral  ::= 'dense' '&lt;' (DenseLiteral | ElementLiteral) '&gt;'</code></pre>
<p><strong>Quantized tensor constants</strong> represent quantized tensor values using the same notation as tensor constants, with elements specified as constants of their storage type. Quantized tensor constants have the following constraints:</p>
<ul><li>(C1) <code>has_syntax(quantized_tensor_literal, storage_type(quantized_tensor_type))</code>.</li>
<li>(C2) <code>has_shape(quantized_tensor_literal, shape(quantized_tensor_type))</code>.</li>
</ul><pre class="ebnf"><code>StringConstant  ::= StringLiteral
StringLiteral   ::= '"' {stringCharacter | escapeSequence} '"'
stringCharacter ::= all ASCII characters except '\00', '\01', ... '\1f' and '"'
escapeSequence  ::= '\' ('"' | '\' | 'n' | 't' | (hexadecimalDigit hexadecimalDigit))</code></pre>
<p><strong>String literals</strong> consist of bytes specified using ASCII characters and escape sequences. They are encoding-agnostic, so the interpretation of these bytes is implementation-defined. String literals have type <code>string</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="ops">Ops<a class="anchor" aria-label="anchor" href="#ops"></a></h2>
<div class="section level3">
<h3 id="abs">abs<a class="anchor" aria-label="anchor" href="#abs"></a></h3>
<div class="section level4">
<h4 id="semantics">Semantics<a class="anchor" aria-label="anchor" href="#semantics"></a></h4>
<p>Performs element-wise abs operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For signed integers: integer modulus.</li>
<li>For floats: <code>abs</code> from IEEE-754.</li>
<li>For complex numbers: complex modulus.</li>
<li>For quantized types: <code>dequantize_op_quantize(abs, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs">Inputs<a class="anchor" aria-label="anchor" href="#inputs"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="5%"><col width="9%"><col width="74%"><col width="10%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of signed integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1-C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs">Outputs<a class="anchor" aria-label="anchor" href="#outputs"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="77%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of signed integer or floating-point type or per-tensor quantized tensor</td>
<td>(C1-C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints">Constraints<a class="anchor" aria-label="anchor" href="#constraints"></a></h4>
<ul><li>(C1) <code>shape(result) = shape(operand)</code>.</li>
<li>(C2) <code>baseline_element_type(result)</code> is defined as:
<ul><li>
<code>complex_element_type(element_type(operand))</code> if <code>is_complex(operand)</code>.</li>
<li>
<code>baseline_element_type(operand)</code> otherwise.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a></h4>
<pre class="mlir"><code>// %operand: [-2, 0, 2]
%result = "stablehlo.abs"(%operand) : (tensor&lt;3xi32&gt;) -&gt; tensor&lt;3xi32&gt;
// %result: [2, 0, 2]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/abs.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="add">add<a class="anchor" aria-label="anchor" href="#add"></a></h3>
<div class="section level4">
<h4 id="semantics-1">Semantics<a class="anchor" aria-label="anchor" href="#semantics-1"></a></h4>
<p>Performs element-wise addition of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical OR.</li>
<li>For integers: integer addition.</li>
<li>For floats: <code>addition</code> from IEEE-754.</li>
<li>For complex numbers: complex addition.</li>
<li>For quantized types: <code>dequantize_op_quantize(add, lhs, rhs, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-1">Inputs<a class="anchor" aria-label="anchor" href="#inputs-1"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C6)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C5), (C7)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-1">Outputs<a class="anchor" aria-label="anchor" href="#outputs-1"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C7)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-1">Constraints<a class="anchor" aria-label="anchor" href="#constraints-1"></a></h4>
<ul><li>If the operation uses non-quantized tensors:
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></li>
<li>If the operation uses quantized tensors:
<ul><li>(C2) <code>is_quantized(lhs) and is_quantized(rhs) and is_quantized(result)</code>.</li>
<li>(C3) <code>storage_type(lhs) = storage_type(rhs) = storage_type(result)</code>.</li>
<li>(C4) <code>expressed_type(lhs) = expressed_type(rhs) = expressed_type(result)</code>.</li>
<li>(C5) <code>(is_per_axis_quantized(lhs) or is_per_axis_quantized(rhs)) =   is_per_axis_quantized(result)</code>.</li>
<li>(C6) If <code>is_per_axis_quantized(lhs)</code>, then <code>quantization_dimension(lhs) =   quantization_dimension(result)</code>.</li>
<li>(C7) If <code>is_per_axis_quantized(rhs)</code>, then <code>quantization_dimension(rhs) =   quantization_dimension(result)</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-1">Examples<a class="anchor" aria-label="anchor" href="#examples-1"></a></h4>
<pre class="mlir"><code>// %lhs: [[1, 2], [3, 4]]
// %rhs: [[5, 6], [7, 8]]
%result = "stablehlo.add"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[6, 8], [10, 12]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/add.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="after_all">after_all<a class="anchor" aria-label="anchor" href="#after_all"></a></h3>
<div class="section level4">
<h4 id="semantics-2">Semantics<a class="anchor" aria-label="anchor" href="#semantics-2"></a></h4>
<p>Ensures that the operations producing the <code>inputs</code> are executed before any operations that depend on <code>result</code>. Execution of this operation does nothing, it only exists to establish data dependencies from <code>result</code> to <code>inputs</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-2">Inputs<a class="anchor" aria-label="anchor" href="#inputs-2"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of <code>token</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-2">Outputs<a class="anchor" aria-label="anchor" href="#outputs-2"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td><code>token</code></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="examples-2">Examples<a class="anchor" aria-label="anchor" href="#examples-2"></a></h4>
<pre class="mlir"><code>// %input0: !stablehlo.token
// %input1: !stablehlo.token
%result = "stablehlo.after_all"(%input0, %input1) : (!stablehlo.token, !stablehlo.token) -&gt; !stablehlo.token</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/after_all.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="all_gather">all_gather<a class="anchor" aria-label="anchor" href="#all_gather"></a></h3>
<div class="section level4">
<h4 id="semantics-3">Semantics<a class="anchor" aria-label="anchor" href="#semantics-3"></a></h4>
<p>Within each process group in the StableHLO process grid, concatenates the values of the <code>operands</code> tensors from each process along <code>all_gather_dim</code> and produces <code>results</code> tensors.</p>
<p>The operation splits the StableHLO process grid into <code>process_groups</code> which is defined as follows:</p>
<ul><li>
<code>cross_replica(replica_groups)</code> if <code>channel_id &lt;= 0 and use_global_device_ids = false</code>.</li>
<li>
<code>cross_replica_and_partition(replica_groups)</code> if <code>channel_id &gt; 0 and use_global_device_ids = false</code>.</li>
<li>
<code>flattened_ids(replica_groups)</code> if <code>channel_id &gt; 0 and use_global_device_ids = true</code>.</li>
</ul><p>Afterwards, within each <code>process_group</code>:</p>
<ul><li>
<code>operands...@receiver = [operand@sender for sender in process_group]</code> for all <code>receiver</code> in <code>process_group</code>.</li>
<li>
<code>results...@process = concatenate(operands...@process, all_gather_dim)</code> for all <code>process</code> in <code>process_group</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-3">Inputs<a class="anchor" aria-label="anchor" href="#inputs-3"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="6%"><col width="23%"><col width="57%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operands</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1), (C6)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>all_gather_dim</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1), (C6)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>replica_groups</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2-C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C5)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>use_global_device_ids</code></td>
<td>constant of type <code>i1</code>
</td>
<td>(C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-3">Outputs<a class="anchor" aria-label="anchor" href="#outputs-3"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="71%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-2">Constraints<a class="anchor" aria-label="anchor" href="#constraints-2"></a></h4>
<ul><li>(C1) <code>0 &lt;= all_gather_dim &lt; rank(operands...)</code>.</li>
<li>(C2) <code>is_unique(replica_groups)</code>.</li>
<li>(C3) <code>size(replica_groups)</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_replicas</code> if <code>cross_replica_and_partition</code> is used.</li>
<li>
<code>num_processes</code> if <code>flattened_ids</code> is used.</li>
</ul></li>
<li>(C4) <code>0 &lt;= replica_groups &lt; size(replica_groups)</code>.</li>
<li>(C5) If <code>use_global_device_ids = true</code>, then <code>channel_id &gt; 0</code>.</li>
<li>(C6) <code>type(results...) = type(operands...)</code> except:
<ul><li>
<code>dim(results..., all_gather_dim) =   dim(operands..., all_gather_dim) * dim(process_groups, 1)</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-3">Examples<a class="anchor" aria-label="anchor" href="#examples-3"></a></h4>
<pre class="mlir"><code>// num_replicas: 2
// num_partitions: 1
// %operand0@(0, 0): [[1, 2], [3, 4]]
// %operand0@(1, 0): [[5, 6], [7, 8]]
// %operand1@(0, 0): [[11, 12], [13, 14]]
// %operand1@(1, 0): [[15, 16], [17, 18]]
%result:2 = "stablehlo.all_gather"(%operand0, %operand1) {
  all_gather_dim = 1 : i64,
  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;,
  // channel_id = 0
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;
  // use_global_device_ids = false
} : (tensor&lt;2x2xi64&gt;, tensor&lt;2x2xi64&gt;) -&gt; (tensor&lt;2x4xi64&gt;, tensor&lt;2x4xi64&gt;)
// %result0@(0, 0): [[1, 2, 5, 6], [3, 4, 7, 8]]
// %result0@(1, 0): [[1, 2, 5, 6], [3, 4, 7, 8]]
// %result1@(0, 0): [[11, 12, 15, 16], [13, 14, 17, 18]]
// %result1@(1, 0): [[11, 12, 15, 16], [13, 14, 17, 18]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/all_gather.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="all_reduce">all_reduce<a class="anchor" aria-label="anchor" href="#all_reduce"></a></h3>
<div class="section level4">
<h4 id="semantics-4">Semantics<a class="anchor" aria-label="anchor" href="#semantics-4"></a></h4>
<p>Within each process group in the StableHLO process grid, applies a reduction function <code>computation</code> to the values of the <code>operands</code> tensors from each process and produces <code>results</code> tensors.</p>
<p>The operation splits the StableHLO process grid into <code>process_groups</code> which is defined as follows:</p>
<ul><li>
<code>cross_replica(replica_groups)</code> if <code>channel_id &lt;= 0 and use_global_device_ids = false</code>.</li>
<li>
<code>cross_replica_and_partition(replica_groups)</code> if <code>channel_id &gt; 0 and use_global_device_ids = false</code>.</li>
<li>
<code>flattened_ids(replica_groups)</code> if <code>channel_id &gt; 0 and use_global_device_ids = true</code>.</li>
</ul><p>Afterwards, within each <code>process_group</code>:</p>
<ul><li>
<code>results...@process[result_index] = exec(schedule)</code> for some binary tree <code>schedule</code> where:
<ul><li>
<code>exec(node)</code> = <code>computation(exec(node.left), exec(node.right))</code>.</li>
<li>
<code>exec(leaf)</code> = <code>leaf.value</code>.</li>
</ul></li>
<li>
<code>schedule</code> is an implementation-defined binary tree whose in-order traversal is <code>to_destination_type(operands...@process_group...[result_index],   type(func_inputs(computation)[0]))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-4">Inputs<a class="anchor" aria-label="anchor" href="#inputs-4"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="22%"><col width="59%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operands</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C5), (C6)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>replica_groups</code></td>
<td>variadic number of 1-dimensional tensor constants of type <code>si64</code>
</td>
<td>(C1-C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>use_global_device_ids</code></td>
<td>constant of type <code>i1</code>
</td>
<td>(C4)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>computation</code></td>
<td>function</td>
<td>(C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-4">Outputs<a class="anchor" aria-label="anchor" href="#outputs-4"></a></h4>
<table class="table"><colgroup><col width="12%"><col width="71%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C6-C7)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-3">Constraints<a class="anchor" aria-label="anchor" href="#constraints-3"></a></h4>
<ul><li>(C1) <code>is_unique(replica_groups)</code>.</li>
<li>(C2) <code>size(replica_groups)</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_replicas</code> if <code>cross_replica_and_partition</code> is used.</li>
<li>
<code>num_processes</code> if <code>flattened_ids</code> is used.</li>
</ul></li>
<li>(C3) <code>0 &lt;= replica_groups &lt; size(replica_groups)</code>.</li>
<li>(C4) If <code>use_global_device_ids = true</code>, then <code>channel_id &gt; 0</code>.</li>
<li>(C5) <code>computation</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; (tensor&lt;E&gt;)</code> where <code>is_promotable(element_type(operand), E)</code>.</li>
<li>(C6) <code>shape(results...) = shape(operands...)</code>.</li>
<li>(C7) <code>element_type(results...) = E</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-4">Examples<a class="anchor" aria-label="anchor" href="#examples-4"></a></h4>
<pre class="mlir"><code>// num_replicas: 2
// num_partitions: 1
// %operand0@(0, 0): [1, 2, 3, 4]
// %operand0@(1, 0): [5, 6, 7, 8]
// %operand1@(0, 0): [9, 10, 11, 12]
// %operand1@(1, 0): [13, 14, 15, 16]
%result:2 = "stablehlo.all_reduce"(%operand0, %operand0) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i64&gt;
    "stablehlo.return"(%0) : (tensor&lt;i64&gt;) -&gt; ()
}) {
  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;,
  // channel_id = 0
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;
  // use_global_device_ids = false
} : (tensor&lt;4xi64&gt;, tensor&lt;4xi64&gt;) -&gt; (tensor&lt;4xi64&gt;, tensor&lt;4xi64&gt;)
// %result0@(0, 0): [6, 8, 10, 12]
// %result0@(1, 0): [6, 8, 10, 12]
// %result1@(0, 0): [22, 24, 26, 28]
// %result1@(1, 0): [22, 24, 26, 28]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/all_reduce.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="all_to_all">all_to_all<a class="anchor" aria-label="anchor" href="#all_to_all"></a></h3>
<div class="section level4">
<h4 id="semantics-5">Semantics<a class="anchor" aria-label="anchor" href="#semantics-5"></a></h4>
<div class="float">
<img src="images/spec/all_to_all.svg" alt="all_to_all"><div class="figcaption">all_to_all</div>
</div>
<p>Within each process group in the StableHLO process grid, splits the values of the <code>operands</code> tensors along <code>split_dimension</code> into parts, scatters the split parts between the processes, concatenates the scattered parts along <code>concat_dimension</code> and produces <code>results</code> tensors. The operation splits the StableHLO process grid into <code>process_groups</code> which is defined as follows:</p>
<ul><li>
<code>cross_replica(replica_groups)</code> if <code>channel_id &lt;= 0</code>.</li>
<li>
<code>cross_partition(replica_groups)</code> if <code>channel_id &gt; 0</code>.</li>
</ul><p>Afterwards, within each <code>process_group</code>:</p>
<ul><li>
<code>split_parts...@sender = split(operands...@sender, split_count, split_dimension)</code> for all <code>sender</code> in <code>process_group</code>.</li>
<li>
<code>scattered_parts...@receiver = [split_parts...@sender[receiver_index] for   sender in process_group]</code> where <code>receiver_index = process_group.index(receiver)</code>.</li>
<li>
<code>results...@process = concatenate(scattered_parts...@process, concat_dimension)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-5">Inputs<a class="anchor" aria-label="anchor" href="#inputs-5"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="6%"><col width="17%"><col width="54%"><col width="21%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operands</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1-C3), (C9)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>split_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1), (C2), (C9)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>concat_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C3), (C9)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>split_count</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C8), (C9)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>replica_groups</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C5-C8)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-5">Outputs<a class="anchor" aria-label="anchor" href="#outputs-5"></a></h4>
<table class="table"><colgroup><col width="12%"><col width="71%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C9)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-4">Constraints<a class="anchor" aria-label="anchor" href="#constraints-4"></a></h4>
<ul><li>(C1) <code>0 &lt;= split_dimension &lt; rank(operands...)</code>.</li>
<li>(C2) <code>dim(operands..., split_dimension) % split_count = 0</code>.</li>
<li>(C3) <code>0 &lt;= concat_dimension &lt; rank(operands...)</code>.</li>
<li>(C4) <code>0 &lt; split_count</code>.</li>
<li>(C5) <code>is_unique(replica_groups)</code>.</li>
<li>(C6) <code>size(replica_groups)</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_partitions</code> if <code>cross_partition</code> is used.</li>
</ul></li>
<li>(C7) <code>0 &lt;= replica_groups &lt; size(replica_groups)</code>.</li>
<li>(C8) <code>dim(replica_groups, 1) = split_count</code>.</li>
<li>(C9) <code>type(results...) = type(operands...)</code> except, if <code>split_dimension !=   concat_dimension</code>:
<ul><li>
<code>dim(results..., split_dimension) =   dim(operands..., split_dimension) / split_count</code>.</li>
<li>
<code>dim(results..., concat_dimension) =   dim(operands..., concat_dimension) * split_count</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-5">Examples<a class="anchor" aria-label="anchor" href="#examples-5"></a></h4>
<pre class="mlir"><code>// num_replicas: 2
// num_partitions: 1
// %operand1@(0, 0): [[1, 2, 3, 4],
//                    [5, 6, 7, 8]]
// %operand1@(1, 0): [[9, 10, 11, 12],
//                    [13, 14, 15, 16]]
// %operand2@(0, 0): [[17, 18, 19, 20],
//                    [21, 22, 23, 24]]
// %operand2@(1, 0): [[25, 26, 27, 28],
//                    [29, 30, 31, 32]]
%result:2 = "stablehlo.all_to_all"(%operand1, %operand2) {
  split_dimension = 1 : i64,
  concat_dimension = 0 : i64,
  split_count = 2 : i64,
  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;
  // channel_id = 0
} : (tensor&lt;2x4xi64&gt;, tensor&lt;2x4xi64&gt;) -&gt; (tensor&lt;4x2xi64&gt;, tensor&lt;4x2xi64&gt;)
// %result#0@(0, 0): [[1, 2], [5, 6], [9, 10], [13, 14]]
// %result#0@(1, 0): [[3, 4], [7, 8], [11, 12], [15, 16]]
// %result#1@(0, 0): [[17, 18], [21, 22], [25, 26], [29, 30]]
// %result#1@(1, 0): [[19, 20], [23, 24], [27, 28], [31, 32]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/all_to_all.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="and">and<a class="anchor" aria-label="anchor" href="#and"></a></h3>
<div class="section level4">
<h4 id="semantics-6">Semantics<a class="anchor" aria-label="anchor" href="#semantics-6"></a></h4>
<p>Performs element-wise AND of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical AND.</li>
<li>For integers: bitwise AND.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-6">Inputs<a class="anchor" aria-label="anchor" href="#inputs-6"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-6">Outputs<a class="anchor" aria-label="anchor" href="#outputs-6"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-5">Constraints<a class="anchor" aria-label="anchor" href="#constraints-5"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-6">Examples<a class="anchor" aria-label="anchor" href="#examples-6"></a></h4>
<pre class="mlir"><code>// %lhs: [[1, 2], [3, 4]]
// %rhs: [[5, 6], [7, 8]]
%result = "stablehlo.and"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[1, 2], [3, 0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/and.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="atan2">atan2<a class="anchor" aria-label="anchor" href="#atan2"></a></h3>
<div class="section level4">
<h4 id="semantics-7">Semantics<a class="anchor" aria-label="anchor" href="#semantics-7"></a></h4>
<p>Performs element-wise atan2 operation on <code>lhs</code> and <code>rhs</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>atan2</code> from IEEE-754.</li>
<li>For complex numbers: complex atan2.</li>
<li>For quantized types: <code>dequantize_op_quantize(atan2, lhs, rhs, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-7">Inputs<a class="anchor" aria-label="anchor" href="#inputs-7"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="7%"><col width="73%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-7">Outputs<a class="anchor" aria-label="anchor" href="#outputs-7"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-6">Constraints<a class="anchor" aria-label="anchor" href="#constraints-6"></a></h4>
<ul><li>(C1) <code>baseline_type(lhs) = baseline_type(rhs) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-7">Examples<a class="anchor" aria-label="anchor" href="#examples-7"></a></h4>
<pre class="mlir"><code>// %lhs: [0.0, 1.0, -1.0]
// %rhs: [0.0, 0.0, 0.0]
%result = "stablehlo.atan2"(%lhs, %rhs) : (tensor&lt;3xf64&gt;, tensor&lt;3xf64&gt;) -&gt; tensor&lt;3xf64&gt;
// %result: [0.0, 1.57079637, -1.57079637] // [0.0, pi/2, -pi/2]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/atan2.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="batch_norm_grad">batch_norm_grad<a class="anchor" aria-label="anchor" href="#batch_norm_grad"></a></h3>
<div class="section level4">
<h4 id="semantics-8">Semantics<a class="anchor" aria-label="anchor" href="#semantics-8"></a></h4>
<p>Computes gradients of several inputs of <code>batch_norm_training</code> backpropagating from <code>grad_output</code>, and produces <code>grad_operand</code>, <code>grad_scale</code> and <code>grad_offset</code> tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python syntax as follows:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="kw">def</span> compute_sum(operand, feature_index):</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a>  (<span class="bu">sum</span>,) <span class="op">=</span> <span class="bu">reduce</span>(</span>
<span id="cb35-3"><a href="#cb35-3" tabindex="-1"></a>      inputs<span class="op">=</span>[operand],</span>
<span id="cb35-4"><a href="#cb35-4" tabindex="-1"></a>      init_values<span class="op">=</span>[constant(<span class="dv">0</span>, element_type(operand))],</span>
<span id="cb35-5"><a href="#cb35-5" tabindex="-1"></a>      dimensions<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(rank(operand)) <span class="cf">if</span> i <span class="op">!=</span> feature_index],</span>
<span id="cb35-6"><a href="#cb35-6" tabindex="-1"></a>      body<span class="op">=</span><span class="kw">lambda</span> x, y: add(x, y))</span>
<span id="cb35-7"><a href="#cb35-7" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">sum</span></span>
<span id="cb35-8"><a href="#cb35-8" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" tabindex="-1"></a><span class="kw">def</span> compute_mean(operand, feature_index):</span>
<span id="cb35-10"><a href="#cb35-10" tabindex="-1"></a>  <span class="bu">sum</span> <span class="op">=</span> compute_sum(operand, feature_index)</span>
<span id="cb35-11"><a href="#cb35-11" tabindex="-1"></a>  divisor <span class="op">=</span> constant(size(operand) <span class="op">/</span> dim(operand, feature_index),</span>
<span id="cb35-12"><a href="#cb35-12" tabindex="-1"></a>                     element_type(operand))</span>
<span id="cb35-13"><a href="#cb35-13" tabindex="-1"></a>  divisor_bcast <span class="op">=</span> broadcast_in_dim(divisor, [], <span class="bu">type</span>(<span class="bu">sum</span>))</span>
<span id="cb35-14"><a href="#cb35-14" tabindex="-1"></a>  <span class="cf">return</span> divide(<span class="bu">sum</span>, divisor_bcast)</span>
<span id="cb35-15"><a href="#cb35-15" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" tabindex="-1"></a><span class="kw">def</span> batch_norm_grad(operand, scale, mean, variance, grad_output, epsilon, feature_index):</span>
<span id="cb35-17"><a href="#cb35-17" tabindex="-1"></a>  <span class="co"># Broadcast inputs to type(operand)</span></span>
<span id="cb35-18"><a href="#cb35-18" tabindex="-1"></a>  scale_bcast <span class="op">=</span> broadcast_in_dim(scale, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb35-19"><a href="#cb35-19" tabindex="-1"></a>  mean_bcast <span class="op">=</span> broadcast_in_dim(mean, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb35-20"><a href="#cb35-20" tabindex="-1"></a>  variance_bcast <span class="op">=</span> broadcast_in_dim(variance, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb35-21"><a href="#cb35-21" tabindex="-1"></a>  epsilon_bcast <span class="op">=</span> broadcast_in_dim(constant(epsilon, element_type(operand)), [],</span>
<span id="cb35-22"><a href="#cb35-22" tabindex="-1"></a>                                   <span class="bu">type</span>(operand))</span>
<span id="cb35-23"><a href="#cb35-23" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" tabindex="-1"></a>  <span class="co"># Perform normalization using the provided `mean` and `variance`</span></span>
<span id="cb35-25"><a href="#cb35-25" tabindex="-1"></a>  <span class="co"># Intermediate values will be useful for computing gradients</span></span>
<span id="cb35-26"><a href="#cb35-26" tabindex="-1"></a>  centered_operand <span class="op">=</span> subtract(operand, mean_bcast)</span>
<span id="cb35-27"><a href="#cb35-27" tabindex="-1"></a>  stddev <span class="op">=</span> sqrt(add(variance_bcast, epsilon_bcast))</span>
<span id="cb35-28"><a href="#cb35-28" tabindex="-1"></a>  normalized_operand <span class="op">=</span> divide(centered_operand, stddev)</span>
<span id="cb35-29"><a href="#cb35-29" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" tabindex="-1"></a>  <span class="co"># Use the implementation from batchnorm_expander.cc in XLA</span></span>
<span id="cb35-31"><a href="#cb35-31" tabindex="-1"></a>  <span class="co"># Temporary variables have exactly the same names as in the C++ code</span></span>
<span id="cb35-32"><a href="#cb35-32" tabindex="-1"></a>  elements_per_feature <span class="op">=</span> broadcast_in_dim(</span>
<span id="cb35-33"><a href="#cb35-33" tabindex="-1"></a>      constant(divide(size(operand), dim(operand, feature_index)),</span>
<span id="cb35-34"><a href="#cb35-34" tabindex="-1"></a>               element_type(grad_output)),</span>
<span id="cb35-35"><a href="#cb35-35" tabindex="-1"></a>      [], <span class="bu">type</span>(operand))</span>
<span id="cb35-36"><a href="#cb35-36" tabindex="-1"></a>  i1 <span class="op">=</span> multiply(grad_output, elements_per_feature)</span>
<span id="cb35-37"><a href="#cb35-37" tabindex="-1"></a>  i2 <span class="op">=</span> broadcast_in_dim(</span>
<span id="cb35-38"><a href="#cb35-38" tabindex="-1"></a>      compute_sum(grad_output, feature_index), [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb35-39"><a href="#cb35-39" tabindex="-1"></a>  i3 <span class="op">=</span> broadcast_in_dim(</span>
<span id="cb35-40"><a href="#cb35-40" tabindex="-1"></a>      compute_sum(multiply(grad_output, centered_operand), feature_index),</span>
<span id="cb35-41"><a href="#cb35-41" tabindex="-1"></a>      [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb35-42"><a href="#cb35-42" tabindex="-1"></a>  i4 <span class="op">=</span> multiply(i3, centered_operand)</span>
<span id="cb35-43"><a href="#cb35-43" tabindex="-1"></a>  i5 <span class="op">=</span> divide(i4, add(variance_bcast, epsilon_bcast))</span>
<span id="cb35-44"><a href="#cb35-44" tabindex="-1"></a>  i6 <span class="op">=</span> subtract(subtract(i1, i2), i5)</span>
<span id="cb35-45"><a href="#cb35-45" tabindex="-1"></a></span>
<span id="cb35-46"><a href="#cb35-46" tabindex="-1"></a>  grad_operand <span class="op">=</span></span>
<span id="cb35-47"><a href="#cb35-47" tabindex="-1"></a>      multiply(divide(divide(scale_bcast, stddev), elements_per_feature), i6)</span>
<span id="cb35-48"><a href="#cb35-48" tabindex="-1"></a>  grad_scale <span class="op">=</span></span>
<span id="cb35-49"><a href="#cb35-49" tabindex="-1"></a>      compute_sum(multiply(grad_output, normalized_operand), feature_index)</span>
<span id="cb35-50"><a href="#cb35-50" tabindex="-1"></a>  grad_offset <span class="op">=</span> compute_sum(grad_output, feature_index)</span>
<span id="cb35-51"><a href="#cb35-51" tabindex="-1"></a></span>
<span id="cb35-52"><a href="#cb35-52" tabindex="-1"></a>  <span class="cf">return</span> grad_operand, grad_scale, grad_offset</span></code></pre></div>
<p>For quantized types, performs <code>dequantize_batch_norm_grad_or_training_quantize(lambda operand, scale, mean, variance, grad_output: batch_norm_grad(operand, scale, mean, variance, grad_output, epsilon, feature_index), operand, scale, mean, variance, grad_output, type(grad_operand), type(grad_scale), type(feature_index))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-8">Inputs<a class="anchor" aria-label="anchor" href="#inputs-8"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="15%"><col width="62%"><col width="16%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1-C3), (C5)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>scale</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C4), (C5)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>mean</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>variance</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C4)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>grad_output</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C2), (C3)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>epsilon</code></td>
<td>constant of type <code>f32</code>
</td>
<td></td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>feature_index</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1), (C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-8">Outputs<a class="anchor" aria-label="anchor" href="#outputs-8"></a></h4>
<table class="table"><colgroup><col width="16%"><col width="70%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>grad_operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C2), (C3)</td>
</tr><tr class="even"><td><code>grad_scale</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C4)</td>
</tr><tr class="odd"><td><code>grad_offset</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-7">Constraints<a class="anchor" aria-label="anchor" href="#constraints-7"></a></h4>
<ul><li>(C1) <code>0 &lt;= feature_index &lt; rank(operand)</code>.</li>
<li>(C2) <code>operand</code>, <code>scale</code>, <code>mean</code>, <code>variance</code>, <code>grad_output</code>, <code>grad_operand</code>, <code>grad_scale</code> and <code>grad_offset</code> have the same <code>baseline_element_type</code>.</li>
<li>(C3) <code>operand</code>, <code>grad_output</code> and <code>grad_operand</code> have the same shape.</li>
<li>(C4) <code>scale</code>, <code>mean</code>, <code>variance</code>, <code>grad_scale</code> and <code>grad_offset</code> have the same shape.</li>
<li>(C5) <code>size(scale) = dim(operand, feature_index)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-8">Examples<a class="anchor" aria-label="anchor" href="#examples-8"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [[1.0, 2.0], [3.0, 4.0]],
//            [[3.0, 4.0], [1.0, 2.0]]
//           ]
// %scale: [1.0, 1.0]
// %mean: [2.0, 3.0]
// %variance: [1.0, 1.0]
// %grad_output: [
//                [[0.1, 0.1], [0.1, 0.1]],
//                [[0.1, 0.1], [0.1, 0.1]]
//               ]
%grad_operand, %grad_scale, %grad_offset =
"stablehlo.batch_norm_grad"(%operand, %scale, %mean, %variance, %grad_output) {
  epsilon = 0.0 : f32,
  feature_index = 2 : i64
} : (tensor&lt;2x2x2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;,
     tensor&lt;2x2x2xf64&gt;) -&gt; (tensor&lt;2x2x2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;)
// %grad_operand: [
//                 [[0.0, 0.0], [0.0, 0.0]],
//                 [[0.0, 0.0], [0.0, 0.0]]
//                ]
// %grad_scale:  [0.0, 0.0]
// %grad_offset: [0.4, 0.4]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="batch_norm_inference">batch_norm_inference<a class="anchor" aria-label="anchor" href="#batch_norm_inference"></a></h3>
<div class="section level4">
<h4 id="semantics-9">Semantics<a class="anchor" aria-label="anchor" href="#semantics-9"></a></h4>
<p>Normalizes the <code>operand</code> tensor across all dimensions except for the <code>feature_index</code> dimension and produces a <code>result</code> tensor. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python syntax as follows:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="kw">def</span> batch_norm_inference(operand, scale, offset, mean, variance, epsilon, feature_index):</span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>  <span class="co"># Broadcast inputs to shape(operand)</span></span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a>  scale_bcast <span class="op">=</span> broadcast_in_dim(scale, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a>  offset_bcast <span class="op">=</span> broadcast_in_dim(offset, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a>  mean_bcast <span class="op">=</span> broadcast_in_dim(mean, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a>  variance_bcast <span class="op">=</span> broadcast_in_dim(variance, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a>  epsilon_bcast <span class="op">=</span> broadcast_in_dim(constant(epsilon, element_type(operand)), [],</span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a>                                   <span class="bu">type</span>(operand))</span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>  <span class="co"># Perform normalization using the provided `mean` and `variance` instead of</span></span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a>  <span class="co"># computing them like `batch_norm_training` does.</span></span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a>  centered_operand <span class="op">=</span> subtract(operand, mean_bcast)</span>
<span id="cb37-13"><a href="#cb37-13" tabindex="-1"></a>  stddev <span class="op">=</span> sqrt(add(variance_bcast, epsilon_bcast))</span>
<span id="cb37-14"><a href="#cb37-14" tabindex="-1"></a>  normalized_operand <span class="op">=</span> divide(centered_operand, stddev)</span>
<span id="cb37-15"><a href="#cb37-15" tabindex="-1"></a>  <span class="cf">return</span> add(multiply(scale_bcast, normalized_operand), offset_bcast)</span></code></pre></div>
<p>For quantized types, performs <code>dequantize_op_quantize(lambda operand, scale, offset, mean, variance: batch_norm_inference(operand, scale, offset, mean, variance, epsilon, feature_index), operand, scale, offset, mean, variance, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-9">Inputs<a class="anchor" aria-label="anchor" href="#inputs-9"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="15%"><col width="63%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1-C7)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>scale</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>offset</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>mean</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C5)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>variance</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized type</td>
<td>(C2), (C6)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>epsilon</code></td>
<td>constant of type <code>f32</code>
</td>
<td></td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>feature_index</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1), (C3-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-9">Outputs<a class="anchor" aria-label="anchor" href="#outputs-9"></a></h4>
<table class="table"><colgroup><col width="11%"><col width="72%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C2), (C7)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-8">Constraints<a class="anchor" aria-label="anchor" href="#constraints-8"></a></h4>
<ul><li>(C1) <code>0 &lt;= feature_index &lt; rank(operand)</code>.</li>
<li>(C2) <code>operand</code>, <code>scale</code>, <code>offset</code>, <code>mean</code>, <code>variance</code> and <code>result</code> have the same <code>baseline_element_type</code>.</li>
<li>(C3) <code>size(scale) = dim(operand, feature_index)</code>.</li>
<li>(C4) <code>size(offset) = dim(operand, feature_index)</code>.</li>
<li>(C5) <code>size(mean) = dim(operand, feature_index)</code>.</li>
<li>(C6) <code>size(variance) = dim(operand, feature_index)</code>.</li>
<li>(C7) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-9">Examples<a class="anchor" aria-label="anchor" href="#examples-9"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [[1.0, 2.0], [3.0, 4.0]],
//            [[3.0, 4.0], [1.0, 2.0]]
//           ]
// %scale: [1.0, 1.0]
// %offset: [1.0, 1.0]
// %mean: [2.0, 3.0]
// %variance: [1.0, 1.0]
%result = "stablehlo.batch_norm_inference"(%operand, %scale, %offset, %mean, %variance) {
  epsilon = 0.0 : f32,
  feature_index = 2 : i64
} : (tensor&lt;2x2x2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;) -&gt; tensor&lt;2x2x2xf64&gt;
// %result: [
//           [[0.0, 0.0], [2.0, 2.0]],
//           [[2.0, 2.0], [0.0, 0.0]]
//          ]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="batch_norm_training">batch_norm_training<a class="anchor" aria-label="anchor" href="#batch_norm_training"></a></h3>
<div class="section level4">
<h4 id="semantics-10">Semantics<a class="anchor" aria-label="anchor" href="#semantics-10"></a></h4>
<p>Computes mean and variance across all dimensions except for the <code>feature_index</code> dimension and normalizes the <code>operand</code> tensor producing <code>output</code>, <code>batch_mean</code> and <code>batch_var</code> tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python syntax as follows:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="kw">def</span> compute_mean(operand, feature_index):</span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a>  (<span class="bu">sum</span>,) <span class="op">=</span> <span class="bu">reduce</span>(</span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a>      inputs<span class="op">=</span>[operand],</span>
<span id="cb39-4"><a href="#cb39-4" tabindex="-1"></a>      init_values<span class="op">=</span>[constant(<span class="dv">0</span>, element_type(operand))],</span>
<span id="cb39-5"><a href="#cb39-5" tabindex="-1"></a>      dimensions<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(rank(operand)) <span class="cf">if</span> i <span class="op">!=</span> feature_index],</span>
<span id="cb39-6"><a href="#cb39-6" tabindex="-1"></a>      body<span class="op">=</span><span class="kw">lambda</span> x, y: add(x, y))</span>
<span id="cb39-7"><a href="#cb39-7" tabindex="-1"></a>  divisor <span class="op">=</span> constant(size(operand) <span class="op">/</span> dim(operand, feature_index),</span>
<span id="cb39-8"><a href="#cb39-8" tabindex="-1"></a>                     element_type(operand))</span>
<span id="cb39-9"><a href="#cb39-9" tabindex="-1"></a>  divisor_bcast <span class="op">=</span> broadcast_in_dim(divisor, [], <span class="bu">type</span>(<span class="bu">sum</span>))</span>
<span id="cb39-10"><a href="#cb39-10" tabindex="-1"></a>  <span class="cf">return</span> divide(<span class="bu">sum</span>, divisor_bcast)</span>
<span id="cb39-11"><a href="#cb39-11" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" tabindex="-1"></a><span class="kw">def</span> compute_variance(operand, feature_index):</span>
<span id="cb39-13"><a href="#cb39-13" tabindex="-1"></a>  mean <span class="op">=</span> compute_mean(operand, feature_index)</span>
<span id="cb39-14"><a href="#cb39-14" tabindex="-1"></a>  mean_bcast <span class="op">=</span> broadcast_in_dim(mean, [feature_index], <span class="bu">type</span>(operand))</span>
<span id="cb39-15"><a href="#cb39-15" tabindex="-1"></a>  centered_operand <span class="op">=</span> subtract(operand, mean_bcast)</span>
<span id="cb39-16"><a href="#cb39-16" tabindex="-1"></a>  <span class="cf">return</span> compute_mean(mul(centered_operand, centered_operand), feature_index)</span>
<span id="cb39-17"><a href="#cb39-17" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" tabindex="-1"></a><span class="kw">def</span> batch_norm_training(operand, scale, offset, epsilon, feature_index):</span>
<span id="cb39-19"><a href="#cb39-19" tabindex="-1"></a>  mean <span class="op">=</span> compute_mean(operand, feature_index)</span>
<span id="cb39-20"><a href="#cb39-20" tabindex="-1"></a>  variance <span class="op">=</span> compute_variance(operand, feature_index)</span>
<span id="cb39-21"><a href="#cb39-21" tabindex="-1"></a>  <span class="cf">return</span> batch_norm_inference(operand, scale, offset, mean, variance, epsilon,</span>
<span id="cb39-22"><a href="#cb39-22" tabindex="-1"></a>                              feature_index),</span>
<span id="cb39-23"><a href="#cb39-23" tabindex="-1"></a>         mean, variance</span></code></pre></div>
<p>For quantized types, performs <code>dequantize_batch_norm_grad_or_training_quantize(lambda operand, scale, offset: batch_norm_training(operand, scale, offset, epsilon, feature_index), operand, scale, offset, type(output), type(batch_mean), type(batch_var))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-10">Inputs<a class="anchor" aria-label="anchor" href="#inputs-10"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="16%"><col width="62%"><col width="14%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>scale</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized</td>
<td>(C2), (C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>offset</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized</td>
<td>(C2), (C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>epsilon</code></td>
<td>constant of type <code>f32</code>
</td>
<td>(C1), (C3-C6)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>feature_index</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1), (C3-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-10">Outputs<a class="anchor" aria-label="anchor" href="#outputs-10"></a></h4>
<table class="table"><colgroup><col width="15%"><col width="70%"><col width="14%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>output</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C7)</td>
</tr><tr class="even"><td><code>batch_mean</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized</td>
<td>(C2), (C5)</td>
</tr><tr class="odd"><td><code>batch_var</code></td>
<td>1-dimensional tensor of floating-point or per-tensor quantized</td>
<td>(C2), (C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-9">Constraints<a class="anchor" aria-label="anchor" href="#constraints-9"></a></h4>
<ul><li>(C1) <code>0 &lt;= feature_index &lt; rank(operand)</code>.</li>
<li>(C2) <code>operand</code>, <code>scale</code>, <code>offset</code>, <code>batch_mean</code>, <code>batch_var</code> and <code>output</code> have the same <code>baseline_element_type</code>.</li>
<li>(C3) <code>size(scale) = dim(operand, feature_index)</code>.</li>
<li>(C4) <code>size(offset) = dim(operand, feature_index)</code>.</li>
<li>(C5) <code>size(batch_mean) = dim(operand, feature_index)</code>.</li>
<li>(C6) <code>size(batch_var) = dim(operand, feature_index)</code>.</li>
<li>(C7) <code>baseline_type(output) = baseline_type(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-10">Examples<a class="anchor" aria-label="anchor" href="#examples-10"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [[1.0, 2.0], [3.0, 4.0]],
//            [[3.0, 4.0], [1.0, 2.0]]
//           ]
// %scale: [1.0, 1.0]
// %offset: [1.0, 1.0]
%output, %batch_mean, %batch_var = "stablehlo.batch_norm_training"(%operand, %scale, %offset) {
  epsilon = 0.0 : f32,
  feature_index = 2 : i64
} : (tensor&lt;2x2x2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;) -&gt;
    (tensor&lt;2x2x2xf64&gt;, tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;)
// %output: [
//           [[0.0, 0.0], [2.0, 2.0]],
//           [[2.0, 2.0], [0.0, 0.0]]
//          ]
// %batch_mean: [2.0, 3.0]
// %batch_var: [1.0, 1.0]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="bitcast_convert">bitcast_convert<a class="anchor" aria-label="anchor" href="#bitcast_convert"></a></h3>
<div class="section level4">
<h4 id="semantics-11">Semantics<a class="anchor" aria-label="anchor" href="#semantics-11"></a></h4>
<p>Performs a bitcast operation on <code>operand</code> tensor and produces a <code>result</code> tensor where the bits of the entire <code>operand</code> tensor are reinterpreted using the type of the <code>result</code> tensor.</p>
<p>More formally, given <code>E = element_type(operand)</code>, <code>E' = element_type(result)</code>, and <code>R = rank(operand)</code>:</p>
<ul><li>If <code>num_bits(E') &lt; num_bits(E)</code>, <code>bits(result[i0, ..., iR-1, :]) = bits(operand[i0, ..., iR-1])</code>.</li>
<li>If <code>num_bits(E') &gt; num_bits(E)</code>, <code>bits(result[i0, ..., iR-2]) = bits(operand[i0, ..., iR-2, :])</code>.</li>
<li>If <code>num_bits(E') = num_bits(E)</code>, <code>bits(result[i0, ..., iR-1]) = bits(operand[i0, ..., iR-1])</code>.</li>
</ul><p><code>bits</code> returns in-memory representation of a given value, and its behavior is implementation-defined because the exact representation of tensors is implementation-defined, and the exact representation of element types is implementation-defined as well.</p>
</div>
<div class="section level4">
<h4 id="inputs-11">Inputs<a class="anchor" aria-label="anchor" href="#inputs-11"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-11">Outputs<a class="anchor" aria-label="anchor" href="#outputs-11"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-10">Constraints<a class="anchor" aria-label="anchor" href="#constraints-10"></a></h4>
<ul><li>(C1) Given <code>E = is_quantized(operand) ? storage_type(operand) :   element_type(operand)</code>, <code>E' = is_quantized(result) ?   storage_type(result) : element_type(result)</code>, and <code>R = rank(operand)</code>:
<ul><li>If <code>num_bits(E') = num_bits(E)</code>, <code>shape(result) = shape(operand)</code>.</li>
<li>If <code>num_bits(E') &lt; num_bits(E)</code>:
<ul><li>
<code>rank(result) = R + 1</code>.</li>
<li>
<code>dim(result, i) = dim(operand, i)</code> for all <code>0 &lt;= i &lt; R</code>.</li>
<li>
<code>dim(result, R) * num_bits(E') = num_bits(E)</code>.</li>
</ul></li>
<li>If <code>num_bits(E') &gt; num_bits(E)</code>:
<ul><li>
<code>rank(result) = R - 1</code>.</li>
<li>
<code>dim(result, i) = dim(operand, i)</code> for all <code>0 &lt;= i &lt; R</code>.</li>
<li>
<code>dim(operand, R - 1) * num_bits(E) = num_bits(E')</code>.</li>
</ul></li>
</ul></li>
<li>(C2) If <code>is_complex(operand) or is_complex(result)</code>, then <code>is_complex(operand) and is_complex(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-11">Examples<a class="anchor" aria-label="anchor" href="#examples-11"></a></h4>
<pre class="mlir"><code>// %operand: 0x0123456789ABCDEF
%result = "stablehlo.bitcast_convert"(%operand) : (tensor&lt;f64&gt;) -&gt; tensor&lt;4xf16&gt;
// %result: [0xCDEF, 0x89AB, 0x4567, 0x0123] // little-endian representation</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/bitcast_convert.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="broadcast_in_dim">broadcast_in_dim<a class="anchor" aria-label="anchor" href="#broadcast_in_dim"></a></h3>
<div class="section level4">
<h4 id="semantics-12">Semantics<a class="anchor" aria-label="anchor" href="#semantics-12"></a></h4>
<p>Expands the dimensions and/or rank of an input tensor by duplicating the data in the <code>operand</code> tensor and produces a <code>result</code> tensor. More formally, <code>result[result_index] = operand[operand_index]</code> where for all <code>d</code> in <code>axes(operand)</code>:</p>
<ul><li>
<code>operand_index[d] = 0</code> if <code>dim(operand, d) = 1</code>.</li>
<li>
<code>operand_index[d] = result_index[broadcast_dimensions[d]]</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-12">Inputs<a class="anchor" aria-label="anchor" href="#inputs-12"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="25%"><col width="48%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C2), (C5-C6)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>broadcast_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-12">Outputs<a class="anchor" aria-label="anchor" href="#outputs-12"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1), (C3), (C5-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-11">Constraints<a class="anchor" aria-label="anchor" href="#constraints-11"></a></h4>
<ul><li>(C1) <code>element_type(result)</code> is given by:
<ul><li>
<code>element_type(operand)</code>, if <code>!is_per_axis_quantized(operand)</code>.</li>
<li>
<code>element_type(operand)</code> except that <code>quantization_dimension(operand)</code>, <code>scales(operand)</code>, and <code>zero_points(operand)</code> may differ from <code>quantization_dimension(result)</code>, <code>scales(result)</code>, and <code>zero_points(result)</code> resp., otherwise.</li>
</ul></li>
<li>(C2) <code>size(broadcast_dimensions) = rank(operand)</code>.</li>
<li>(C3) <code>0 &lt;= broadcast_dimensions &lt; rank(result)</code>.</li>
<li>(C4) <code>is_unique(broadcast_dimensions)</code>.</li>
<li>(C5) For all <code>d</code> in <code>axes(operand)</code>:
<ul><li>
<code>dim(operand, d) = 1</code> or</li>
<li>
<code>dim(operand, d) = dim(result, broadcast_dimensions[d])</code>.</li>
</ul></li>
<li>(C6) If <code>is_per_axis_quantized(result)</code>:
<ul><li>
<code>quantization_dimension(result) = broadcast_dimensions[quantization_dimension(operand)]</code>.</li>
<li>If <code>dim(operand, quantization_dimension(operand)) = 1</code>, then <code>scales(result)[i] = scales(operand)[0] and zero_points(result)[i] =   zero_points(operand)[0] for i in   range(dim(result, quantization_dimension(result)))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-12">Examples<a class="anchor" aria-label="anchor" href="#examples-12"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [1, 2, 3]
//           ]
%result = "stablehlo.broadcast_in_dim"(%operand) {
  broadcast_dimensions = array&lt;i64: 2, 1&gt;
} : (tensor&lt;1x3xi32&gt;) -&gt; tensor&lt;2x3x2xi32&gt;
// %result: [
//            [
//             [1, 1],
//             [2, 2],
//             [3, 3]
//            ],
//            [
//             [1, 1],
//             [2, 2],
//             [3, 3]
//            ]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/broadcast_in_dim.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="case">case<a class="anchor" aria-label="anchor" href="#case"></a></h3>
<div class="section level4">
<h4 id="semantics-13">Semantics<a class="anchor" aria-label="anchor" href="#semantics-13"></a></h4>
<p>Produces the output from executing exactly one function from <code>branches</code> depending on the value of <code>index</code>. More formally, <code>result = selected_branch()</code> where:</p>
<ul><li>
<code>selected_branch = branches[index]</code> if <code>0 &lt;= index &lt; size(branches)</code>.</li>
<li>
<code>selected_branch = branches[-1]</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-13">Inputs<a class="anchor" aria-label="anchor" href="#inputs-13"></a></h4>
<table class="table"><colgroup><col width="10%"><col width="17%"><col width="53%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>index</code></td>
<td>0-dimensional tensor of type <code>si32</code>
</td>
<td></td>
</tr><tr class="even"><td>(I2)</td>
<td><code>branches</code></td>
<td>variadic number of functions</td>
<td>(C1-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-13">Outputs<a class="anchor" aria-label="anchor" href="#outputs-13"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="70%"><col width="16%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors, quantized tensors or tokens</td>
<td>(C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-12">Constraints<a class="anchor" aria-label="anchor" href="#constraints-12"></a></h4>
<ul><li>(C1) <code>0 &lt; size(branches)</code>.</li>
<li>(C2) <code>input_types(branches...) = []</code>.</li>
<li>(C3) <code>same(output_types(branches...))</code>.</li>
<li>(C4) <code>type(results...) = output_types(branches[0])</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-13">Examples<a class="anchor" aria-label="anchor" href="#examples-13"></a></h4>
<pre class="mlir"><code>// %index: -1
// %result_branch0: [0, 0]
// %result_branch1: [1, 1]
%result0, %result1 = "stablehlo.case"(%index) ({
  "stablehlo.return"(%result_branch0, %result_branch0) : (tensor&lt;2xi64&gt;, tensor&lt;2xi64&gt;) -&gt; ()
}, {
  "stablehlo.return"(%result_branch1, %result_branch1) : (tensor&lt;2xi64&gt;, tensor&lt;2xi64&gt;) -&gt; ()
}) : (tensor&lt;i32&gt;) -&gt; (tensor&lt;2xi64&gt;, tensor&lt;2xi64&gt;)
// %result0: [1, 1]
// %result1: [1, 1]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/case.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="cbrt">cbrt<a class="anchor" aria-label="anchor" href="#cbrt"></a></h3>
<div class="section level4">
<h4 id="semantics-14">Semantics<a class="anchor" aria-label="anchor" href="#semantics-14"></a></h4>
<p>Performs element-wise cubic root operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>rootn(x, 3)</code> from IEEE-754.</li>
<li>For complex numbers: complex cubic root.</li>
<li>For quantized types: <code>dequantize_op_quantize(cbrt, operand, type(result))</code>
</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-14">Inputs<a class="anchor" aria-label="anchor" href="#inputs-14"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-14">Outputs<a class="anchor" aria-label="anchor" href="#outputs-14"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-13">Constraints<a class="anchor" aria-label="anchor" href="#constraints-13"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-14">Examples<a class="anchor" aria-label="anchor" href="#examples-14"></a></h4>
<pre class="mlir"><code>// %operand: [0.0, 1.0, 8.0, 27.0]
%result = "stablehlo.cbrt"(%operand) : (tensor&lt;4xf64&gt;) -&gt; tensor&lt;4xf64&gt;
// %result: [0.0, 1.0, 2.0, 3.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/cbrt.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="ceil">ceil<a class="anchor" aria-label="anchor" href="#ceil"></a></h3>
<div class="section level4">
<h4 id="semantics-15">Semantics<a class="anchor" aria-label="anchor" href="#semantics-15"></a></h4>
<p>Performs element-wise ceil of <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTowardPositive</code> operation from the IEEE-754 specification. For quantized types, performs <code>dequantize_op_quantize(ceil, operand, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-15">Inputs<a class="anchor" aria-label="anchor" href="#inputs-15"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="7%"><col width="11%"><col width="66%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-15">Outputs<a class="anchor" aria-label="anchor" href="#outputs-15"></a></h4>
<table class="table"><colgroup><col width="11%"><col width="72%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-14">Constraints<a class="anchor" aria-label="anchor" href="#constraints-14"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-15">Examples<a class="anchor" aria-label="anchor" href="#examples-15"></a></h4>
<pre class="mlir"><code>// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0]
%result = "stablehlo.ceil"(%operand) : (tensor&lt;5xf32&gt;) -&gt; tensor&lt;5xf32&gt;
// %result: [-0.0, -0.0, 1.0, 1.0, 2.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/ceil.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="cholesky">cholesky<a class="anchor" aria-label="anchor" href="#cholesky"></a></h3>
<div class="section level4">
<h4 id="semantics-16">Semantics<a class="anchor" aria-label="anchor" href="#semantics-16"></a></h4>
<p>Computes the Cholesky decomposition of a batch of matrices.</p>
<p>More formally, for all <code>i</code> in <code>index_space(result)</code>, <code>result[i0, ..., iR-3, :, :]</code> is a Cholesky decomposition of <code>a[i0, ..., iR-3, :, :]</code>, in the form of either of a lower-triangular (if <code>lower</code> is <code>true</code>) or upper-triangular (if <code>lower</code> is <code>false</code>) matrix. The output values in the opposite triangle, i.e. the strict upper triangle or strict lower triangle correspondingly, are implementation-defined.</p>
<p>If there exists <code>i</code> where the input matrix is not an Hermitian positive-definite matrix, then the behavior is undefined.</p>
<p>For quantized types, performs <code>dequantize_op_quantize(lambda operand: cholesky(operand, lower), a, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-16">Inputs<a class="anchor" aria-label="anchor" href="#inputs-16"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="8%"><col width="71%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>a</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1-C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>lower</code></td>
<td>0-dimensional tensor constant of type <code>i1</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-16">Outputs<a class="anchor" aria-label="anchor" href="#outputs-16"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-15">Constraints<a class="anchor" aria-label="anchor" href="#constraints-15"></a></h4>
<ul><li>(C1) <code>baseline_type(a) = baseline_type(result)</code>.</li>
<li>(C2) <code>2 &lt;= rank(a)</code>.</li>
<li>(C3) <code>dim(a, -2) = dim(a, -1)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-16">Examples<a class="anchor" aria-label="anchor" href="#examples-16"></a></h4>
<pre class="mlir"><code>// %a: [
//      [1.0, 2.0, 3.0],
//      [2.0, 20.0, 26.0],
//      [3.0, 26.0, 70.0]
//     ]
%result = "stablehlo.cholesky"(%a) {
  lower = true
} : (tensor&lt;3x3xf32&gt;) -&gt; tensor&lt;3x3xf64&gt;
// %result: [
//           [1.0, 0.0, 0.0],
//           [2.0, 4.0, 0.0],
//           [3.0, 5.0, 6.0]
//          ]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="clamp">clamp<a class="anchor" aria-label="anchor" href="#clamp"></a></h3>
<div class="section level4">
<h4 id="semantics-17">Semantics<a class="anchor" aria-label="anchor" href="#semantics-17"></a></h4>
<p>Clamps every element of the <code>operand</code> tensor between a minimum and maximum value and produces a <code>result</code> tensor. More formally, <code>result[result_index] = minimum(maximum(operand[result_index], min_element), max_element)</code>, where <code>min_element = rank(min) = 0 ? min[] : min[result_index]</code>, <code>max_element = rank(max) = 0 ? max[] : max[result_index]</code>. For quantized types, performs <code>dequantize_op_quantize(clamp, min, operand, max, type(result))</code>.</p>
<p>Imposing an ordering on complex numbers involves surprising semantics, so in the future we are planning to remove support for complex numbers for this operation (<a href="https://github.com/openxla/stablehlo/issues/560" class="external-link">#560</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-17">Inputs<a class="anchor" aria-label="anchor" href="#inputs-17"></a></h4>
<table class="table"><colgroup><col width="10%"><col width="15%"><col width="55%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>min</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C4)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>max</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C2), (C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-17">Outputs<a class="anchor" aria-label="anchor" href="#outputs-17"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-16">Constraints<a class="anchor" aria-label="anchor" href="#constraints-16"></a></h4>
<ul><li>(C1) <code>rank(min) = 0 or shape(min) = shape(operand)</code>.</li>
<li>(C2) <code>rank(max) = 0 or shape(max) = shape(operand)</code>.</li>
<li>(C3) <code>baseline_element_type(min) = baseline_element_type(operand) = baseline_element_type(max)</code>.</li>
<li>(C4) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-17">Examples<a class="anchor" aria-label="anchor" href="#examples-17"></a></h4>
<pre class="mlir"><code>// %min: [5, 10, 15]
// %operand: [3, 13, 23]
// %max: [10, 15, 20]
%result = "stablehlo.clamp"(%min, %operand, %max) : (tensor&lt;3xi32&gt;, tensor&lt;3xi32&gt;, tensor&lt;3xi32&gt;) -&gt; tensor&lt;3xi32&gt;
// %result: [5, 13, 20]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/clamp.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="collective_broadcast">collective_broadcast<a class="anchor" aria-label="anchor" href="#collective_broadcast"></a></h3>
<div class="section level4">
<h4 id="semantics-18">Semantics<a class="anchor" aria-label="anchor" href="#semantics-18"></a></h4>
<p>Within each process group in the StableHLO process grid, send the value of the <code>operand</code> tensor from the source process to the target processes and produce a <code>result</code> tensor.</p>
<p>The operation splits the StableHLO process grid into <code>process_groups</code> which is defined as follows:</p>
<ul><li>
<code>cross_replica(replica_groups)</code> if <code>channel_id &lt;= 0</code>.</li>
<li>
<code>cross_partition(replica_groups)</code> if <code>channel_id &gt; 0</code>.</li>
</ul><p>Afterwards, <code>result@process</code> is given by:</p>
<ul><li>
<code>operand@process_groups[i, 0]</code> if there exists an <code>i</code> such that the process is in <code>process_groups[i]</code>.</li>
<li>
<code>broadcast_in_dim(constant(is_quantized(result) ? quantize(0,   element_type(result)) : 0, element_type(result)), [], type(result))</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-18">Inputs<a class="anchor" aria-label="anchor" href="#inputs-18"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="17%"><col width="63%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>replica_groups</code></td>
<td>variadic number of 1-dimensional tensor constants of type <code>si64</code>
</td>
<td>(C1), (C2)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-18">Outputs<a class="anchor" aria-label="anchor" href="#outputs-18"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-17">Constraints<a class="anchor" aria-label="anchor" href="#constraints-17"></a></h4>
<ul><li>(C1) <code>is_unique(replica_groups)</code>.</li>
<li>(C2) <code>0 &lt;= replica_groups &lt; N</code> where <code>N</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_partitions</code> if <code>cross_partition</code> is used.</li>
</ul></li>
<li>(C3) <code>type(result) = type(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-18">Examples<a class="anchor" aria-label="anchor" href="#examples-18"></a></h4>
<pre class="mlir"><code>// num_replicas: 4
// num_partitions: 1
// %operand@(0, 0): [[1, 2]]
// %operand@(1, 0): [[3, 4]]
// %operand@(2, 0): [[5, 6]]
// %operand@(3, 0): [[7, 8]]
%result = "stablehlo.collective_broadcast"(%operand) {
  replica_groups = dense&lt;[[2, 1]]&gt; : tensor&lt;1x2xi64&gt;,
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;
} : (tensor1x2xi64&gt;) -&gt; tensor&lt;1x2xi64&gt;
// %result@(0, 0): [[0, 0]]
// %result@(1, 0): [[5, 6]]
// %result@(2, 0): [[5, 6]]
// %result@(3, 0): [[0, 0]]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="collective_permute">collective_permute<a class="anchor" aria-label="anchor" href="#collective_permute"></a></h3>
<div class="section level4">
<h4 id="semantics-19">Semantics<a class="anchor" aria-label="anchor" href="#semantics-19"></a></h4>
<p>Within each process group in the StableHLO process grid, sends the value of the <code>operand</code> tensor from the source process to the target process and produces a <code>result</code> tensor.</p>
<p>The operation splits the StableHLO process grid into <code>process_groups</code> which is defined as follows:</p>
<ul><li>
<code>cross_replica(source_target_pairs)</code> if <code>channel_id &lt;= 0</code>.</li>
<li>
<code>cross_partition(source_target_pairs)</code> if <code>channel_id &gt; 0</code>.</li>
</ul><p>Afterwards, <code>result@process</code> is given by:</p>
<ul><li>
<code>operand@process_groups[i, 0]</code>, if there exists an <code>i</code> such that <code>process_groups[i, 1] = process</code>.</li>
<li>
<code>broadcast_in_dim(constant(is_quantized(result) ? quantize(0,   element_type(result)) : 0, element_type(result)), [], type(result))</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-19">Inputs<a class="anchor" aria-label="anchor" href="#inputs-19"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="25%"><col width="51%"><col width="14%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C5)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>source_target_pairs</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1-C4)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-19">Outputs<a class="anchor" aria-label="anchor" href="#outputs-19"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-18">Constraints<a class="anchor" aria-label="anchor" href="#constraints-18"></a></h4>
<ul><li>(C1) <code>dim(source_target_pairs, 1) = 2</code>.</li>
<li>(C2) <code>is_unique(source_target_pairs[:, 0])</code>.</li>
<li>(C3) <code>is_unique(source_target_pairs[:, 1])</code>.</li>
<li>(C4) <code>0 &lt;= source_target_pairs &lt; N</code>, where <code>N</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_partitions</code> if <code>cross_partition</code> is used.</li>
</ul></li>
<li>(C5) <code>type(result) = type(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-19">Examples<a class="anchor" aria-label="anchor" href="#examples-19"></a></h4>
<pre class="mlir"><code>// num_replicas: 3
// num_partitions: 1
// %operand@(0, 0): [[1, 2], [3, 4]]
// %operand@(1, 0): [[5, 6], [7, 8]]
// %operand@(2, 0): [[9, 10], [11, 12]]
%result = "stablehlo.collective_permute"(%operand) {
  source_target_pairs = dense&lt;[[0, 1], [1, 2]]&gt; : tensor&lt;2x2xi64&gt;,
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;
} : (tensor&lt;2x2xi64&gt;) -&gt; tensor&lt;2x2xi64&gt;
//
// %result@(0, 0): [[0, 0], [0, 0]]
// %result@(1, 0): [[1, 2], [3, 4]]
// %result@(2, 0): [[5, 6], [7, 8]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/collective_permute.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="compare">compare<a class="anchor" aria-label="anchor" href="#compare"></a></h3>
<div class="section level4">
<h4 id="semantics-20">Semantics<a class="anchor" aria-label="anchor" href="#semantics-20"></a></h4>
<p>Performs element-wise comparison of <code>lhs</code> and <code>rhs</code> tensors according to <code>comparison_direction</code> and <code>compare_type</code>, and produces a <code>result</code> tensor.</p>
<p>The values of <code>comparison_direction</code> and <code>compare_type</code> have the following semantics:</p>
<p>For boolean and integer element types:</p>
<ul><li>
<code>EQ</code>: <code>lhs = rhs</code>.</li>
<li>
<code>NE</code>: <code>lhs != rhs</code>.</li>
<li>
<code>GE</code>: <code>lhs &gt;= rhs</code>.</li>
<li>
<code>GT</code>: <code>lhs &gt; rhs</code>.</li>
<li>
<code>LE</code>: <code>lhs &lt;= rhs</code>.</li>
<li>
<code>LT</code>: <code>lhs &lt; rhs</code>.</li>
</ul><p>For floating-point element types with <code>compare_type = FLOAT</code>, the op implements the following IEEE-754 operations:</p>
<ul><li>
<code>EQ</code>: <code>compareQuietEqual</code>.</li>
<li>
<code>NE</code>: <code>compareQuietNotEqual</code>.</li>
<li>
<code>GE</code>: <code>compareQuietGreaterEqual</code>.</li>
<li>
<code>GT</code>: <code>compareQuietGreater</code>.</li>
<li>
<code>LE</code>: <code>compareQuietLessEqual</code>.</li>
<li>
<code>LT</code>: <code>compareQuietLess</code>.</li>
</ul><p>For floating-point element types with <code>compare_type = TOTALORDER</code>, the op uses the combination of <code>totalOrder</code> and <code>compareQuietEqual</code> operations from IEEE-754.</p>
<p>For complex element types, lexicographic comparison of <code>(real, imag)</code> pairs is performed using the provided <code>comparison_direction</code> and <code>compare_type</code>. Imposing an ordering on complex numbers involves surprising semantics, so in the future we are planning to remove support for complex numbers when <code>comparison_direction</code> is <code>GE</code>, <code>GT</code>, <code>LE</code> or <code>LT</code> (<a href="https://github.com/openxla/stablehlo/issues/560" class="external-link">#560</a>).</p>
<p>For quantized types. performs <code>dequantize_compare(lhs, rhs, comparison_direction)</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-20">Inputs<a class="anchor" aria-label="anchor" href="#inputs-20"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="6%"><col width="23%"><col width="56%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C2)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>comparison_direction</code></td>
<td>enum of <code>EQ</code>, <code>NE</code>, <code>GE</code>, <code>GT</code>, <code>LE</code>, and <code>LT</code>
</td>
<td></td>
</tr><tr class="even"><td>(I4)</td>
<td><code>compare_type</code></td>
<td>enum of <code>FLOAT</code>, <code>TOTALORDER</code>, <code>SIGNED</code>, and <code>UNSIGNED</code>
</td>
<td>(C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-20">Outputs<a class="anchor" aria-label="anchor" href="#outputs-20"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of boolean type</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-19">Constraints<a class="anchor" aria-label="anchor" href="#constraints-19"></a></h4>
<ul><li>(C1) <code>baseline_element_type(lhs) = baseline_element_type(rhs)</code>.</li>
<li>(C2) <code>shape(lhs) = shape(rhs) = shape(result)</code>.</li>
<li>(C3) <code>compare_type</code> is defined as:
<ul><li>
<code>SIGNED</code> if <code>is_signed_integer(element_type(lhs))</code>.</li>
<li>
<code>UNSIGNED</code> if <code>is_unsigned_integer(element_type(lhs)) or   is_boolean(element_type(lhs))</code>.</li>
<li>
<code>FLOAT</code> or <code>TOTALORDER</code> if <code>is_float(element_type(lhs))</code>.</li>
<li>
<code>FLOAT</code> if <code>is_complex(element_type(lhs))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-20">Examples<a class="anchor" aria-label="anchor" href="#examples-20"></a></h4>
<pre class="mlir"><code>// %lhs: [1.0, 3.0]
// %rhs: [1.1, 2.9]
%result = "stablehlo.compare"(%lhs, %rhs) {
  comparison_direction = #stablehlo&lt;comparison_direction LT&gt;,
  compare_type = #stablehlo&lt;comparison_type FLOAT&gt;
} : (tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;) -&gt; tensor&lt;2xi1&gt;
// %result: [true, false]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/compare.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="complex">complex<a class="anchor" aria-label="anchor" href="#complex"></a></h3>
<div class="section level4">
<h4 id="semantics-21">Semantics<a class="anchor" aria-label="anchor" href="#semantics-21"></a></h4>
<p>Performs element-wise conversion to a complex value from a pair of real and imaginary values, <code>lhs</code> and <code>rhs</code>, and produces a <code>result</code> tensor.</p>
</div>
<div class="section level4">
<h4 id="inputs-21">Inputs<a class="anchor" aria-label="anchor" href="#inputs-21"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of type <code>f32</code> or <code>f64</code>
</td>
<td>(C1-C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of type <code>f32</code> or <code>f64</code>
</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-21">Outputs<a class="anchor" aria-label="anchor" href="#outputs-21"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of complex type</td>
<td>(C2), (C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-20">Constraints<a class="anchor" aria-label="anchor" href="#constraints-20"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs)</code>.</li>
<li>(C2) <code>shape(result) = shape(lhs)</code>.</li>
<li>(C3) <code>element_type(result)</code> has type <code>complex&lt;E&gt;</code> where <code>E = element_type(lhs)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-21">Examples<a class="anchor" aria-label="anchor" href="#examples-21"></a></h4>
<pre class="mlir"><code>// %lhs: [1.0, 3.0]
// %rhs: [2.0, 4.0]
%result = "stablehlo.complex"(%lhs, %rhs) : (tensor&lt;2xf64&gt;, tensor&lt;2xf64&gt;) -&gt; tensor&lt;2xcomplex&lt;f64&gt;&gt;
// %result: [(1.0, 2.0), (3.0, 4.0)]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/complex.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="composite">composite<a class="anchor" aria-label="anchor" href="#composite"></a></h3>
<div class="section level4">
<h4 id="semantics-22">Semantics<a class="anchor" aria-label="anchor" href="#semantics-22"></a></h4>
<p>Encapsulates an operation made up (composed) of other StableHLO operations, taking <code>inputs</code> and <code>composite_attributes</code> and producing <code>results</code>. The semantics of the op are implemented by the <code>decomposition</code> attribute. The <code>composite</code> op can be replaced with its decomposition without changing program semantics. In cases where inlining the decomposition does not provide the same op semantics, prefer using <code>custom_call</code>.</p>
<p>The <code>version</code> field (defaults to <code>0</code>) is used to denote when a composite’s semantics change.</p>
</div>
<div class="section level4">
<h4 id="inputs-22">Inputs<a class="anchor" aria-label="anchor" href="#inputs-22"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of values</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>name</code></td>
<td>constant of type <code>string</code>
</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>composite_attributes</code></td>
<td>attribute dictionary</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>decomposition</code></td>
<td>constant of type <code>string</code>
</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>version</code></td>
<td>constant of type <code>si32</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-22">Outputs<a class="anchor" aria-label="anchor" href="#outputs-22"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of values</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-21">Constraints<a class="anchor" aria-label="anchor" href="#constraints-21"></a></h4>
<ul><li>(C1) <code>is_namespaced_op_name(name)</code>
</li>
<li>(C2) <code>is_defined_in_parent_scope(decomposition)</code>
</li>
<li>(C3) <code>types(inputs...) == input_types(decomposition)</code>
</li>
<li>(C4) <code>types(results...) == output_types(decomposition)</code>
</li>
</ul></div>
<div class="section level4">
<h4 id="examples-22">Examples<a class="anchor" aria-label="anchor" href="#examples-22"></a></h4>
<pre class="mlir"><code>%results = "stablehlo.composite"(%input0, %input1) {
  name = "my_namespace.my_op",
  composite_attributes = {
    my_attribute = "my_value"
  },
  decomposition = @my_op,
  version = 1 : i32
} : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/composite.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="concatenate">concatenate<a class="anchor" aria-label="anchor" href="#concatenate"></a></h3>
<div class="section level4">
<h4 id="semantics-23">Semantics<a class="anchor" aria-label="anchor" href="#semantics-23"></a></h4>
<p>Concatenates <code>inputs</code> along <code>dimension</code> dimension in the same order as the given arguments and produces a <code>result</code> tensor. More formally, <code>result[i0, ..., id, ..., iR-1] = inputs[k][i0, ..., kd, ..., iR-1]</code>, where:</p>
<ol style="list-style-type: decimal"><li>
<code>id = d0 + ... + dk-1 + kd</code>.</li>
<li>
<code>d</code> is equal to <code>dimension</code>, and <code>d0</code>, … are <code>d</code>th dimension sizes of <code>inputs</code>.</li>
</ol></div>
<div class="section level4">
<h4 id="inputs-23">Inputs<a class="anchor" aria-label="anchor" href="#inputs-23"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="13%"><col width="61%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1-C6)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-23">Outputs<a class="anchor" aria-label="anchor" href="#outputs-23"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C5-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-22">Constraints<a class="anchor" aria-label="anchor" href="#constraints-22"></a></h4>
<ul><li>(C1) <code>same(element_type(inputs...))</code>.</li>
<li>(C2) <code>same(shape(inputs...))</code> except for <code>dim(inputs..., dimension)</code>.</li>
<li>(C3) <code>0 &lt; size(inputs)</code>.</li>
<li>(C4) <code>0 &lt;= dimension &lt; rank(inputs[0])</code>.</li>
<li>(C5) <code>element_type(result) = element_type(inputs[0])</code>.</li>
<li>(C6) <code>shape(result) = shape(inputs[0])</code> except for:
<ul><li>
<code>dim(result, dimension) = dim(inputs[0], dimension) + ...</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-23">Examples<a class="anchor" aria-label="anchor" href="#examples-23"></a></h4>
<pre class="mlir"><code>// %input0: [[1, 2], [3, 4], [5, 6]]
// %input1: [[7, 8]]
%result = "stablehlo.concatenate"(%input0, %input1) {
  dimension = 0 : i64
} : (tensor&lt;3x2xi64&gt;, tensor&lt;1x2xi64&gt;) -&gt; tensor&lt;4x2xi64&gt;
// %result: [[1, 2], [3, 4], [5, 6], [7, 8]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/concatenate.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="constant">constant<a class="anchor" aria-label="anchor" href="#constant"></a></h3>
<div class="section level4">
<h4 id="semantics-24">Semantics<a class="anchor" aria-label="anchor" href="#semantics-24"></a></h4>
<p>Produces an <code>output</code> tensor from a constant <code>value</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-24">Inputs<a class="anchor" aria-label="anchor" href="#inputs-24"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>value</code></td>
<td>constant</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-24">Outputs<a class="anchor" aria-label="anchor" href="#outputs-24"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>output</code></td>
<td>tensor or quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-23">Constraints<a class="anchor" aria-label="anchor" href="#constraints-23"></a></h4>
<ul><li>(C1) <code>type(value) = type(output)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-24">Examples<a class="anchor" aria-label="anchor" href="#examples-24"></a></h4>
<pre class="mlir"><code>%output = "stablehlo.constant"() {
  value = dense&lt;[[0.0, 1.0], [2.0, 3.0]]&gt; : tensor&lt;2x2xf32&gt;
} : () -&gt; tensor&lt;2x2xf32&gt;
// %output: [[0.0, 1.0], [2.0, 3.0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/constant.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="convert">convert<a class="anchor" aria-label="anchor" href="#convert"></a></h3>
<div class="section level4">
<h4 id="semantics-25">Semantics<a class="anchor" aria-label="anchor" href="#semantics-25"></a></h4>
<p>Performs an element-wise conversion from one element type to another on <code>operand</code> tensor and produces a <code>result</code> tensor.</p>
<p>For <strong>boolean-to-any-supported-type</strong> conversions, the value <code>false</code> is converted to zero, and the value <code>true</code> is converted to one. For <strong>any-supported-type-to-boolean</strong> conversions, a zero value is converted to <code>false</code>, and non-zero values are converted to <code>true</code>. See below for how this work for complex types.</p>
<p>For conversions involving <strong>integer-to-integer</strong>, <strong>integer-to-floating-point</strong> or <strong>floating-point-to-floating-point</strong>, if the source value can be exactly represented in the destination type, the result value is that exact representation. Otherwise, the behavior is TBD (<a href="https://github.com/openxla/stablehlo/issues/180" class="external-link">#180</a>).</p>
<p>For conversions involving <strong>floating-point-to-integer</strong>, the fractional part is truncated. If the truncated value cannot be represented in the destination type, the behavior is TBD (<a href="https://github.com/openxla/stablehlo/issues/180" class="external-link">#180</a>).</p>
<p>Conversion involving <strong>complex-to-complex</strong> follow the same behavior of <strong>floating-point-to-floating-point</strong> conversions for converting real and imaginary parts.</p>
<p>For <strong>complex-to-any-other-type</strong> and <strong>any-other-type-to-complex</strong> conversions, the source imaginary value is ignored or the destination imaginary value is zeroed, respectively. The conversion of the real part follows the floating-point conversions.</p>
<p>In principle, this operation could express dequantization (conversion from quantized tensors to regular tensors), quantization (conversion from regular tensors to quantized tensors) and requantization (conversion between quantized tensors), but at the moment we have dedicated operations for that - <code>uniform_dequantize</code> for the first use case and <code>uniform_quantize</code> for the second and the third use cases. In the future, these two ops may be merged into <code>convert</code> (<a href="https://github.com/openxla/stablehlo/issues/1576" class="external-link">#1576</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-25">Inputs<a class="anchor" aria-label="anchor" href="#inputs-25"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-25">Outputs<a class="anchor" aria-label="anchor" href="#outputs-25"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-24">Constraints<a class="anchor" aria-label="anchor" href="#constraints-24"></a></h4>
<ul><li>(C1) <code>shape(operand) = shape(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-25">Examples<a class="anchor" aria-label="anchor" href="#examples-25"></a></h4>
<pre class="mlir"><code>// %operand: [-1, 0, 1]
%result = "stablehlo.convert"(%operand) : (tensor&lt;3xi64&gt;) -&gt; tensor&lt;3xcomplex&lt;f64&gt;&gt;
// %result: [(-1.0, 0.0), (0.0, 0.0), (1.0, 0.0)]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/convert.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="convolution">convolution<a class="anchor" aria-label="anchor" href="#convolution"></a></h3>
<div class="section level4">
<h4 id="semantics-26">Semantics<a class="anchor" aria-label="anchor" href="#semantics-26"></a></h4>
<p>Computes dot products between windows of <code>lhs</code> and slices of <code>rhs</code> and produces <code>result</code>. The following diagram shows how elements in <code>result</code> are computed from <code>lhs</code> and <code>rhs</code> using a concrete example.</p>
<div class="float">
<img src="images/spec/convolution.svg" alt="convolution"><div class="figcaption">convolution</div>
</div>
<p>More formally, consider the following reframing of the inputs in terms of <code>lhs</code> in order to be able to express windows of <code>lhs</code>:</p>
<!-- markdownlint-disable line-length -->
<ul><li>
<code>lhs_window_dimensions = lhs_shape(dim(lhs, input_batch_dimension), dim(rhs, kernel_spatial_dimensions), dim(lhs, input_feature_dimension))</code>.</li>
<li>
<code>lhs_window_strides = lhs_shape(1, window_strides, 1)</code>.</li>
<li>
<code>lhs_padding = lhs_shape([0, 0], padding, [0, 0])</code>.</li>
<li>
<code>lhs_base_dilations = lhs_shape(1, lhs_dilation, 1)</code>.</li>
<li>
<code>lhs_window_dilations = lhs_shape(1, rhs_dilation, 1)</code>.</li>
</ul><p>This reframing uses the following helper functions:</p>
<ul><li>
<code>lhs_shape(n, hw, c) = permute([n] + hw + [c], [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension])</code>.</li>
<li>
<code>result_shape(n1, hw, c1) = permute([n1] + hw + [c1], [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension])</code>.</li>
<li>
<code>permute([j0, j1, ..., jR-1], permutation) = [i0, i1, ..., iR-1]</code> where <code>j[d] = i[permutation[d]]</code>.</li>
</ul><p>If <code>feature_group_count = 1</code> and <code>batch_group_count = 1</code>, then for all <code>output_spatial_index</code> in <code>index_space(dim(result, output_spatial_dimensions...))</code>, <code>result[result_shape(:, output_spatial_index, :)] = dot_product</code> where:</p>
<ul><li>
<code>padding_value = constant(0, element_type(lhs))</code>.</li>
<li>
<code>padded_lhs = pad(lhs, padding_value, lhs_padding[:, 0], lhs_padding[:, 1], lhs_base_dilations - 1)</code>.</li>
<li>
<code>lhs_window_start = lhs_shape(0, output_spatial_index, 0) * lhs_window_strides</code>.</li>
<li>
<code>lhs_window = slice(padded_lhs, lhs_window_start, lhs_window_start + lhs_window_dimensions, lhs_window_dilations)</code>.</li>
<li>
<code>reversed_lhs_window = reverse(lhs_window, [input_spatial_dimensions[dim] for dim in range(size(window_reversal)) if window_reversal[dim] = true])</code>. This feature appears to be unused, so in the future we are planning to remove it (<a href="https://github.com/openxla/stablehlo/issues/1181" class="external-link">#1181</a>).</li>
<li>
<code>dot_product = dot_general(reversed_lhs_window, rhs,     lhs_batching_dimensions=[],     lhs_contracting_dimensions=input_spatial_dimensions + [input_feature_dimension],     rhs_batching_dimensions=[],     rhs_contracting_dimensions=kernel_spatial_dimensions + [kernel_input_feature_dimension])</code>.</li>
</ul><p>If <code>feature_group_count &gt; 1</code>:</p>
<ul><li>
<code>lhses = split(lhs, feature_group_count, input_feature_dimension)</code>.</li>
<li>
<code>rhses = split(rhs, feature_group_count, kernel_output_feature_dimension)</code>.</li>
<li>
<code>results... = convolution(lhses..., rhses..., ..., feature_group_count=1, ...)</code>.</li>
<li>
<code>result = concatenate(results, output_feature_dimension)</code>.</li>
</ul><p>If <code>batch_group_count &gt; 1</code>:</p>
<ul><li>
<code>lhses = split(lhs, batch_group_count, input_batch_dimension)</code>.</li>
<li>
<code>rhses = split(rhs, batch_group_count, kernel_output_feature_dimension)</code>.</li>
<li>
<code>results... = convolution(lhses..., rhses..., ..., batch_group_count=1, ...)</code>.</li>
<li>
<code>result = concatenate(results, output_feature_dimension)</code>. <!-- markdownlint-enable line-length -->
</li>
</ul><p>For quantized types, performs <code>dequantize_op_quantize(     lambda lhs, rhs: convolution(lhs, rhs, window_strides, padding,         lhs_dilation, rhs_dilation, window_reversal, input_batch_dimension,         input_feature_dimension, input_spatial_dimensions,         kernel_input_feature_dimension, kernel_output_feature_dimension,         kernel_spatial_dimensions, output_batch_dimension,         output_feature_dimension, output_spatial_dimensions,         feature_group_count, batch_group_count, precision_config), lhs, rhs,         type(result))</code>.</p>
<p>For hybrid quantized types, performs <code>hybrid_dequantize_then_op(     lambda lhs, rhs: convolution(lhs, rhs, window_strides, padding,         lhs_dilation, rhs_dilation, window_reversal, input_batch_dimension,         input_feature_dimension, input_spatial_dimensions,         kernel_input_feature_dimension, kernel_output_feature_dimension,         kernel_spatial_dimensions, output_batch_dimension,         output_feature_dimension, output_spatial_dimensions,         feature_group_count, batch_group_count, precision_config), lhs, rhs)</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-26">Inputs<a class="anchor" aria-label="anchor" href="#inputs-26"></a></h4>
<table class="table"><colgroup><col width="4%"><col width="21%"><col width="38%"><col width="36%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C10-C11), (C14) (C25), (C27-C28), (C31-C32), (C34)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or quantized tensor</td>
<td>(C1), (C14-C16), (C25), (C27-C29), (C31-C34)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>window_strides</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2-C3), (C25)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>padding</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C4), (C25)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>lhs_dilation</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C5-C6), (C25)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>rhs_dilation</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C7-C8), (C25)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>window_reversal</code></td>
<td>1-dimensional tensor constant of type <code>i1</code>
</td>
<td>(C9)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>input_batch_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C10), (C13), (C25)</td>
</tr><tr class="odd"><td>(I9)</td>
<td><code>input_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C11), (C13-C14)</td>
</tr><tr class="even"><td>(I10)</td>
<td><code>input_spatial_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C12), (C13), (C25)</td>
</tr><tr class="odd"><td>(I11)</td>
<td><code>kernel_input_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C14), (C18)</td>
</tr><tr class="even"><td>(I12)</td>
<td><code>kernel_output_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C15-C16), (C18), (C25), (C29)</td>
</tr><tr class="odd"><td>(I13)</td>
<td><code>kernel_spatial_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C17-C18), (C25)</td>
</tr><tr class="even"><td>(I14)</td>
<td><code>output_batch_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C20), (C25)</td>
</tr><tr class="odd"><td>(I15)</td>
<td><code>output_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C20), (C25), (C30)</td>
</tr><tr class="even"><td>(I16)</td>
<td><code>output_spatial_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C19-C20), (C25)</td>
</tr><tr class="odd"><td>(I17)</td>
<td><code>feature_group_count</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C11), (C14), (C16), (C21), (C23)</td>
</tr><tr class="even"><td>(I18)</td>
<td><code>batch_group_count</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C10), (C15), (C22), (C23), (C25)</td>
</tr><tr class="odd"><td>(I19)</td>
<td><code>precision_config</code></td>
<td>variadic number of enums of <code>DEFAULT</code>, <code>HIGH</code>, and <code>HIGHEST</code>
</td>
<td>(C24)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-26">Outputs<a class="anchor" aria-label="anchor" href="#outputs-26"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C25-C28), (C30), (C32-34)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-25">Constraints<a class="anchor" aria-label="anchor" href="#constraints-25"></a></h4>
<!-- markdownlint-disable line-length -->
<ul><li>(C1) <code>N = rank(lhs) = rank(rhs)</code>.</li>
<li>(C2) <code>size(window_strides) = N - 2</code>.</li>
<li>(C3) <code>0 &lt; window_strides</code>.</li>
<li>(C4) <code>shape(padding) = [N - 2, 2]</code>.</li>
<li>(C5) <code>size(lhs_dilation) = N - 2</code>.</li>
<li>(C6) <code>0 &lt; lhs_dilation</code>.</li>
<li>(C7) <code>size(rhs_dilation) = N - 2</code>.</li>
<li>(C8) <code>0 &lt; rhs_dilation</code>.</li>
<li>(C9) <code>size(window_reversal) = N - 2</code>.</li>
<li>(C10) <code>dim(lhs, input_batch_dimension) % batch_group_count = 0</code>.</li>
<li>(C11) <code>dim(lhs, input_feature_dimension) % feature_group_count = 0</code>.</li>
<li>(C12) <code>size(input_spatial_dimensions) = N - 2</code>.</li>
<li>(C13) Given <code>input_dimensions = [input_batch_dimension] +        input_spatial_dimensions + [input_feature_dimension]</code>:
<ul><li>
<code>is_unique(input_dimensions)</code>.</li>
<li>
<code>0 &lt;= input_dimensions &lt; N</code>.</li>
</ul></li>
<li>(C14) <code>dim(rhs, kernel_input_feature_dimension) = dim(lhs, input_feature_dimension) / feature_group_count</code>.</li>
<li>(C15) <code>dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0</code>.</li>
<li>(C16) <code>dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0</code>.</li>
<li>(C17) <code>size(kernel_spatial_dimensions) = N - 2</code>.</li>
<li>(C18) Given <code>kernel_dimensions = kernel_spatial_dimensions +         [kernel_input_feature_dimension] + [kernel_output_feature_dimension]</code>:
<ul><li>
<code>is_unique(kernel_dimensions)</code>.</li>
<li>
<code>0 &lt;= kernel_dimensions &lt; N</code>.</li>
</ul></li>
<li>(C19) <code>size(output_spatial_dimensions) = N - 2</code>.</li>
<li>(C20) Given <code>output_dimensions = [output_batch_dimension] +         output_spatial_dimensions + [output_feature_dimension]</code>:
<ul><li>
<code>is_unique(output_dimensions)</code>.</li>
<li>
<code>0 &lt;= output_dimensions &lt; N</code>.</li>
</ul></li>
<li>(C21) <code>0 &lt; feature_group_count</code>.</li>
<li>(C22) <code>0 &lt; batch_group_count</code>.</li>
<li>(C23) <code>feature_group_count = 1 or batch_group_count = 1</code>.</li>
<li>(C24) <code>size(precision_config) = 2</code>.</li>
<li>(C25) <code>dim(result, result_dim)</code> is defined as:
<ul><li>
<code>dim(lhs, input_batch_dimension) / batch_group_count</code> if <code>result_dim = output_batch_dimension</code>.</li>
<li>
<code>dim(rhs, kernel_output_feature_dimension)</code> if <code>result_dim = output_feature_dimension</code>.</li>
<li>
<code>num_windows</code> otherwise, where:
<ul><li>
<code>output_spatial_dimensions[spatial_dim] = result_dim</code>.</li>
<li>
<code>lhs_dim = input_spatial_dimensions[spatial_dim]</code>.</li>
<li>
<code>rhs_dim = kernel_spatial_dimensions[spatial_dim]</code>.</li>
<li>
<code>dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) = 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1</code>.</li>
<li>
<code>padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]</code>.</li>
<li>
<code>dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) = 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1</code>.</li>
<li>
<code>is_empty_window[lhs_dim] = padded_input_shape[lhs_dim] = 0 || dilated_window_shape[lhs_dim] &gt; padded_input_shape[lhs_dim]</code>.</li>
<li>
<code>num_windows = is_empty_window[lhs_dim] ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1</code>.</li>
</ul></li>
</ul></li>
<li>(C26) <code>rank(result) = N</code>.</li>
<li>If the operation uses non-quantized tensors:
<ul><li>(C27) <code>element_type(lhs) = element_type(rhs) = element_type(result)</code>.</li>
</ul></li>
<li>If the operation uses quantized tensors:
<ul><li>(C28) <code>is_quantized(lhs) = is_quantized(result) and is_quantized(rhs)</code>.</li>
<li>(C29) If <code>is_per_axis_quantized(rhs)</code>, then <code>quantization_dimension(rhs) = kernel_output_feature_dimension</code>.</li>
<li>(C30) If <code>is_per_axis_quantized(result)</code>, then <code>quantization_dimension(result) = output_feature_dimension</code>.</li>
<li>If <code>is_quantized(lhs)</code>:
<ul><li>(C31) <code>storage_type(lhs) = storage_type(rhs)</code>.</li>
<li>(C32) <code>expressed_type(lhs) = expressed_type(rhs) = expressed_type(result)</code>.</li>
<li>(C33) If <code>is_per_tensor_quantized(rhs)</code>, then <code>is_per_tensor_quantized(result)</code>.</li>
</ul></li>
<li>If <code>!is_quantized(lhs)</code>:
<ul><li>(C34) <code>element_type(lhs) = expressed_type(rhs) = element_type(result)</code>. <!-- markdownlint-enable line-length -->
</li>
</ul></li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-26">Examples<a class="anchor" aria-label="anchor" href="#examples-26"></a></h4>
<pre class="mlir"><code>// %lhs: [[
//        [
//          [1], [2], [5], [6]
//        ],
//        [
//          [3], [4], [7], [8]
//        ],
//        [
//          [10], [11], [14], [15]
//        ],
//        [
//          [12], [13], [16], [17]
//        ]
//      ]]
//
// %rhs: [
//        [[[1]], [[1]], [[1]]],
//        [[[1]], [[1]], [[1]]],
//        [[[1]], [[1]], [[1]]]
//       ]
%result = "stablehlo.convolution"(%lhs, %rhs) {
  window_strides = array&lt;i64: 4, 4&gt;,
  padding = dense&lt;0&gt; : tensor&lt;2x2xi64&gt;,
  lhs_dilation = array&lt;i64: 2, 2&gt;,
  rhs_dilation = array&lt;i64: 1, 1&gt;,
  window_reversal = array&lt;i1: false, false&gt;,
  // In the StableHLO dialect, dimension numbers are encoded via:
  // `[&lt;input dimensions&gt;]x[&lt;kernel dimensions&gt;]-&gt;[output dimensions]`.
  // "b" is batch dimension, "f" is feature dimension,
  // "i" is input feature dimension, "o" is output feature dimension,
  // "0/1/etc" are spatial dimensions.
  dimension_numbers = #stablehlo.conv&lt;[b, 0, 1, f]x[0, 1, i, o]-&gt;[b, 0, 1, f]&gt;,
  batch_group_count = 1 : i64,
  feature_group_count = 1 : i64,
  precision_config = [#stablehlo&lt;precision DEFAULT&gt;, #stablehlo&lt;precision DEFAULT&gt;]
} : (tensor&lt;1x4x4x1xi64&gt;, tensor&lt;3x3x1x1xi64&gt;) -&gt; tensor&lt;1x2x2x1xi64&gt;
// %result: [[
//            [[10], [26]],
//            [[46], [62]]
//          ]]</code></pre>
<p> <a href="../stablehlo/tests/interpret/convolution.mlir">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="cosine">cosine<a class="anchor" aria-label="anchor" href="#cosine"></a></h3>
<div class="section level4">
<h4 id="semantics-27">Semantics<a class="anchor" aria-label="anchor" href="#semantics-27"></a></h4>
<p>Performs element-wise cosine operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>cos</code> from IEEE-754.</li>
<li>For complex numbers: complex cosine.</li>
<li>For quantized types: <code>dequantize_op_quantize(cosine, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-27">Inputs<a class="anchor" aria-label="anchor" href="#inputs-27"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-27">Outputs<a class="anchor" aria-label="anchor" href="#outputs-27"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-26">Constraints<a class="anchor" aria-label="anchor" href="#constraints-26"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-27">Examples<a class="anchor" aria-label="anchor" href="#examples-27"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [0.0, 1.57079632],       // [0, pi/2]
//            [3.14159265, 4.71238898] // [pi, 3pi/2]
//           ]
%result = "stablehlo.cosine"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;
// %result: [[1.0, 0.0], [-1.0, 0.0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/cosine.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="count_leading_zeros">count_leading_zeros<a class="anchor" aria-label="anchor" href="#count_leading_zeros"></a></h3>
<div class="section level4">
<h4 id="semantics-28">Semantics<a class="anchor" aria-label="anchor" href="#semantics-28"></a></h4>
<p>Performs element-wise count of the number of leading zero bits in the <code>operand</code> tensor and produces a <code>result</code> tensor.</p>
</div>
<div class="section level4">
<h4 id="inputs-28">Inputs<a class="anchor" aria-label="anchor" href="#inputs-28"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-28">Outputs<a class="anchor" aria-label="anchor" href="#outputs-28"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-27">Constraints<a class="anchor" aria-label="anchor" href="#constraints-27"></a></h4>
<ul><li>(C1) <code>type(operand) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-28">Examples<a class="anchor" aria-label="anchor" href="#examples-28"></a></h4>
<pre class="mlir"><code>// %operand: [[0, 1], [128, -1]]
%result = "stablehlo.count_leading_zeros"(%operand) : (tensor&lt;2x2xi64&gt;) -&gt; tensor&lt;2x2xi64&gt;
// %result: [[64, 63], [56, 0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/count_leading_zeros.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="custom_call">custom_call<a class="anchor" aria-label="anchor" href="#custom_call"></a></h3>
<div class="section level4">
<h4 id="semantics-29">Semantics<a class="anchor" aria-label="anchor" href="#semantics-29"></a></h4>
<p>Encapsulates an implementation-defined operation <code>call_target_name</code> that takes <code>inputs</code> and <code>called_computations</code> and produces <code>results</code>. <code>has_side_effect</code>, <code>backend_config</code> and <code>api_version</code> may be used to provide additional implementation-defined metadata.</p>
<p>At the moment, this operation contains a fairly disorganized collection of metadata which reflects organic evolution of its counterpart operation in the XLA compiler. In the future, we are planning to unify this metadata (<a href="https://github.com/openxla/stablehlo/issues/741" class="external-link">#741</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-29">Inputs<a class="anchor" aria-label="anchor" href="#inputs-29"></a></h4>
<table class="table"><colgroup><col width="8%"><col width="28%"><col width="62%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of values</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>call_target_name</code></td>
<td>constant of type <code>string</code>
</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>has_side_effect</code></td>
<td>constant of type <code>i1</code>
</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>backend_config</code></td>
<td>constant of type <code>string</code> or attribute dictionary</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>api_version</code></td>
<td>constant of type <code>si32</code>
</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>called_computations</code></td>
<td>variadic number of constants of type <code>string</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-29">Outputs<a class="anchor" aria-label="anchor" href="#outputs-29"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of values</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="examples-29">Examples<a class="anchor" aria-label="anchor" href="#examples-29"></a></h4>
<pre class="mlir"><code>%results = "stablehlo.custom_call"(%input0) {
  call_target_name = "foo",
  has_side_effect = false,
  backend_config = {bar = 42 : i32},
  api_version = 4 : i32,
  called_computations = [@foo]
} : (tensor&lt;f64&gt;) -&gt; tensor&lt;f64&gt;</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="divide">divide<a class="anchor" aria-label="anchor" href="#divide"></a></h3>
<div class="section level4">
<h4 id="semantics-30">Semantics<a class="anchor" aria-label="anchor" href="#semantics-30"></a></h4>
<p>Performs element-wise division of dividend <code>lhs</code> and divisor <code>rhs</code> tensors and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For integers: integer division which produces the algebraic quotient with any fractional part discarded.</li>
<li>For floats: <code>division</code> from IEEE-754.</li>
<li>For complex numbers: complex division.</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(divide, lhs, rhs, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-30">Inputs<a class="anchor" aria-label="anchor" href="#inputs-30"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="6%"><col width="75%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer, floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer, floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-30">Outputs<a class="anchor" aria-label="anchor" href="#outputs-30"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-28">Constraints<a class="anchor" aria-label="anchor" href="#constraints-28"></a></h4>
<ul><li>(C1) <code>baseline_type(lhs) = baseline_type(rhs) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-30">Examples<a class="anchor" aria-label="anchor" href="#examples-30"></a></h4>
<pre class="mlir"><code>// %lhs: [17.1, -17.1, 17.1, -17.1]
// %rhs: [3.0, 3.0, -3.0, -3.0]
%result = "stablehlo.divide"(%lhs, %rhs) : (tensor&lt;4xf32&gt;, tensor&lt;4xf32&gt;) -&gt; tensor&lt;4xf32&gt;
// %result: [5.66666651, -5.66666651, -5.66666651, 5.66666651]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/divide.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dot_general">dot_general<a class="anchor" aria-label="anchor" href="#dot_general"></a></h3>
<div class="section level4">
<h4 id="semantics-31">Semantics<a class="anchor" aria-label="anchor" href="#semantics-31"></a></h4>
<p>Computes dot products between slices of <code>lhs</code> and slices of <code>rhs</code> and produces a <code>result</code> tensor.</p>
<p>More formally, <code>result[result_index] = dot_product</code>, where:</p>
<!-- markdownlint-disable line-length -->
<ul><li>
<code>lhs_result_dimensions = [d for d in axes(lhs) and d not in lhs_batching_dimensions and d not in lhs_contracting_dimensions]</code>.</li>
<li>
<code>rhs_result_dimensions = [d for d in axes(rhs) and d not in rhs_batching_dimensions and d not in rhs_contracting_dimensions]</code>.</li>
<li>
<code>result_batching_index + result_lhs_index + result_rhs_index = result_index</code> where <code>size(result_batching_index) = size(lhs_batching_dimensions)</code>, <code>size(result_lhs_index) = size(lhs_result_dimensions)</code> and <code>size(result_rhs_index) = size(rhs_result_dimensions)</code>.</li>
<li>
<code>transposed_lhs = transpose(lhs, lhs_batching_dimensions + lhs_result_dimensions + lhs_contracting_dimensions)</code>.</li>
<li>
<code>transposed_lhs_slice = slice(transposed_lhs, result_batching_index + result_lhs_index + [:, ..., :])</code>.</li>
<li>
<code>reshaped_lhs_slice = reshape(transposed_lhs_slice, dims(lhs, lhs_contracting_dimensions))</code>.</li>
<li>
<code>transposed_rhs = transpose(rhs, rhs_batching_dimensions + rhs_result_dimensions + rhs_contracting_dimensions)</code>.</li>
<li>
<code>transposed_rhs_slice = slice(transposed_rhs, result_batching_index + result_rhs_index + [:, ..., :])</code>.</li>
<li>
<code>reshaped_rhs_slice = reshape(transposed_rhs_slice, dims(rhs, rhs_contracting_dimensions))</code>.</li>
<li>
<code>dot_product = reduce(     inputs=[multiply(reshaped_lhs_slice, reshaped_rhs_slice)],     init_values=[constant(0, element_type(result))],     dimensions=range(size(lhs_contracting_dimensions)),     body=lambda x, y: add(x, y))</code>. <!-- markdownlint-enable line-length -->
</li>
</ul><p>For quantized types, performs <code>dequantize_op_quantize(     lambda lhs, rhs: dot_general(lhs, rhs, lhs_batching_dimensions,         rhs_batching_dimensions, lhs_contracting_dimensions,         rhs_contracting_dimensions, precision_config), lhs, rhs, type(result))</code>.</p>
<p>For hybrid quantized types, performs <code>hybrid_dequantize_then_op(     lambda lhs, rhs: dot_general(lhs, rhs, lhs_batching_dimensions,         rhs_batching_dimensions, lhs_contracting_dimensions,         rhs_contracting_dimensions, precision_config), lhs, rhs)</code>.</p>
<p><code>precision_config</code> controls the tradeoff between speed and accuracy for computations on accelerator backends. This can be one of the following (at the moment, the semantics of these enum values is underspecified, but we are planning to address this in <a href="https://github.com/openxla/stablehlo/issues/755" class="external-link">#755</a>):</p>
<ul><li>
<code>DEFAULT</code>: Fastest calculation, but least accurate approximation to the original number.</li>
<li>
<code>HIGH</code>: Slower calculation, but more accurate approximation to the original number.</li>
<li>
<code>HIGHEST</code>: Slowest calculation, but most accurate approximation to the original number.</li>
</ul><p>A <code>DotAlgorithm</code> defines the main properties of the algorithm used to implement the dot operation, which also defines the precision. If the algorithm attribute fields are set, then the <code>precision_config</code> must be <code>DEFAULT</code>. <code>DotAlgorithms</code> do not have a default value, as the default parameters are implementation defined. As such, all dot algorithm fields may be set to <code>None</code> to specify an empty dot algorithm, which will instead use the <code>precision_config</code> value.</p>
<p><code>DotAlgorithm</code> fields include:</p>
<ul><li>
<code>lhs_precision_type</code> and <code>rhs_precision_type</code>, the precisions that the LHS and RHS of the operation are rounded to. Precision types are independent from the storage types of the inputs and the output.</li>
<li>
<code>accumulation_type</code> the precision used for accumulation.</li>
<li>
<code>lhs_component_count</code>, <code>rhs_component_count</code>, and <code>num_primitive_operations</code> apply when we are doing an algorithm which decomposes the LHS and/or RHS into multiple components and does multiple “primitive” dot operations on those values - usually to emulate a higher precision (e.g. <a href="https://arxiv.org/pdf/1904.06376.pdf" class="external-link">Leveraging the bfloat16 Artificial Intelligence Datatype For Higher-Precision Computations</a>: bf16_6x tf32_3x, etc). For algorithms with no decomposition, these values should be set to <code>1</code>.</li>
<li>
<code>allow_imprecise_accumulation</code> to specify if accumulation in lower precision is permitted for some steps (e.g. <code>CUBLASLT_MATMUL_DESC_FAST_ACCUM</code>).</li>
</ul><p>Example <code>DotAlgorithm</code> attributes:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb61-1"><a href="#cb61-1" tabindex="-1"></a>// Inputs are casted to tf32, and then accumulated in f32:</span>
<span id="cb61-2"><a href="#cb61-2" tabindex="-1"></a>{lhs_precision_type = tf32,</span>
<span id="cb61-3"><a href="#cb61-3" tabindex="-1"></a> rhs_precision_type = tf32,</span>
<span id="cb61-4"><a href="#cb61-4" tabindex="-1"></a> accumulation_type = f32,</span>
<span id="cb61-5"><a href="#cb61-5" tabindex="-1"></a> lhs_component_count = 1,</span>
<span id="cb61-6"><a href="#cb61-6" tabindex="-1"></a> rhs_component_count = 1,</span>
<span id="cb61-7"><a href="#cb61-7" tabindex="-1"></a> num_primitive_operations = 1,</span>
<span id="cb61-8"><a href="#cb61-8" tabindex="-1"></a> allow_imprecise_accumulation = false}</span>
<span id="cb61-9"><a href="#cb61-9" tabindex="-1"></a></span>
<span id="cb61-10"><a href="#cb61-10" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" tabindex="-1"></a>// bf16_6x: each input is decomposed to 3 bf16 components, then 6 dot operations are done on those components, and the result is accumulated in f32.</span>
<span id="cb61-12"><a href="#cb61-12" tabindex="-1"></a>{lhs_precision_type = bf16,</span>
<span id="cb61-13"><a href="#cb61-13" tabindex="-1"></a> rhs_precision_type = bf16,</span>
<span id="cb61-14"><a href="#cb61-14" tabindex="-1"></a> accumulation_type = f32,</span>
<span id="cb61-15"><a href="#cb61-15" tabindex="-1"></a> lhs_component_count = 3,</span>
<span id="cb61-16"><a href="#cb61-16" tabindex="-1"></a> rhs_component_count = 3,</span>
<span id="cb61-17"><a href="#cb61-17" tabindex="-1"></a> num_primitive_operations = 6,</span>
<span id="cb61-18"><a href="#cb61-18" tabindex="-1"></a> allow_imprecise_accumulation = false}</span>
<span id="cb61-19"><a href="#cb61-19" tabindex="-1"></a></span>
<span id="cb61-20"><a href="#cb61-20" tabindex="-1"></a></span>
<span id="cb61-21"><a href="#cb61-21" tabindex="-1"></a>// Inputs are (casted to) f8e5m2, and we accumulate in f32, but for some steps we may accumulate in lower precision.</span>
<span id="cb61-22"><a href="#cb61-22" tabindex="-1"></a>{lhs_precision_type = f8e5m2,</span>
<span id="cb61-23"><a href="#cb61-23" tabindex="-1"></a> rhs_precision_type = f8e5m2,</span>
<span id="cb61-24"><a href="#cb61-24" tabindex="-1"></a> accumulation_type = f32,</span>
<span id="cb61-25"><a href="#cb61-25" tabindex="-1"></a> lhs_component_count = 1,</span>
<span id="cb61-26"><a href="#cb61-26" tabindex="-1"></a> rhs_component_count = 1,</span>
<span id="cb61-27"><a href="#cb61-27" tabindex="-1"></a> num_primitive_operations = 1,</span>
<span id="cb61-28"><a href="#cb61-28" tabindex="-1"></a> allow_imprecise_accumulation = true}</span></code></pre></div>
<p>It is up to the implementations to decide which combinations are supported. In general, it is not guaranteed that each algorithm is supported on each accelerator type by the consumer of the StableHLO. If a given algorithm is not supported, an error should be raised as opposed to falling back to an alternative. StableHLO verification will provide best effort verification, preventing algorithms that are not known to be supported on <em>any</em> hardware.</p>
<p>See <a href="https://github.com/openxla/xla/blob/e8a707554de6b3d6bfd891583a81ff7020a97b54/xla/xla_data.proto#L1022" class="external-link"><code>xla_data.proto &gt; Algorithm</code></a> for some supported algorithm values. Ticket #2483 captures the plan to create a centralized doc on supported algorithms by backend.</p>
</div>
<div class="section level4">
<h4 id="inputs-31">Inputs<a class="anchor" aria-label="anchor" href="#inputs-31"></a></h4>
<table class="table"><colgroup><col width="4%"><col width="21%"><col width="41%"><col width="32%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C5-C6), (C9-C10), (C12-C14), (C17-C18), (C20)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or quantized tensor</td>
<td>(C7-C10), (C12-C20)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>lhs_batching_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C3), (C5), (C9), (C12)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>rhs_batching_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C4), (C7), (C9)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>lhs_contracting_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C3), (C6), (C10)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>rhs_contracting_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C8), (C10), (C16)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>precision_config</code></td>
<td>variadic number of enums of <code>DEFAULT</code>, <code>HIGH</code>, and <code>HIGHEST</code>
</td>
<td>(C11), (C21)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>lhs_precision_type</code></td>
<td>FloatType or TensorFloat32</td>
<td>(C21)</td>
</tr><tr class="odd"><td>(I9)</td>
<td><code>rhs_precision_type</code></td>
<td>FloatType or TensorFloat32</td>
<td>(C21)</td>
</tr><tr class="even"><td>(I10)</td>
<td><code>accumulation_type</code></td>
<td>FloatType or TensorFloat32</td>
<td>(C21)</td>
</tr><tr class="odd"><td>(I11)</td>
<td><code>lhs_component_count</code></td>
<td>constant of type <code>si32</code>
</td>
<td>(C21), (C22)</td>
</tr><tr class="even"><td>(I12)</td>
<td><code>rhs_component_count</code></td>
<td>constant of type <code>si32</code>
</td>
<td>(C21), (C23)</td>
</tr><tr class="odd"><td>(I13)</td>
<td><code>num_primitive_operations</code></td>
<td>constant of type <code>si32</code>
</td>
<td>(C21), (C24)</td>
</tr><tr class="even"><td>(I14)</td>
<td><code>allow_imprecise_accumulation</code></td>
<td>constant of type <code>bool</code>
</td>
<td>(C21)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-31">Outputs<a class="anchor" aria-label="anchor" href="#outputs-31"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C12), (C14), (C18-C20)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-29">Constraints<a class="anchor" aria-label="anchor" href="#constraints-29"></a></h4>
<ul><li>(C1) <code>size(lhs_batching_dimensions) = size(rhs_batching_dimensions)</code>.</li>
<li>(C2) <code>size(lhs_contracting_dimensions) =   size(rhs_contracting_dimensions)</code>.</li>
<li>(C3) <code>is_unique(lhs_batching_dimensions + lhs_contracting_dimensions)</code>.</li>
<li>(C4) <code>is_unique(rhs_batching_dimensions + rhs_contracting_dimensions)</code>.</li>
<li>(C5) <code>0 &lt;= lhs_batching_dimensions &lt; rank(lhs)</code>.</li>
<li>(C6) <code>0 &lt;= lhs_contracting_dimensions &lt; rank(lhs)</code>.</li>
<li>(C7) <code>0 &lt;= rhs_batching_dimensions &lt; rank(rhs)</code>.</li>
<li>(C8) <code>0 &lt;= rhs_contracting_dimensions &lt; rank(rhs)</code>.</li>
<li>(C9) <code>dim(lhs, lhs_batching_dimensions...) =   dim(rhs, rhs_batching_dimensions...)</code>.</li>
<li>(C10) <code>dim(lhs, lhs_contracting_dimensions...) =   dim(rhs, rhs_contracting_dimensions...)</code>.</li>
<li>(C11) <code>size(precision_config) = 2</code>.</li>
<li>(C12) <code>shape(result) = dim(lhs, lhs_batching_dimensions) +   dim(lhs, lhs_result_dimensions) + dim(rhs, rhs_result_dimensions)</code>.</li>
<li>If the operation uses non-quantized tensors:
<ul><li>(C13) <code>element_type(lhs) = element_type(rhs)</code>.</li>
</ul></li>
<li>If the operation uses quantized tensors:
<ul><li>(C14) <code>is_quantized(lhs) = is_quantized(result) and is_quantized(rhs)</code>.</li>
<li>(C15) <code>zero_points(rhs) = 0</code>.</li>
<li>(C16) If <code>is_per_axis_quantized(rhs)</code>, then <code>quantization_dimension(rhs)</code> not in <code>rhs_contracting_dimensions</code>.</li>
<li>If <code>is_quantized(lhs)</code>:
<ul><li>(C17) <code>storage_type(lhs) = storage_type(rhs)</code>.</li>
<li>(C18) <code>expressed_type(lhs) = expressed_type(rhs) = expressed_type(result)</code>.</li>
<li>(C19) If <code>is_per_tensor_quantized(rhs)</code>, then <code>is_per_tensor_quantized(result)</code>.</li>
</ul></li>
<li>If <code>!is_quantized(lhs)</code>:
<ul><li>(C20) <code>element_type(lhs) = expressed_type(rhs) = element_type(result)</code>.</li>
</ul></li>
</ul></li>
<li>If <code>!is_empty_algorithm(lhs_precision_type, rhs_precision_type,   accumulation_type, lhs_component_count, rhs_component_count,   num_primitive_operations allow_imprecise_accumulation)</code>:
<ul><li>(C21) <code>precision_config... = DEFAULT</code>.</li>
<li>(C22) <code>0 &lt; lhs_component_count</code>.</li>
<li>(C23) <code>0 &lt; rhs_component_count</code>.</li>
<li>(C24) <code>0 &lt; num_primitive_operations</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-31">Examples<a class="anchor" aria-label="anchor" href="#examples-31"></a></h4>
<pre class="mlir"><code>// %lhs: [
//        [[1, 2],
//         [3, 4]],
//        [[5, 6],
//         [7, 8]]
//       ]
// %rhs: [
//        [[1, 0],
//         [0, 1]],
//        [[1, 0],
//         [0, 1]]
//       ]
%result = "stablehlo.dot_general"(%lhs, %rhs) {
  dot_dimension_numbers = #stablehlo.dot&lt;
    lhs_batching_dimensions = [0],
    rhs_batching_dimensions = [0],
    lhs_contracting_dimensions = [2],
    rhs_contracting_dimensions = [1]
  &gt;,
  precision_config = [#stablehlo&lt;precision DEFAULT&gt;, #stablehlo&lt;precision DEFAULT&gt;],
  algorithm = #stablehlo.dot_algorithm&lt;
    lhs_precision_type = tf32,
    rhs_precision_type = tf32,
    accumulation_type = f32,
    lhs_component_count = 1,
    rhs_component_count = 1,
    num_primitive_operations = 1,
    allow_imprecise_accumulation = false
  &gt;
} : (tensor&lt;2x2x2xi64&gt;, tensor&lt;2x2x2xi64&gt;) -&gt; tensor&lt;2x2x2xi64&gt;
// %result: [
//           [[1, 2],
//            [3, 4]],
//           [[5, 6],
//            [7, 8]]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dot_general.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_broadcast_in_dim">dynamic_broadcast_in_dim<a class="anchor" aria-label="anchor" href="#dynamic_broadcast_in_dim"></a></h3>
<div class="section level4">
<h4 id="semantics-32">Semantics<a class="anchor" aria-label="anchor" href="#semantics-32"></a></h4>
<p>This operation is functionally identical to <a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md#broadcast_in_dim" class="external-link">broadcast_in_dim</a> op, but the result shape is specified dynamically via <code>output_dimensions</code>.</p>
<p>The operation also accepts optional attributes <code>known_expanding_dimensions</code>, <code>known_nonexpanding_dimensions</code> to express static knowledge about the expanding behavior of dimensions. If not specified, all dimensions are assumed to be possibly expanding.</p>
</div>
<div class="section level4">
<h4 id="inputs-32">Inputs<a class="anchor" aria-label="anchor" href="#inputs-32"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="30%"><col width="41%"><col width="21%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C2), (C5-C6), (C9)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>output_dimensions</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C7)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>broadcast_dimensions</code></td>
<td>1-dimensional constant tensor of integer type</td>
<td>(C2-C6)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>known_expanding_dimensions</code></td>
<td>1-dimensional constant tensor of integer type</td>
<td>(C8-C9)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>known_nonexpanding_dimensions</code></td>
<td>1-dimensional constant tensor of integer type</td>
<td>(C8-C9)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-32">Outputs<a class="anchor" aria-label="anchor" href="#outputs-32"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1), (C3), (C5-C7)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-30">Constraints<a class="anchor" aria-label="anchor" href="#constraints-30"></a></h4>
<ul><li>(C1) <code>element_type(result)</code> is given by:
<ul><li>
<code>element_type(operand)</code>, if <code>!is_per_axis_quantized(operand)</code>.</li>
<li>
<code>element_type(operand)</code> except that <code>quantization_dimension(operand)</code>, <code>scales(operand)</code>, and <code>zero_points(operand)</code> may differ from <code>quantization_dimension(result)</code>, <code>scales(result)</code>, and <code>zero_points(result)</code> resp., otherwise.</li>
</ul></li>
<li>(C2) <code>size(broadcast_dimensions) = rank(operand)</code>.</li>
<li>(C3) <code>0 &lt;= broadcast_dimensions &lt; rank(result)</code>.</li>
<li>(C4) <code>is_unique(broadcast_dimensions)</code>.</li>
<li>(C5) For all <code>d</code> in <code>axes(operand)</code>:
<ul><li>
<code>dim(operand, d) = 1</code> or</li>
<li>
<code>dim(operand, d) = dim(result, broadcast_dimensions[d])</code>.</li>
</ul></li>
<li>(C6) If <code>is_per_axis_quantized(result)</code>:
<ul><li>
<code>quantization_dimension(result) = broadcast_dimensions[quantization_dimension(operand)]</code>.</li>
<li>If <code>dim(operand, quantization_dimension(operand)) = 1</code>, then <code>scales(result)[i] = scales(operand)[0] and zero_points(result)[i] =   zero_points(operand)[0] for i in   range(dim(result, quantization_dimension(result)))</code>.</li>
</ul></li>
<li>(C7) <code>size(output_dimensions) = rank(result)</code>.</li>
<li>(C8) <code>is_unique(known_expanding_dimensions + known_nonexpanding_dimensions)</code>.</li>
<li>(C9) <code>0 &lt;= known_expanding_dimensions &lt; rank(operand)</code>.</li>
<li>(C10) <code>0 &lt;= known_nonexpanding_dimensions &lt; rank(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-32">Examples<a class="anchor" aria-label="anchor" href="#examples-32"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [1, 2, 3]
//           ]
%operand = stablehlo.constant dense&lt;[[1, 2, 3]]&gt; : tensor&lt;1x3xi64&gt;
%output_dimensions = stablehlo.constant dense&lt;[2, 3, 2]&gt; : tensor&lt;3xi64&gt;
%result = "stablehlo.dynamic_broadcast_in_dim"(%operand, %output_dimensions) {
  broadcast_dimensions = array&lt;i64: 2, 1&gt;,
  known_expanding_dimensions = array&lt;i64: 0&gt;,
  known_nonexpanding_dimensions = array&lt;i64: 1&gt;
} : (tensor&lt;1x3xi64&gt;, tensor&lt;3xi64&gt;) -&gt; tensor&lt;2x3x2xi64&gt;
// %result: [
//            [
//             [1, 1],
//             [2, 2],
//             [3, 3]
//            ],
//            [
//             [1, 1],
//             [2, 2],
//             [3, 3]
//            ]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_broadcast_in_dim.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_conv">dynamic_conv<a class="anchor" aria-label="anchor" href="#dynamic_conv"></a></h3>
<div class="section level4">
<h4 id="semantics-33">Semantics<a class="anchor" aria-label="anchor" href="#semantics-33"></a></h4>
<p>This operation is functionally identical to <a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convolution" class="external-link">convolution</a> op, but the padding is specified dynamically via <code>padding</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-33">Inputs<a class="anchor" aria-label="anchor" href="#inputs-33"></a></h4>
<table class="table"><colgroup><col width="4%"><col width="21%"><col width="38%"><col width="36%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C10-C11), (C14) (C25), (C26-C27), (C30-C31), (C33)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or quantized tensor</td>
<td>(C1), (C14-C16), (C26-C28), (C30-C33)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>padding</code></td>
<td>2-dimensional tensor of integer type</td>
<td>(C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>window_strides</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2-C3)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>lhs_dilation</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C5-C6)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>rhs_dilation</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C7-C8)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>window_reversal</code></td>
<td>1-dimensional tensor constant of type <code>i1</code>
</td>
<td>(C9)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>input_batch_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C10), (C13)</td>
</tr><tr class="odd"><td>(I9)</td>
<td><code>input_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C11), (C13-C14)</td>
</tr><tr class="even"><td>(I10)</td>
<td><code>input_spatial_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C12), (C13)</td>
</tr><tr class="odd"><td>(I11)</td>
<td><code>kernel_input_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C14), (C18)</td>
</tr><tr class="even"><td>(I12)</td>
<td><code>kernel_output_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C15-C16), (C18), (C28)</td>
</tr><tr class="odd"><td>(I13)</td>
<td><code>kernel_spatial_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C17-C18)</td>
</tr><tr class="even"><td>(I14)</td>
<td><code>output_batch_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C20)</td>
</tr><tr class="odd"><td>(I15)</td>
<td><code>output_feature_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C20), (C29)</td>
</tr><tr class="even"><td>(I16)</td>
<td><code>output_spatial_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C19-C20)</td>
</tr><tr class="odd"><td>(I17)</td>
<td><code>feature_group_count</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C11), (C14), (C16), (C21), (C23)</td>
</tr><tr class="even"><td>(I18)</td>
<td><code>batch_group_count</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C10), (C15), (C22), (C23)</td>
</tr><tr class="odd"><td>(I19)</td>
<td><code>precision_config</code></td>
<td>variadic number of enums of <code>DEFAULT</code>, <code>HIGH</code>, and <code>HIGHEST</code>
</td>
<td>(C24)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-33">Outputs<a class="anchor" aria-label="anchor" href="#outputs-33"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C25-C27), (C29), (C31-C33)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-31">Constraints<a class="anchor" aria-label="anchor" href="#constraints-31"></a></h4>
<!-- markdownlint-disable line-length -->
<ul><li>(C1) <code>N = rank(lhs) = rank(rhs)</code>.</li>
<li>(C2) <code>size(window_strides) = N - 2</code>.</li>
<li>(C3) <code>0 &lt; window_strides</code>.</li>
<li>(C4) <code>shape(padding) = [N - 2, 2]</code>.</li>
<li>(C5) <code>size(lhs_dilation) = N - 2</code>.</li>
<li>(C6) <code>0 &lt; lhs_dilation</code>.</li>
<li>(C7) <code>size(rhs_dilation) = N - 2</code>.</li>
<li>(C8) <code>0 &lt; rhs_dilation</code>.</li>
<li>(C9) <code>size(window_reversal) = N - 2</code>.</li>
<li>(C10) <code>dim(lhs, input_batch_dimension) % batch_group_count = 0</code>.</li>
<li>(C11) <code>dim(lhs, input_feature_dimension) % feature_group_count = 0</code>.</li>
<li>(C12) <code>size(input_spatial_dimensions) = N - 2</code>.</li>
<li>(C13) Given <code>input_dimensions = [input_batch_dimension] +        input_spatial_dimensions + [input_feature_dimension]</code>:
<ul><li>
<code>is_unique(input_dimensions)</code>.</li>
<li>
<code>0 &lt;= input_dimensions &lt; N</code>.</li>
</ul></li>
<li>(C14) <code>dim(rhs, kernel_input_feature_dimension) = dim(lhs, input_feature_dimension) / feature_group_count</code>.</li>
<li>(C15) <code>dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0</code>.</li>
<li>(C16) <code>dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0</code>.</li>
<li>(C17) <code>size(kernel_spatial_dimensions) = N - 2</code>.</li>
<li>(C18) Given <code>kernel_dimensions = kernel_spatial_dimensions +         [kernel_input_feature_dimension] + [kernel_output_feature_dimension]</code>:
<ul><li>
<code>is_unique(kernel_dimensions)</code>.</li>
<li>
<code>0 &lt;= kernel_dimensions &lt; N</code>.</li>
</ul></li>
<li>(C19) <code>size(output_spatial_dimensions) = N - 2</code>.</li>
<li>(C20) Given <code>output_dimensions = [output_batch_dimension] +         output_spatial_dimensions + [output_feature_dimension]</code>:
<ul><li>
<code>is_unique(output_dimensions)</code>.</li>
<li>
<code>0 &lt;= output_dimensions &lt; N</code>.</li>
</ul></li>
<li>(C21) <code>0 &lt; feature_group_count</code>.</li>
<li>(C22) <code>0 &lt; batch_group_count</code>.</li>
<li>(C23) <code>feature_group_count = 1 or batch_group_count = 1</code>.</li>
<li>(C24) <code>size(precision_config) = 2</code>.</li>
<li>(C25) <code>dim(result, result_dim)</code> is defined as:
<ul><li>
<code>dim(lhs, input_batch_dimension) / batch_group_count</code> if <code>result_dim = output_batch_dimension</code>.</li>
<li>
<code>dim(rhs, kernel_output_feature_dimension)</code> if <code>result_dim = output_feature_dimension</code>.</li>
<li>
<code>num_windows</code> otherwise, where:
<ul><li>
<code>output_spatial_dimensions[spatial_dim] = result_dim</code>.</li>
<li>
<code>lhs_dim = input_spatial_dimensions[spatial_dim]</code>.</li>
<li>
<code>rhs_dim = kernel_spatial_dimensions[spatial_dim]</code>.</li>
<li>
<code>dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) = 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1</code>.</li>
<li>
<code>padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]</code>.</li>
<li>
<code>dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) = 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1</code>.</li>
<li>
<code>is_empty_window[lhs_dim] = padded_input_shape[lhs_dim] = 0 || dilated_window_shape[lhs_dim] &gt; padded_input_shape[lhs_dim]</code>.</li>
<li>
<code>num_windows = is_empty_window[lhs_dim] ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1</code>.</li>
</ul></li>
</ul></li>
<li>(C26) <code>rank(result) = N</code>.</li>
<li>If the operation uses non-quantized tensors:
<ul><li>(C27) <code>element_type(lhs) = element_type(rhs) = element_type(result)</code>.</li>
</ul></li>
<li>If the operation uses quantized tensors:
<ul><li>(C28) <code>is_quantized(lhs) = is_quantized(result) and is_quantized(rhs)</code>.</li>
<li>(C29) If <code>is_per_axis_quantized(rhs)</code>, then <code>quantization_dimension(rhs) = kernel_output_feature_dimension</code>.</li>
<li>(C30) If <code>is_per_axis_quantized(result)</code>, then <code>quantization_dimension(result) = output_feature_dimension</code>.</li>
<li>If <code>is_quantized(lhs)</code>:
<ul><li>(C31) <code>storage_type(lhs) = storage_type(rhs)</code>.</li>
<li>(C32) <code>expressed_type(lhs) = expressed_type(rhs) = expressed_type(result)</code>.</li>
<li>(C33) If <code>is_per_tensor_quantized(rhs)</code>, then <code>is_per_tensor_quantized(result)</code>.</li>
</ul></li>
<li>If <code>!is_quantized(lhs)</code>:
<ul><li>(C34) <code>element_type(lhs) = expressed_type(rhs) = element_type(result)</code>. <!-- markdownlint-enable line-length -->
</li>
</ul></li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-33">Examples<a class="anchor" aria-label="anchor" href="#examples-33"></a></h4>
<pre class="mlir"><code>// %lhs: [[
//        [[1], [2], [5], [6]],
//        [[3], [4], [7], [8]],
//        [[10], [11], [14], [15]],
//        [[12], [13], [16], [17]]
//      ]]
//
// %rhs: [
//         [[[1]], [[1]], [[1]]],
//         [[[1]], [[1]], [[1]]],
//         [[[1]], [[1]], [[1]]]
//        ]
// %padding: [[1, 1],
//            [1, 1]]
%result = "stablehlo.dynamic_conv"(%lhs, %rhs, %padding) {
  window_strides = array&lt;i64: 4, 4&gt;,
  lhs_dilation = array&lt;i64: 2, 2&gt;,
  rhs_dilation = array&lt;i64: 1, 1&gt;,
  window_reversal = array&lt;i1: false, false&gt;,
  dimension_numbers = #stablehlo.conv&lt;raw
    input_batch_dimension = 0,
    input_feature_dimension = 3,
    input_spatial_dimensions = [0, 1],
    kernel_input_feature_dimension = 2,
    kernel_output_feature_dimension = 3,
    kernel_spatial_dimensions = [0, 1],
    output_batch_dimension = 0,
    output_feature_dimension = 3,
    output_spatial_dimensions = [1, 2]
  &gt;,
  feature_group_count = 1 : i64,
  batch_group_count = 1 : i64,
  precision_config = [#stablehlo&lt;precision DEFAULT&gt;, #stablehlo&lt;precision DEFAULT&gt;]
} : (tensor&lt;1x4x4x1xi64&gt;, tensor&lt;3x3x1x1xi64&gt;, tensor&lt;2x2xi64&gt;) -&gt; tensor&lt;1x2x2x1xi64&gt;
// %result: [[
//            [[1], [5]],
//            [[10], [14]]
//          ]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_conv.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_gather">dynamic_gather<a class="anchor" aria-label="anchor" href="#dynamic_gather"></a></h3>
<div class="section level4">
<h4 id="semantics-34">Semantics<a class="anchor" aria-label="anchor" href="#semantics-34"></a></h4>
<p>This operation is functionally identical to <a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md#gather" class="external-link">gather</a> op, with the <code>slice_sizes</code> specified dynamically as a value.</p>
</div>
<div class="section level4">
<h4 id="inputs-34">Inputs<a class="anchor" aria-label="anchor" href="#inputs-34"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="22%"><col width="42%"><col width="28%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C7), (C10-C12), (C14)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>start_indices</code></td>
<td>tensor of integer type</td>
<td>(C2), (C3), (C13)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>slice_sizes</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C8), (C11-C13)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>offset_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C4-C5), (C13)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>collapsed_slice_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C6-C8), (C13)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>start_index_map</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C3), (C9), (C10)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>index_vector_dim</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C2), (C3), (C13)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>indices_are_sorted</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-34">Outputs<a class="anchor" aria-label="anchor" href="#outputs-34"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C5), (C13-C14)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-32">Constraints<a class="anchor" aria-label="anchor" href="#constraints-32"></a></h4>
<ul><li>(C1) <code>rank(operand) = size(offset_dims) + size(collapsed_slice_dims)</code>.</li>
<li>(C2) <code>0 &lt;= index_vector_dim &lt;= rank(start_indices)</code>.</li>
<li>(C3) <code>size(start_index_map) =        index_vector_dim &lt; rank(start_indices) ?        dim(start_indices, index_vector_dim) : 1</code>.</li>
<li>(C4) <code>is_unique(offset_dims) and is_sorted(offset_dims)</code>.</li>
<li>(C5) <code>0 &lt;= offset_dims &lt; rank(result)</code>.</li>
<li>(C6) <code>is_unique(collapsed_slice_dims) and is_sorted(collapsed_slice_dims)</code>.</li>
<li>(C7) <code>0 &lt;= collapsed_slice_dims &lt; rank(operand)</code>.</li>
<li>(C8) <code>slice_sizes[collapsed_slice_dims...] &lt;= 1</code>.</li>
<li>(C9) <code>is_unique(start_index_map)</code>.</li>
<li>(C10) <code>0 &lt;= start_index_map &lt; rank(operand)</code>.</li>
<li>(C11) <code>size(slice_sizes) = rank(operand)</code>.</li>
<li>(C12) <code>0 &lt;= slice_sizes &lt;= shape(operand)</code>.</li>
<li>(C13) <code>shape(result) = combine(batch_dim_sizes, offset_dim_sizes)</code> where:
<ul><li>
<code>batch_dim_sizes = shape(start_indices)</code> except that the dimension size of <code>start_indices</code> corresponding to <code>index_vector_dim</code> is not included.</li>
<li>
<code>offset_dim_sizes = shape(slice_sizes)</code> except that the dimension sizes in <code>slice_sizes</code> corresponding to <code>collapsed_slice_dims</code> are not included.</li>
<li>
<code>combine</code> puts <code>batch_dim_sizes</code> at axes corresponding to <code>batch_dims</code> and <code>offset_dim_sizes</code> at axes corresponding to <code>offset_dims</code>.</li>
</ul></li>
<li>(C14) <code>element_type(operand) = element_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-34">Examples<a class="anchor" aria-label="anchor" href="#examples-34"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [[1, 2], [3, 4], [5, 6], [7, 8]],
//            [[9, 10],[11, 12], [13, 14], [15, 16]],
//            [[17, 18], [19, 20], [21, 22], [23, 24]]
//           ]
// %start_indices: [
//                  [[0, 0], [1, 0], [2, 1]],
//                  [[0, 1], [1, 1], [0, 2]]
//                 ]
// %slize_sizes: [1, 2, 2]
%result = "stablehlo.dynamic_gather"(%operand, %start_indices, %slize_sizes) {
  dimension_numbers = #stablehlo.gather&lt;
    offset_dims = [2, 3],
    collapsed_slice_dims = [0],
    start_index_map = [1, 0],
    index_vector_dim = 2&gt;,
  indices_are_sorted = false
} : (tensor&lt;3x4x2xi64&gt;, tensor&lt;2x3x2xi64&gt;, tensor&lt;3xi64&gt;) -&gt; tensor&lt;2x3x2x2xi64&gt;
// %result: [
//            [
//              [[1, 2], [3, 4]],
//              [[3, 4], [5, 6]],
//              [[13, 14], [15, 16]]
//            ],
//            [
//              [[9, 10], [11, 12]],
//              [[11, 12], [13, 14]],
//              [[17, 18], [19, 20]]
//            ]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_gather.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_iota">dynamic_iota<a class="anchor" aria-label="anchor" href="#dynamic_iota"></a></h3>
<div class="section level4">
<h4 id="semantics-35">Semantics<a class="anchor" aria-label="anchor" href="#semantics-35"></a></h4>
<p>This operation is functionally identical to <a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md#iota" class="external-link">iota</a> op, but the result shape is specified dynamically via <code>output_shape</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-35">Inputs<a class="anchor" aria-label="anchor" href="#inputs-35"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="23%"><col width="50%"><col width="17%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>output_shape</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C1), (C2)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>iota_dimension</code></td>
<td><code>si64</code></td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-35">Outputs<a class="anchor" aria-label="anchor" href="#outputs-35"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-33">Constraints<a class="anchor" aria-label="anchor" href="#constraints-33"></a></h4>
<ul><li>(C1) <code>0 &lt;= iota_dimension &lt; size(output_shape)</code>.</li>
<li>(C2) <code>rank(result) = size(output_shape)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-35">Examples<a class="anchor" aria-label="anchor" href="#examples-35"></a></h4>
<pre class="mlir"><code>%output_shape = stablehlo.constant dense&lt;[4, 5]&gt; : tensor&lt;2xi64&gt;
%result = "stablehlo.dynamic_iota"(%output_shape) {
  iota_dimension = 0 : i64
} : (tensor&lt;2xi64&gt;) -&gt; tensor&lt;4x5xi64&gt;
// %result: [
//           [0, 0, 0, 0, 0],
//           [1, 1, 1, 1, 1],
//           [2, 2, 2, 2, 2],
//           [3, 3, 3, 3, 3]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_iota.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_pad">dynamic_pad<a class="anchor" aria-label="anchor" href="#dynamic_pad"></a></h3>
<div class="section level4">
<h4 id="semantics-36">Semantics<a class="anchor" aria-label="anchor" href="#semantics-36"></a></h4>
<p>This operation is functionally identical to <a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md#pad" class="external-link">pad</a> op, but with <code>edge_padding_low</code>, <code>edge_padding_high</code>, and <code>interior_padding</code> specified dynamically as values.</p>
</div>
<div class="section level4">
<h4 id="inputs-36">Inputs<a class="anchor" aria-label="anchor" href="#inputs-36"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="21%"><col width="53%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C2), (C4)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>padding_value</code></td>
<td>0-dimensional tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>edge_padding_low</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C1), (C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>edge_padding_high</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C1), (C4)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>interior_padding</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C2-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-36">Outputs<a class="anchor" aria-label="anchor" href="#outputs-36"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C3-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-34">Constraints<a class="anchor" aria-label="anchor" href="#constraints-34"></a></h4>
<ul><li>(C1) <code>element_type(operand) = element_type(padding_value) =   element_type(result)</code>.</li>
<li>(C2) <code>size(edge_padding_low) = size(edge_padding_high) =   size(interior_padding) = rank(operand)</code>.</li>
<li>(C3) <code>0 &lt;= interior_padding</code>.</li>
<li>(C4) <code>shape(result) = shape(operand) + edge_padding_low +   max(shape(operand) - 1, 0) * interior_padding + edge_padding_high</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-36">Examples<a class="anchor" aria-label="anchor" href="#examples-36"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [1, 2, 3],
//            [4, 5, 6]
//           ]
// %padding_value: 0
// %edge_padding_low: [0, 1]
// %edge_padding_high: [2, 1]
// %interior_padding: [1, 2]
%result = "stablehlo.dynamic_pad"(%operand, %padding_value,
  %edge_padding_low, %edge_padding_high, %interior_padding
) : (tensor&lt;2x3xi64&gt;, tensor&lt;i64&gt;, tensor&lt;2xi64&gt;, tensor&lt;2xi64&gt;, tensor&lt;2xi64&gt;) -&gt; tensor&lt;5x9xi64&gt;
// %result: [
//           [0, 1, 0, 0, 2, 0, 0, 3, 0],
//           [0, 0, 0, 0, 0, 0, 0, 0, 0],
//           [0, 4, 0, 0, 5, 0, 0, 6, 0],
//           [0, 0, 0, 0, 0, 0, 0, 0, 0],
//           [0, 0, 0, 0, 0, 0, 0, 0, 0]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_pad.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_reshape">dynamic_reshape<a class="anchor" aria-label="anchor" href="#dynamic_reshape"></a></h3>
<div class="section level4">
<h4 id="semantics-37">Semantics<a class="anchor" aria-label="anchor" href="#semantics-37"></a></h4>
<p>This operation is functionally identical to <a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reshape" class="external-link">reshape</a> op, but the result shape is specified dynamically via <code>output_shape</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-37">Inputs<a class="anchor" aria-label="anchor" href="#inputs-37"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="21%"><col width="51%"><col width="17%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>output_shape</code></td>
<td>1-dimensional tensor of integer type</td>
<td>(C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-37">Outputs<a class="anchor" aria-label="anchor" href="#outputs-37"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-35">Constraints<a class="anchor" aria-label="anchor" href="#constraints-35"></a></h4>
<ul><li>(C1) <code>element_type(result)</code> is given by:
<ul><li>
<code>element_type(operand)</code>, if <code>!is_per_axis_quantized(operand)</code>.</li>
<li>
<code>element_type(operand)</code> except that <code>quantization_dimension(operand)</code> and <code>quantization_dimension(result)</code> may differ, otherwise.</li>
</ul></li>
<li>(C2) <code>size(operand) = size(result)</code>.</li>
<li>(C3) If <code>is_per_axis_quantized(operand)</code>:
<ul><li>
<code>reduce(dims(operand, [0, 1, ..., quantization_dimension(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result, [0, 1, ..., quantization_dimension(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y)</code>.</li>
<li>
<code>dim(operand, quantization_dimension(operand)) =   dim(result, quantization_dimension(result))</code>.</li>
<li>
<code>reduce(dims(operand,   [quantization_dimension(operand) + 1, ..., rank(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result,   [quantization_dimension(result) + 1, ..., rank(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y)</code>.</li>
</ul></li>
<li>(C4) <code>size(output_shape) = rank(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-37">Examples<a class="anchor" aria-label="anchor" href="#examples-37"></a></h4>
<pre class="mlir"><code>// %operand: [[1, 2, 3], [4, 5, 6]]
// %output_shape: [3, 2]
%result = "stablehlo.dynamic_reshape"(%operand, %output_shape) : (tensor&lt;2x3xi64&gt;, tensor&lt;2xi64&gt;) -&gt; tensor&lt;3x2xi64&gt;
// %result: [[1, 2], [3, 4], [5, 6]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_reshape.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_slice">dynamic_slice<a class="anchor" aria-label="anchor" href="#dynamic_slice"></a></h3>
<div class="section level4">
<h4 id="semantics-38">Semantics<a class="anchor" aria-label="anchor" href="#semantics-38"></a></h4>
<p>Extracts a slice from the <code>operand</code> using dynamically-computed starting indices and produces a <code>result</code> tensor. <code>start_indices</code> contain the starting indices of the slice for each dimension subject to potential adjustment, and <code>slice_sizes</code> contain the sizes of the slice for each dimension. More formally, <code>result[result_index] = operand[operand_index]</code> where:</p>
<ul><li>
<code>adjusted_start_indices = clamp(0, start_indices, shape(operand) -   slice_sizes)</code>.</li>
<li>
<code>operand_index = adjusted_start_indices + result_index</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-38">Inputs<a class="anchor" aria-label="anchor" href="#inputs-38"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="17%"><col width="57%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C2), (C4)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>start_indices</code></td>
<td>variadic number of 0-dimensional tensors of integer type</td>
<td>(C2), (C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>slice_sizes</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-38">Outputs<a class="anchor" aria-label="anchor" href="#outputs-38"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-36">Constraints<a class="anchor" aria-label="anchor" href="#constraints-36"></a></h4>
<ul><li>(C1) <code>element_type(operand) = element_type(result)</code>.</li>
<li>(C2) <code>size(start_indices) = size(slice_sizes) = rank(operand)</code>.</li>
<li>(C3) <code>same(type(start_indices...))</code>.</li>
<li>(C4) <code>0 &lt;= slice_sizes &lt;= shape(operand)</code>.</li>
<li>(C5) <code>shape(result) = slice_sizes</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-38">Examples<a class="anchor" aria-label="anchor" href="#examples-38"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [0, 0, 1, 1],
//            [0, 0, 1, 1],
//            [0, 0, 0, 0],
//            [0, 0, 0, 0]
//           ]
// %start_indices0: -1
// %start_indices1: 3
%result = "stablehlo.dynamic_slice"(%operand, %start_indices0, %start_indices1) {
  slice_sizes = array&lt;i64: 2, 2&gt;
} : (tensor&lt;4x4xi32&gt;, tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [
//           [1, 1],
//           [1, 1]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_slice.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="dynamic_update_slice">dynamic_update_slice<a class="anchor" aria-label="anchor" href="#dynamic_update_slice"></a></h3>
<div class="section level4">
<h4 id="semantics-39">Semantics<a class="anchor" aria-label="anchor" href="#semantics-39"></a></h4>
<p>Produces a <code>result</code> tensor which is equal to the <code>operand</code> tensor except that the slice starting at <code>start_indices</code> is updated with the values in <code>update</code>. More formally, <code>result[result_index]</code> is defined as:</p>
<ul><li>
<code>update[update_index]</code> if <code>0 &lt;= update_index &lt; shape(update)</code> where:
<ul><li>
<code>adjusted_start_indices = clamp(0, start_indices, shape(operand) -   shape(update))</code>.</li>
<li>
<code>update_index = result_index - adjusted_start_indices</code>.</li>
</ul></li>
<li>
<code>operand[result_index]</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-39">Inputs<a class="anchor" aria-label="anchor" href="#inputs-39"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="17%"><col width="57%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C4), (C6)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>update</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C2), (C3), (C6)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>start_indices</code></td>
<td>variadic number of 0-dimensional tensors of integer type</td>
<td>(C4), (C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-39">Outputs<a class="anchor" aria-label="anchor" href="#outputs-39"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-37">Constraints<a class="anchor" aria-label="anchor" href="#constraints-37"></a></h4>
<ul><li>(C1) <code>type(operand) = type(result)</code>.</li>
<li>(C2) <code>element_type(update) = element_type(operand)</code>.</li>
<li>(C3) <code>rank(update) = rank(operand)</code>.</li>
<li>(C4) <code>size(start_indices) = rank(operand)</code>.</li>
<li>(C5) <code>same(type(start_indices...))</code>.</li>
<li>(C6) <code>0 &lt;= shape(update) &lt;= shape(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-39">Examples<a class="anchor" aria-label="anchor" href="#examples-39"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [1, 1, 0, 0],
//            [1, 1, 0, 0],
//            [1, 1, 1, 1],
//            [1, 1, 1, 1]
//           ]
// %update: [
//           [1, 1],
//           [1, 1]
//          ]
// %start_indices0: -1
// %start_indices1: 3
%result = "stablehlo.dynamic_update_slice"(%operand, %update, %start_indices0, %start_indices1)
  : (tensor&lt;4x4xi32&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;4x4xi32&gt;
// %result: [
//           [1, 1, 1, 1],
//           [1, 1, 1, 1],
//           [1, 1, 1, 1],
//           [1, 1, 1, 1]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/dynamic_update_slice.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="exponential">exponential<a class="anchor" aria-label="anchor" href="#exponential"></a></h3>
<div class="section level4">
<h4 id="semantics-40">Semantics<a class="anchor" aria-label="anchor" href="#semantics-40"></a></h4>
<p>Performs element-wise exponential operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>exp</code> from IEEE-754.</li>
<li>For complex numbers: complex exponential.</li>
<li>For quantized types: <code>dequantize_op_quantize(exponential, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-40">Inputs<a class="anchor" aria-label="anchor" href="#inputs-40"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-40">Outputs<a class="anchor" aria-label="anchor" href="#outputs-40"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-38">Constraints<a class="anchor" aria-label="anchor" href="#constraints-38"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-40">Examples<a class="anchor" aria-label="anchor" href="#examples-40"></a></h4>
<pre class="mlir"><code>// %operand: [[0.0, 1.0], [2.0, 3.0]]
%result = "stablehlo.exponential"(%operand) : (tensor&lt;2x2xf64&gt;) -&gt; tensor&lt;2x2xf64&gt;
// %result: [[1.0, 2.7182818284590451], [7.3890560989306504, 20.085536923187668]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/exponential.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="exponential_minus_one">exponential_minus_one<a class="anchor" aria-label="anchor" href="#exponential_minus_one"></a></h3>
<div class="section level4">
<h4 id="semantics-41">Semantics<a class="anchor" aria-label="anchor" href="#semantics-41"></a></h4>
<p>Performs element-wise exponential minus one operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>expm1</code> from IEEE-754.</li>
<li>For complex numbers: complex exponential minus one.</li>
<li>For quantized types: <code>dequantize_op_quantize(exponential_minus_one, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-41">Inputs<a class="anchor" aria-label="anchor" href="#inputs-41"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-41">Outputs<a class="anchor" aria-label="anchor" href="#outputs-41"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-39">Constraints<a class="anchor" aria-label="anchor" href="#constraints-39"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-41">Examples<a class="anchor" aria-label="anchor" href="#examples-41"></a></h4>
<pre class="mlir"><code>// %operand: [0.0, 1.0]
%result = "stablehlo.exponential_minus_one"(%operand) : (tensor&lt;2xf64&gt;) -&gt; tensor&lt;2xf64&gt;
// %result: [0.0, 1.71828187]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/exponential_minus_one.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="fft">fft<a class="anchor" aria-label="anchor" href="#fft"></a></h3>
<div class="section level4">
<h4 id="semantics-42">Semantics<a class="anchor" aria-label="anchor" href="#semantics-42"></a></h4>
<p>Performs the forward and inverse Fourier transforms for real and complex inputs/outputs.</p>
<p><code>fft_type</code> is one of the following:</p>
<ul><li>
<code>FFT</code>: Forward complex-to-complex FFT.</li>
<li>
<code>IFFT</code>: Inverse complex-to-complex FFT.</li>
<li>
<code>RFFT</code>: Forward real-to-complex FFT.</li>
<li>
<code>IRFFT</code>: Inverse real-to-complex FFT (i.e. takes complex, returns real).</li>
</ul><p>More formally, given the function <code>fft</code> which takes 1-dimensional tensors of complex types as input, produces 1-dimensional tensors of same types as output and computes the discrete Fourier transform:</p>
<p>For <code>fft_type = FFT</code>, <code>result</code> is defined as the final result of a series of L computations where <code>L = size(fft_length)</code>. For example, for <code>L = 3</code>:</p>
<ul><li>
<code>result1[i0, ..., :] = fft(operand[i0, ..., :])</code>.</li>
<li>
<code>result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1])</code>.</li>
<li>
<code>result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1])</code>.</li>
</ul><p>Furthermore, given the function <code>ifft</code> which has the same type signature and computes the inverse of <code>fft</code>:</p>
<p>For <code>fft_type = IFFT</code>, <code>result</code> is defined as the inverse of the computations for <code>fft_type = FFT</code>. For example, for <code>L = 3</code>:</p>
<ul><li>
<code>result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1])</code>.</li>
<li>
<code>result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1])</code>.</li>
<li>
<code>result[i0, ..., :] = ifft(result2[i0, ..., :])</code>.</li>
</ul><p>Furthermore, given the function <code>rfft</code> which takes 1-dimensional tensors of floating-point types, produces 1-dimensional tensors of complex types of the same floating-point semantics and works as follows:</p>
<ul><li>
<code>rfft(real_operand) = truncated_result</code> where</li>
<li>
<code>complex_operand... = (real_operand..., 0.0)</code>.</li>
<li>
<code>complex_result = fft(complex_operand)</code>.</li>
<li>
<code>truncated_result = complex_result[:(rank(complex_result) / 2 + 1)]</code>.</li>
</ul><p>(When the discrete Fourier transform is computed for real operands, the first <code>N/2 + 1</code> elements of the result unambiguously define the rest of the result, so the result of <code>rfft</code> is truncated to avoid computing redundant elements).</p>
<p>For <code>fft_type = RFFT</code>, <code>result</code> is defined as the final result of a series of L computations where <code>L = size(fft_length)</code>. For example, for <code>L = 3</code>:</p>
<ul><li>
<code>result1[i0, ..., :] = rfft(operand[i0, ..., :])</code>.</li>
<li>
<code>result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1])</code>.</li>
<li>
<code>result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1])</code>.</li>
</ul><p>Finally, given the function <code>irfft</code> which has the same type signature and computes the inverse of <code>rfft</code>:</p>
<p>For <code>fft_type = IRFFT</code>, <code>result</code> is defined as the inverse of the computations for <code>fft_type = RFFT</code>. For example, for <code>L = 3</code>:</p>
<ul><li>
<code>result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1])</code>.</li>
<li>
<code>result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1])</code>.</li>
<li>
<code>result[i0, ..., :] = irfft(result2[i0, ..., :])</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-42">Inputs<a class="anchor" aria-label="anchor" href="#inputs-42"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="15%"><col width="50%"><col width="26%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type</td>
<td>(C1), (C2), (C4), (C5)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>fft_type</code></td>
<td>enum of <code>FFT</code>, <code>IFFT</code>, <code>RFFT</code>, and <code>IRFFT</code>
</td>
<td>(C2), (C5)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>fft_length</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C3), (C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-42">Outputs<a class="anchor" aria-label="anchor" href="#outputs-42"></a></h4>
<table class="table"><colgroup><col width="14%"><col width="60%"><col width="25%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type</td>
<td>(C2), (C4), (C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-40">Constraints<a class="anchor" aria-label="anchor" href="#constraints-40"></a></h4>
<ul><li>(C1) <code>size(fft_length) &lt;= rank(operand)</code>.</li>
<li>(C2) The relationship between <code>operand</code> and <code>result</code> element types varies:
<ul><li>If <code>fft_type = FFT</code>, <code>element_type(operand)</code> and <code>element_type(result)</code> have the same complex type.</li>
<li>If <code>fft_type = IFFT</code>, <code>element_type(operand)</code> and <code>element_type(result)</code> have the same complex type.</li>
<li>If <code>fft_type = RFFT</code>, <code>element_type(operand)</code> is a floating-point type and <code>element_type(result)</code> is a complex type of the same floating-point semantics.</li>
<li>If <code>fft_type = IRFFT</code>, <code>element_type(operand)</code> is a complex type and <code>element_type(result)</code> is a floating-point type of the same floating-point semantics.</li>
</ul></li>
<li>(C3) <code>1 &lt;= size(fft_length) &lt;= 3</code>.</li>
<li>(C4) If among <code>operand</code> and <code>result</code>, there is a tensor <code>real</code> of a floating-point type, then <code>shape(real)[-size(fft_length):] = fft_length</code>.</li>
<li>(C5) <code>shape(result) = shape(operand)</code> except for:
<ul><li>If <code>fft_type = RFFT</code>, <code>dim(result, -1) = dim(operand, -1) = 0 ? 0 : dim(operand, -1) / 2 + 1</code>.</li>
<li>If <code>fft_type = IRFFT</code>, <code>dim(operand, -1) = dim(result, -1) = 0 ? 0 : dim(result, -1) / 2 + 1</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-42">Examples<a class="anchor" aria-label="anchor" href="#examples-42"></a></h4>
<pre class="mlir"><code>// %operand: [(1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]
%result = "stablehlo.fft"(%operand) {
  fft_type = #stablehlo&lt;fft_type FFT&gt;,
  fft_length = array&lt;i64: 4&gt;
} : (tensor&lt;4xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;4xcomplex&lt;f32&gt;&gt;
// %result: [(1.0, 0.0), (1.0, 0.0), (1.0, 0.0), (1.0, 0.0)]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="floor">floor<a class="anchor" aria-label="anchor" href="#floor"></a></h3>
<div class="section level4">
<h4 id="semantics-43">Semantics<a class="anchor" aria-label="anchor" href="#semantics-43"></a></h4>
<p>Performs element-wise floor of <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTowardNegative</code> operation from the IEEE-754 specification. For quantized types, performs <code>dequantize_op_quantize(floor, operand, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-43">Inputs<a class="anchor" aria-label="anchor" href="#inputs-43"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="7%"><col width="11%"><col width="66%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-43">Outputs<a class="anchor" aria-label="anchor" href="#outputs-43"></a></h4>
<table class="table"><colgroup><col width="11%"><col width="72%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-41">Constraints<a class="anchor" aria-label="anchor" href="#constraints-41"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-43">Examples<a class="anchor" aria-label="anchor" href="#examples-43"></a></h4>
<pre class="mlir"><code>// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0]
%result = "stablehlo.floor"(%operand) : (tensor&lt;5xf32&gt;) -&gt; tensor&lt;5xf32&gt;
// %result: [-1.0, -1.0, 0.0, 0.0, 2.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/floor.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="gather">gather<a class="anchor" aria-label="anchor" href="#gather"></a></h3>
<div class="section level4">
<h4 id="semantics-44">Semantics<a class="anchor" aria-label="anchor" href="#semantics-44"></a></h4>
<p>Gathers slices from <code>operand</code> tensor from offsets specified in <code>start_indices</code> and produces a <code>result</code> tensor.</p>
<p>The following diagram shows how elements in <code>result</code> map on elements in <code>operand</code> using a concrete example. The diagram picks a few example <code>result</code> indices and explains in detail which <code>operand</code> indices they correspond to.</p>
<div class="float">
<img src="images/spec/gather.svg" alt="gather"><div class="figcaption">gather</div>
</div>
<p>More formally, <code>result[result_index] = operand[operand_index]</code> where:</p>
<!-- markdownlint-disable line-length -->
<ul><li>
<code>batch_dims = [d for d in axes(result) and d not in offset_dims]</code>.</li>
<li>
<code>batch_index = result_index[batch_dims...]</code>.</li>
<li>
<code>start_index</code> is defined as:
<ul><li>
<code>start_indices[bi0, ..., :, ..., biN]</code> where <code>bi</code> are individual elements in <code>batch_index</code> and <code>:</code> is inserted at the <code>index_vector_dim</code> index, if <code>index_vector_dim</code> &lt; <code>rank(start_indices)</code>.</li>
<li>
<code>[start_indices[batch_index]]</code> otherwise.</li>
</ul></li>
<li>For <code>d_operand</code> in <code>axes(operand)</code>,
<ul><li>
<code>full_start_index[d_operand] = clamp(start_index[d_start], 0,   dim(operand, d_operand) - slice_sizes[d_operand])</code> if <code>d_operand = start_index_map[d_start]</code>.</li>
<li>
<code>full_start_index[d_operand] = 0</code> otherwise.</li>
</ul></li>
<li>For <code>d_operand</code> in <code>axes(operand)</code>,
<ul><li>
<code>full_batching_index[d_operand] =   batch_index[d_start - (d_start &lt; index_vector_dim ? 0 : 1)]</code> if <code>d_operand = operand_batching_dims[i_batching]</code> and <code>d_start = start_indices_batching_dims[i_batching]</code>.</li>
<li>
<code>full_batching_index[d_operand] = 0</code> otherwise.</li>
</ul></li>
<li>
<code>offset_index = result_index[offset_dims...]</code>.</li>
<li>
<code>full_offset_index = [oi0, ..., 0, ..., oiN]</code> where <code>oi</code> are individual elements in <code>offset_index</code>, and <code>0</code> is inserted at indices from <code>collapsed_slice_dims</code> and <code>operand_batching_dims</code>.</li>
<li>
<code>operand_index = full_start_index + full_batching_index + full_offset_index</code>. <!-- markdownlint-enable line-length -->
</li>
</ul><p>If <code>indices_are_sorted</code> is <code>true</code> then the implementation can assume that <code>start_indices</code> are sorted with respect to <code>start_index_map</code>, otherwise the behavior is undefined. More formally, for all <code>i1 &lt; i2</code> from <code>indices(result)</code>, <code>full_start_index(i1) &lt;= full_start_index(i2)</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-44">Inputs<a class="anchor" aria-label="anchor" href="#inputs-44"></a></h4>
<table class="table"><colgroup><col width="5%"><col width="24%"><col width="35%"><col width="34%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C8), (C11), (C17), (C19-C21), (C23)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>start_indices</code></td>
<td>tensor of integer type</td>
<td>(C2-C3), (C14), (C17), (C22)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>offset_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C4-C5), (C22)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>collapsed_slice_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C6-C9), (C22)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>operand_batching_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C6), (C10-C12), (C16-C18), (C22)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>start_indices_batching_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C13-C17)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>start_index_map</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C3), (C18-C19)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>index_vector_dim</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C2-C3), (C15), (C22)</td>
</tr><tr class="odd"><td>(I9)</td>
<td><code>slice_sizes</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C9), (C12), (C20-C22)</td>
</tr><tr class="even"><td>(I10)</td>
<td><code>indices_are_sorted</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-44">Outputs<a class="anchor" aria-label="anchor" href="#outputs-44"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C5), (C22-C23)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-42">Constraints<a class="anchor" aria-label="anchor" href="#constraints-42"></a></h4>
<ul><li>(C1) <code>rank(operand) = size(offset_dims) + size(collapsed_slice_dims) +        size(operand_batching_dims)</code>.</li>
<li>(C2) <code>0 &lt;= index_vector_dim &lt;= rank(start_indices)</code>.</li>
<li>(C3) <code>size(start_index_map) =        index_vector_dim &lt; rank(start_indices) ?        dim(start_indices, index_vector_dim) : 1</code>.</li>
<li>(C4) <code>is_unique(offset_dims) and is_sorted(offset_dims)</code>.</li>
<li>(C5) <code>0 &lt;= offset_dims &lt; rank(result)</code>.</li>
<li>(C6) <code>is_unique(concatenate(collapsed_slice_dims, operand_batching_dims))</code>
</li>
<li>(C7) <code>is_sorted(collapsed_slice_dims)</code>.</li>
<li>(C8) <code>0 &lt;= collapsed_slice_dims &lt; rank(operand)</code>.</li>
<li>(C9) <code>slice_sizes[collapsed_slice_dims...] &lt;= 1</code>.</li>
<li>(C10) <code>is_sorted(operand_batching_dims)</code>.</li>
<li>(C11) <code>0 &lt;= operand_batching_dims &lt; rank(operand)</code>.</li>
<li>(C12) <code>slice_sizes[operand_batching_dims...] &lt;= 1</code>.</li>
<li>(C13) <code>is_unique(start_indices_batching_dims)</code>.</li>
<li>(C14) <code>0 &lt;= start_indices_batching_dims &lt; rank(start_indices)</code>.</li>
<li>(C15) <code>index_vector_dim not in start_indices_batching_dims</code>.</li>
<li>(C16) <code>size(operand_batching_dims) == size(start_indices_batching_dims)</code>.</li>
<li>(C17) <code>dim(operand, operand_batching_dims...) =         dim(start_indices, start_indices_batching_dims...)</code>.</li>
<li>(C18) <code>is_unique(concatenate(start_index_map, operand_batching_dims))</code>.</li>
<li>(C19) <code>0 &lt;= start_index_map &lt; rank(operand)</code>.</li>
<li>(C20) <code>size(slice_sizes) = rank(operand)</code>.</li>
<li>(C21) <code>0 &lt;= slice_sizes &lt;= shape(operand)</code>.</li>
<li>(C22) <code>shape(result) = combine(batch_dim_sizes, offset_dim_sizes)</code> where:
<ul><li>
<code>batch_dim_sizes = shape(start_indices)</code> except that the dimension size of <code>start_indices</code> corresponding to <code>index_vector_dim</code> is not included.</li>
<li>
<code>offset_dim_sizes = slice_sizes</code> except that the dimension sizes in <code>slice_sizes</code> corresponding to <code>collapsed_slice_dims</code> and <code>operand_batching_dims</code> are not included.</li>
<li>
<code>combine</code> puts <code>batch_dim_sizes</code> at axes corresponding to <code>batch_dims</code> and <code>offset_dim_sizes</code> at axes corresponding to <code>offset_dims</code>.</li>
</ul></li>
<li>(C23) <code>element_type(operand) = element_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-44">Examples<a class="anchor" aria-label="anchor" href="#examples-44"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [
//             [[1, 2], [3, 4], [5, 6], [7, 8]],
//             [[9, 10],[11, 12], [13, 14], [15, 16]],
//             [[17, 18], [19, 20], [21, 22], [23, 24]]
//            ],
//            [
//             [[25, 26], [27, 28], [29, 30], [31, 32]],
//             [[33, 34], [35, 36], [37, 38], [39, 40]],
//             [[41, 42], [43, 44], [45, 46], [47, 48]]
//            ]
//           ]
// %start_indices: [
//                  [
//                   [[0, 0], [1, 0], [2, 1]],
//                   [[0, 1], [1, 1], [0, 9]]
//                  ],
//                  [
//                   [[0, 0], [2, 1], [2, 2]],
//                   [[1, 2], [0, 1], [1, 0]]
//                  ]
//                 ]
%result = "stablehlo.gather"(%operand, %start_indices) {
  dimension_numbers = #stablehlo.gather&lt;
    offset_dims = [3, 4],
    collapsed_slice_dims = [1],
    operand_batching_dims = [0],
    start_indices_batching_dims = [1],
    start_index_map = [2, 1],
    index_vector_dim = 3&gt;,
  slice_sizes = array&lt;i64: 1, 1, 2, 2&gt;,
  indices_are_sorted = false
} : (tensor&lt;2x3x4x2xi32&gt;, tensor&lt;2x2x3x2xi64&gt;) -&gt; tensor&lt;2x2x3x2x2xi32&gt;
// %result: [
//           [
//            [
//             [[1, 2], [3, 4]],
//             [[3, 4], [5, 6]],
//             [[13, 14], [15, 16]]
//            ],
//            [
//             [[33, 34], [35, 36]],
//             [[35, 36], [37, 38]],
//             [[41, 42], [43, 44]]
//            ]
//           ],
//           [
//            [
//             [[1, 2], [3, 4]],
//             [[13, 14], [15, 16]],
//             [[21, 22], [23, 24]]
//            ],
//            [
//             [[43, 44], [45, 46]],
//             [[33, 34], [35, 36]],
//             [[27, 28], [29, 30]]
//            ]
//           ]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/gather.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="get_dimension_size">get_dimension_size<a class="anchor" aria-label="anchor" href="#get_dimension_size"></a></h3>
<div class="section level4">
<h4 id="semantics-45">Semantics<a class="anchor" aria-label="anchor" href="#semantics-45"></a></h4>
<p>Produces the size of the given <code>dimension</code> of the <code>operand</code>. More formally, <code>result = dim(operand, dimension)</code>. The Semantics concerns only with the shape component of the type. The element-type could be anything.</p>
</div>
<div class="section level4">
<h4 id="inputs-45">Inputs<a class="anchor" aria-label="anchor" href="#inputs-45"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-45">Outputs<a class="anchor" aria-label="anchor" href="#outputs-45"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>0-dimensional tensor of type <code>si32</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-43">Constraints<a class="anchor" aria-label="anchor" href="#constraints-43"></a></h4>
<ul><li>(C1) <code>0 &lt;= dimension &lt; rank(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-45">Examples<a class="anchor" aria-label="anchor" href="#examples-45"></a></h4>
<pre class="mlir"><code>// %operand: [[1, 2, 3], [4, 5, 6]]
%result = "stablehlo.get_dimension_size"(%operand) {
  dimension = 1 : i64
} : (tensor&lt;2x3xi64&gt;) -&gt; tensor&lt;i32&gt;
// %result: 3</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/get_dimension_size.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="get_tuple_element">get_tuple_element<a class="anchor" aria-label="anchor" href="#get_tuple_element"></a></h3>
<blockquote>
<p>Note: Per <a href="https://github.com/openxla/stablehlo/pull/2283" class="external-link">StableHLO v1.0 Cleanup #2283</a>, this op is being explored for deprecation as it appears to be unused by both frameworks and compilers. As such, it has limited compatibility guarantees (6 months).</p>
</blockquote>
<div class="section level4">
<h4 id="semantics-46">Semantics<a class="anchor" aria-label="anchor" href="#semantics-46"></a></h4>
<p>Extracts element at <code>index</code> position of the <code>operand</code> tuple and produces a <code>result</code>. More formally, <code>result = operand[index]</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-46">Inputs<a class="anchor" aria-label="anchor" href="#inputs-46"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tuple</td>
<td>(C1), (C2)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>index</code></td>
<td>constant of type <code>si32</code>
</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-46">Outputs<a class="anchor" aria-label="anchor" href="#outputs-46"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>any supported type</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-44">Constraints<a class="anchor" aria-label="anchor" href="#constraints-44"></a></h4>
<ul><li>(C1) <code>0 &lt;= index &lt; size(operand)</code>.</li>
<li>(C2) <code>type(result) = tuple_element_types(operand)[index]</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-46">Examples<a class="anchor" aria-label="anchor" href="#examples-46"></a></h4>
<pre class="mlir"><code>// %operand: ([1.0, 2.0], (3))
%result = "stablehlo.get_tuple_element"(%operand) &lt;{index = 0 : i32}&gt; : (tuple&lt;tensor&lt;2xf64&gt;, tuple&lt;tensor&lt;i64&gt;&gt;&gt;) -&gt; tensor&lt;2xf64&gt;
// %result: [1.0, 2.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/tuple_and_get_tuple_element.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="if">if<a class="anchor" aria-label="anchor" href="#if"></a></h3>
<div class="section level4">
<h4 id="semantics-47">Semantics<a class="anchor" aria-label="anchor" href="#semantics-47"></a></h4>
<p>Produces the output from executing exactly one function from <code>true_branch</code> or <code>false_branch</code> depending on the value of <code>pred</code>. More formally, <code>result = pred ? true_branch() : false_branch()</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-47">Inputs<a class="anchor" aria-label="anchor" href="#inputs-47"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="22%"><col width="49%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>pred</code></td>
<td>0-dimensional tensor of type <code>i1</code>
</td>
<td></td>
</tr><tr class="even"><td>(I2)</td>
<td><code>true_branch</code></td>
<td>function</td>
<td>(C1-C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>false_branch</code></td>
<td>function</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-47">Outputs<a class="anchor" aria-label="anchor" href="#outputs-47"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="70%"><col width="16%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors, quantized tensors or tokens</td>
<td>(C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-45">Constraints<a class="anchor" aria-label="anchor" href="#constraints-45"></a></h4>
<ul><li>(C1) <code>input_types(true_branch) = input_types(false_branch) = []</code>.</li>
<li>(C2) <code>output_types(true_branch) = output_types(false_branch)</code>.</li>
<li>(C3) <code>type(results...) = output_types(true_branch)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-47">Examples<a class="anchor" aria-label="anchor" href="#examples-47"></a></h4>
<pre class="mlir"><code>// %result_true_branch: 10
// %result_false_branch: 11
// %pred: true
%result = "stablehlo.if"(%pred) ({
  "stablehlo.return"(%result_true_branch) : (tensor&lt;i32&gt;) -&gt; ()
}, {
  "stablehlo.return"(%result_false_branch) : (tensor&lt;i32&gt;) -&gt; ()
}) : (tensor&lt;i1&gt;) -&gt; tensor&lt;i32&gt;
// %result: 10</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/if.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="imag">imag<a class="anchor" aria-label="anchor" href="#imag"></a></h3>
<div class="section level4">
<h4 id="semantics-48">Semantics<a class="anchor" aria-label="anchor" href="#semantics-48"></a></h4>
<p>Extracts the imaginary part, element-wise, from the <code>operand</code> and produces a <code>result</code> tensor. More formally, for each element <code>x</code>: <code>imag(x) = is_complex(x) ? imaginary_part(x) : constant(0, element_type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-48">Inputs<a class="anchor" aria-label="anchor" href="#inputs-48"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="15%"><col width="57%"><col width="17%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-48">Outputs<a class="anchor" aria-label="anchor" href="#outputs-48"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-46">Constraints<a class="anchor" aria-label="anchor" href="#constraints-46"></a></h4>
<ul><li>(C1) <code>shape(result) = shape(operand)</code>.</li>
<li>(C2) <code>element_type(result)</code> is defined as:
<ul><li>
<code>complex_element_type(element_type(operand))</code> if <code>is_complex(operand)</code>.</li>
<li>
<code>element_type(operand)</code> otherwise.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-48">Examples<a class="anchor" aria-label="anchor" href="#examples-48"></a></h4>
<pre class="mlir"><code>// %operand: [(1.0, 2.0), (3.0, 4.0)]
%result = "stablehlo.imag"(%operand) : (tensor&lt;2xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;2xf32&gt;
// %result: [2.0, 4.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/imag.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="infeed">infeed<a class="anchor" aria-label="anchor" href="#infeed"></a></h3>
<div class="section level4">
<h4 id="semantics-49">Semantics<a class="anchor" aria-label="anchor" href="#semantics-49"></a></h4>
<p>Reads data from the infeed and produces <code>results</code>.</p>
<p>Semantics of <code>infeed_config</code> is implementation-defined.</p>
<p><code>results</code> consist of payload values which come first and a token which comes last. In the future, we are planning to split the payload and the token into two separate outputs to improve clarity (<a href="https://github.com/openxla/stablehlo/issues/670" class="external-link">#670</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-49">Inputs<a class="anchor" aria-label="anchor" href="#inputs-49"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>token</code></td>
<td><code>token</code></td>
</tr><tr class="even"><td>(I2)</td>
<td><code>infeed_config</code></td>
<td>constant of type <code>string</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-49">Outputs<a class="anchor" aria-label="anchor" href="#outputs-49"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="70%"><col width="16%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors, quantized tensors or tokens</td>
<td>(C1-C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-47">Constraints<a class="anchor" aria-label="anchor" href="#constraints-47"></a></h4>
<ul><li>(C1) <code>0 &lt; size(results)</code>.</li>
<li>(C2) <code>is_empty(result[:-1])</code> or <code>is_tensor(type(results[:-1]))</code>.</li>
<li>(C3) <code>is_token(type(results[-1]))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-49">Examples<a class="anchor" aria-label="anchor" href="#examples-49"></a></h4>
<pre class="mlir"><code>// %token: !stablehlo.token
// infeed_queue[0]: [[1, 2], [3, 4]]
// infeed_queue[1]: [[5, 6], [7, 8]]
%results0:2 = "stablehlo.infeed"(%token) {
  infeed_config = ""
} : (!stablehlo.token) -&gt; (tensor&lt;2x2xi64&gt;, !stablehlo.token)
// results0#0: [[1, 2], [3, 4]]
%results1:2 = "stablehlo.infeed"(%token) {
  infeed_config = ""
} : (!stablehlo.token) -&gt; (tensor&lt;2x2xi64&gt;, !stablehlo.token)
// results1#0: [[5, 6], [7, 8]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/infeed.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="iota">iota<a class="anchor" aria-label="anchor" href="#iota"></a></h3>
<div class="section level4">
<h4 id="semantics-50">Semantics<a class="anchor" aria-label="anchor" href="#semantics-50"></a></h4>
<p>Fills an <code>output</code> tensor with values in increasing order starting from zero along the <code>iota_dimension</code> dimension. More formally,</p>
<p><code>output[output_index] = constant(is_quantized(output) ? quantize(output_index[iota_dimension], element_type(output)) : output_index[iota_dimension], element_type(output))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-50">Inputs<a class="anchor" aria-label="anchor" href="#inputs-50"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>iota_dimension</code></td>
<td><code>si64</code></td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-50">Outputs<a class="anchor" aria-label="anchor" href="#outputs-50"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>output</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-48">Constraints<a class="anchor" aria-label="anchor" href="#constraints-48"></a></h4>
<ul><li>(C1) <code>0 &lt;= iota_dimension &lt; rank(output)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-50">Examples<a class="anchor" aria-label="anchor" href="#examples-50"></a></h4>
<pre class="mlir"><code>%output = "stablehlo.iota"() {
  iota_dimension = 0 : i64
} : () -&gt; tensor&lt;4x5xi32&gt;
// %output: [
//           [0, 0, 0, 0, 0],
//           [1, 1, 1, 1, 1],
//           [2, 2, 2, 2, 2],
//           [3, 3, 3, 3, 3]
//          ]

%output = "stablehlo.iota"() {
  iota_dimension = 1 : i64
} : () -&gt; tensor&lt;4x5xi32&gt;
// %output: [
//           [0, 1, 2, 3, 4],
//           [0, 1, 2, 3, 4],
//           [0, 1, 2, 3, 4],
//           [0, 1, 2, 3, 4]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/iota.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="is_finite">is_finite<a class="anchor" aria-label="anchor" href="#is_finite"></a></h3>
<div class="section level4">
<h4 id="semantics-51">Semantics<a class="anchor" aria-label="anchor" href="#semantics-51"></a></h4>
<p>Performs element-wise check whether the value in <code>x</code> is finite (i.e. is neither +Inf, -Inf, nor NaN) and produces a <code>y</code> tensor. Implements the <code>isFinite</code> operation from the IEEE-754 specification. For quantized types, the result is always <code>true</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-51">Inputs<a class="anchor" aria-label="anchor" href="#inputs-51"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="6%"><col width="70%"><col width="14%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>x</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-51">Outputs<a class="anchor" aria-label="anchor" href="#outputs-51"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>y</code></td>
<td>tensor of boolean type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-49">Constraints<a class="anchor" aria-label="anchor" href="#constraints-49"></a></h4>
<ul><li>(C1) <code>shape(x) = shape(y)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-51">Examples<a class="anchor" aria-label="anchor" href="#examples-51"></a></h4>
<pre class="mlir"><code>// Logical values: -Inf, +Inf, NaN, ...
// %x: [0xFFF0000000000000, 0x7FF0000000000000, 0x7FF8000000000000, -10.0, -0.0, 0.0, 10.0]
%y = "stablehlo.is_finite"(%x) : (tensor&lt;7xf64) -&gt; tensor&lt;7xi1&gt;
// %y: [false, false, false, true, true, true, true]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/is_finite.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="log">log<a class="anchor" aria-label="anchor" href="#log"></a></h3>
<div class="section level4">
<h4 id="semantics-52">Semantics<a class="anchor" aria-label="anchor" href="#semantics-52"></a></h4>
<p>Performs element-wise logarithm operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>log</code> from IEEE-754.</li>
<li>For complex numbers: complex logarithm.</li>
<li>For quantized types: <code>dequantize_op_quantize(log, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-52">Inputs<a class="anchor" aria-label="anchor" href="#inputs-52"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-52">Outputs<a class="anchor" aria-label="anchor" href="#outputs-52"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-50">Constraints<a class="anchor" aria-label="anchor" href="#constraints-50"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-52">Examples<a class="anchor" aria-label="anchor" href="#examples-52"></a></h4>
<pre class="mlir"><code>// %operand: [[1.0, 2.0], [3.0, 4.0]]
%result = "stablehlo.log"(%operand) : (tensor&lt;2x2xf64&gt;) -&gt; tensor&lt;2x2xf64&gt;
// %result: [[0.0, 0.69314718055994529], [1.0986122886681098, 1.3862943611198906]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/log.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="log_plus_one">log_plus_one<a class="anchor" aria-label="anchor" href="#log_plus_one"></a></h3>
<div class="section level4">
<h4 id="semantics-53">Semantics<a class="anchor" aria-label="anchor" href="#semantics-53"></a></h4>
<p>Performs element-wise logarithm plus one operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>logp1</code> from IEEE-754.</li>
<li>For complex numbers: <code>complex(log(hypot(real(x) + 1, imag(x))), atan2(imag(x), real(x) + 1))</code>
</li>
<li>For quantized types: <code>dequantize_op_quantize(log_plus_one, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-53">Inputs<a class="anchor" aria-label="anchor" href="#inputs-53"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-53">Outputs<a class="anchor" aria-label="anchor" href="#outputs-53"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-51">Constraints<a class="anchor" aria-label="anchor" href="#constraints-51"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-53">Examples<a class="anchor" aria-label="anchor" href="#examples-53"></a></h4>
<pre class="mlir"><code>// %operand: [0.0, -0.999, 7.0, 6.38905621, 15.0]
%result = "stablehlo.log_plus_one"(%operand) : (tensor&lt;5xf64&gt;) -&gt; tensor&lt;5xf64&gt;
// %result: [0.0, -6.90776825, 2.07944155, 2.0, 2.77258873]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/log_plus_one.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="logistic">logistic<a class="anchor" aria-label="anchor" href="#logistic"></a></h3>
<div class="section level4">
<h4 id="semantics-54">Semantics<a class="anchor" aria-label="anchor" href="#semantics-54"></a></h4>
<p>Performs element-wise logistic operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>division(1, addition(1, exp(-x)))</code> from IEEE-754.</li>
<li>For complex numbers: complex logistic.</li>
<li>For quantized types: <code>dequantize_op_quantize(logistic, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-54">Inputs<a class="anchor" aria-label="anchor" href="#inputs-54"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-54">Outputs<a class="anchor" aria-label="anchor" href="#outputs-54"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-52">Constraints<a class="anchor" aria-label="anchor" href="#constraints-52"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-54">Examples<a class="anchor" aria-label="anchor" href="#examples-54"></a></h4>
<pre class="mlir"><code>// %operand: [[0.0, 1.0], [2.0, 3.0]]
%result = "stablehlo.logistic"(%operand) : (tensor&lt;2x2xf64&gt;) -&gt; tensor&lt;2x2xf64&gt;
// %result: [[0.5, 0.73105858], [0.88079708, 0.95257413]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/logistic.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="map">map<a class="anchor" aria-label="anchor" href="#map"></a></h3>
<blockquote>
<p>Note: Per <a href="https://github.com/openxla/stablehlo/pull/2283" class="external-link">StableHLO v1.0 Cleanup #2283</a>, this op is being explored for deprecation as it appears to be unused by both frameworks and compilers. As such, it has limited compatibility guarantees (6 months).</p>
</blockquote>
<div class="section level4">
<h4 id="semantics-55">Semantics<a class="anchor" aria-label="anchor" href="#semantics-55"></a></h4>
<p>Applies a map function <code>computation</code> to <code>inputs</code> along the <code>dimensions</code> and produces a <code>result</code> tensor.</p>
<p>More formally, <code>result[result_index] = computation(inputs...[result_index])</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-55">Inputs<a class="anchor" aria-label="anchor" href="#inputs-55"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="15%"><col width="63%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1-C4)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>computation</code></td>
<td>function</td>
<td>(C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-55">Outputs<a class="anchor" aria-label="anchor" href="#outputs-55"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-53">Constraints<a class="anchor" aria-label="anchor" href="#constraints-53"></a></h4>
<ul><li>(C1) <code>shape(inputs...) = shape(result)</code>.</li>
<li>(C2) <code>0 &lt; size(inputs) = N</code>.</li>
<li>(C3) <code>dimensions = range(rank(inputs[0]))</code>.</li>
<li>(C4) <code>computation</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;) -&gt; tensor&lt;E'&gt;</code> where <code>Ei = element_type(inputs[i])</code> and <code>E' = element_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-55">Examples<a class="anchor" aria-label="anchor" href="#examples-55"></a></h4>
<pre class="mlir"><code>// %input0: [[0, 1], [2, 3]]
// %input1: [[4, 5], [6, 7]]
%result = "stablehlo.map"(%input0, %input1) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = stablehlo.multiply %arg0, %arg1 : tensor&lt;i64&gt;
    stablehlo.return %0 : tensor&lt;i64&gt;
}) {
  dimensions = array&lt;i64: 0, 1&gt;
} : (tensor&lt;2x2xi64&gt;, tensor&lt;2x2xi64&gt;) -&gt; tensor&lt;2x2xi64&gt;
// %result: [[0, 5], [12, 21]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/map.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="maximum">maximum<a class="anchor" aria-label="anchor" href="#maximum"></a></h3>
<div class="section level4">
<h4 id="semantics-56">Semantics<a class="anchor" aria-label="anchor" href="#semantics-56"></a></h4>
<p>Performs element-wise max operation on tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical OR.</li>
<li>For integers: integer maximum.</li>
<li>For floats: <code>maximum</code> from IEEE-754.</li>
<li>For complex numbers: lexicographic maximum for the <code>(real, imaginary)</code> pair. Imposing an ordering on complex numbers involves surprising semantics, so in the future we are planning to remove support for complex numbers for this operation (<a href="https://github.com/openxla/stablehlo/issues/560" class="external-link">#560</a>).</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(maximum, lhs, rhs, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-56">Inputs<a class="anchor" aria-label="anchor" href="#inputs-56"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-56">Outputs<a class="anchor" aria-label="anchor" href="#outputs-56"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-54">Constraints<a class="anchor" aria-label="anchor" href="#constraints-54"></a></h4>
<ul><li>(C1) <code>baseline_type(lhs) = baseline_type(rhs) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-56">Examples<a class="anchor" aria-label="anchor" href="#examples-56"></a></h4>
<pre class="mlir"><code>// %lhs: [[1, 2], [7, 8]]
// %rhs: [[5, 6], [3, 4]]
%result = "stablehlo.maximum"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[5, 6], [7, 8]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/maximum.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="minimum">minimum<a class="anchor" aria-label="anchor" href="#minimum"></a></h3>
<div class="section level4">
<h4 id="semantics-57">Semantics<a class="anchor" aria-label="anchor" href="#semantics-57"></a></h4>
<p>Performs element-wise min operation on tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical AND.</li>
<li>For integers: integer minimum.</li>
<li>For floats: <code>minimum</code> from IEEE-754.</li>
<li>For complex numbers: lexicographic minimum for the <code>(real, imaginary)</code> pair. Imposing an ordering on complex numbers involves surprising semantics, so in the future we are planning to remove support for complex numbers for this operation (<a href="https://github.com/openxla/stablehlo/issues/560" class="external-link">#560</a>).</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(minimum, lhs, rhs, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-57">Inputs<a class="anchor" aria-label="anchor" href="#inputs-57"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-57">Outputs<a class="anchor" aria-label="anchor" href="#outputs-57"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-55">Constraints<a class="anchor" aria-label="anchor" href="#constraints-55"></a></h4>
<ul><li>(C1) <code>baseline_type(lhs) = baseline_type(rhs) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-57">Examples<a class="anchor" aria-label="anchor" href="#examples-57"></a></h4>
<pre class="mlir"><code>// %lhs: [[1, 2], [7, 8]]
// %rhs: [[5, 6], [3, 4]]
%result = "stablehlo.minimum"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[1, 2], [3, 4]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/minimum.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="multiply">multiply<a class="anchor" aria-label="anchor" href="#multiply"></a></h3>
<div class="section level4">
<h4 id="semantics-58">Semantics<a class="anchor" aria-label="anchor" href="#semantics-58"></a></h4>
<p>Performs element-wise product of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical AND.</li>
<li>For integers: integer multiplication.</li>
<li>For floats: <code>multiplication</code> from IEEE-754.</li>
<li>For complex numbers: complex multiplication.</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(multiply, lhs, rhs, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-58">Inputs<a class="anchor" aria-label="anchor" href="#inputs-58"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-58">Outputs<a class="anchor" aria-label="anchor" href="#outputs-58"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-56">Constraints<a class="anchor" aria-label="anchor" href="#constraints-56"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-58">Examples<a class="anchor" aria-label="anchor" href="#examples-58"></a></h4>
<pre class="mlir"><code>// %lhs: [[1, 2], [3, 4]]
// %rhs: [[5, 6], [7, 8]]
%result = "stablehlo.multiply"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[5, 12], [21, 32]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/multiply.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="negate">negate<a class="anchor" aria-label="anchor" href="#negate"></a></h3>
<div class="section level4">
<h4 id="semantics-59">Semantics<a class="anchor" aria-label="anchor" href="#semantics-59"></a></h4>
<p>Performs element-wise negation of <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For signed integers: integer negation.</li>
<li>For unsigned integers: bitcast to signed integer, integer negation, bitcast back to unsigned integer.</li>
<li>For floats: <code>negate</code> from IEEE-754.</li>
<li>For complex numbers: complex negation.</li>
<li>For quantized types: <code>dequantize_op_quantize(negate, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-59">Inputs<a class="anchor" aria-label="anchor" href="#inputs-59"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="9%"><col width="72%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-59">Outputs<a class="anchor" aria-label="anchor" href="#outputs-59"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-57">Constraints<a class="anchor" aria-label="anchor" href="#constraints-57"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-59">Examples<a class="anchor" aria-label="anchor" href="#examples-59"></a></h4>
<pre class="mlir"><code>// Negation operation with integer Tensors
// %operand: [0, -2]
%result = "stablehlo.negate"(%operand) : (tensor&lt;2xi32&gt;) -&gt; tensor&lt;2xi32&gt;
// %result: [0, 2]

// Negation operation with with complex tensors
// %operand: (2.5, 0.0)
%result = "stablehlo.negate"(%operand) : (tensor&lt;1xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;1xcomplex&lt;f32&gt;&gt;
// %result: [-2.5, -0.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/negate.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="not">not<a class="anchor" aria-label="anchor" href="#not"></a></h3>
<div class="section level4">
<h4 id="semantics-60">Semantics<a class="anchor" aria-label="anchor" href="#semantics-60"></a></h4>
<p>Performs element-wise NOT of tensor <code>operand</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical NOT.</li>
<li>For integers: bitwise NOT.</li>
</ul></div>
<div class="section level4">
<h4 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>operand</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-60">Outputs<a class="anchor" aria-label="anchor" href="#outputs-60"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-58">Constraints<a class="anchor" aria-label="anchor" href="#constraints-58"></a></h4>
<ul><li>(C1) <code>type(operand) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-60">Examples<a class="anchor" aria-label="anchor" href="#examples-60"></a></h4>
<pre class="mlir"><code>// Bitwise operation with with integer tensors
// %operand: [[1, 2], [3, 4]]
%result = "stablehlo.not"(%operand) : (tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[-2, -3], [-4, -5]]

// Bitwise operation with with boolean tensors
// %operand: [true, false]
%result = "stablehlo.not"(%operand) : (tensor&lt;2xi1&gt;) -&gt; tensor&lt;2xi1&gt;
// %result: [false, true]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/not.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="optimization_barrier">optimization_barrier<a class="anchor" aria-label="anchor" href="#optimization_barrier"></a></h3>
<div class="section level4">
<h4 id="semantics-61">Semantics<a class="anchor" aria-label="anchor" href="#semantics-61"></a></h4>
<p>Ensures that the operations that produce the <code>operand</code> are executed before any operations that depend on the <code>result</code> and prevents compiler transformations from moving operations across the barrier. Other than that, the operation is an identity, i.e. <code>result = operand</code>.</p>
</div>
<div class="section level4">
<h4 id="arguments-1">Arguments<a class="anchor" aria-label="anchor" href="#arguments-1"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="11%"><col width="73%"><col width="14%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>operand</code></td>
<td>variadic number of tensors, per-tensor quantized tensors or tokens</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-61">Outputs<a class="anchor" aria-label="anchor" href="#outputs-61"></a></h4>
<table class="table"><colgroup><col width="10%"><col width="74%"><col width="14%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>variadic number of tensors, per-tensor quantized tensors or tokens</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-59">Constraints<a class="anchor" aria-label="anchor" href="#constraints-59"></a></h4>
<ul><li>(C1) <code>type(operand...) = type(result...)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-61">Examples<a class="anchor" aria-label="anchor" href="#examples-61"></a></h4>
<pre class="mlir"><code>// %operand0: 0.0
// %operand1: 1.0
%result0, %result1 = "stablehlo.optimization_barrier"(%operand0, %operand1) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; (tensor&lt;f32&gt;, tensor&lt;f32&gt;)
// %result0: 0.0
// %result1: 1.0</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/optimization_barrier.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="or">or<a class="anchor" aria-label="anchor" href="#or"></a></h3>
<div class="section level4">
<h4 id="semantics-62">Semantics<a class="anchor" aria-label="anchor" href="#semantics-62"></a></h4>
<p>Performs element-wise OR of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical OR.</li>
<li>For integers: bitwise OR.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-60">Inputs<a class="anchor" aria-label="anchor" href="#inputs-60"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer or boolean type</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer or boolean type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-62">Outputs<a class="anchor" aria-label="anchor" href="#outputs-62"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer or boolean type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-60">Constraints<a class="anchor" aria-label="anchor" href="#constraints-60"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-62">Examples<a class="anchor" aria-label="anchor" href="#examples-62"></a></h4>
<pre class="mlir"><code>// Bitwise operation with with integer tensors
// %lhs: [[1, 2], [3, 4]]
// %rhs: [[5, 6], [7, 8]]
%result = "stablehlo.or"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[5, 6], [7, 12]]

// Logical operation with with boolean tensors
// %lhs: [[false, false], [true, true]]
// %rhs: [[false, true], [false, true]]
%result = "stablehlo.or"(%lhs, %rhs) : (tensor&lt;2x2xi1&gt;, tensor&lt;2x2xi1&gt;) -&gt; tensor&lt;2x2xi1&gt;
// %result: [[false, true], [true, true]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/or.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="outfeed">outfeed<a class="anchor" aria-label="anchor" href="#outfeed"></a></h3>
<div class="section level4">
<h4 id="semantics-63">Semantics<a class="anchor" aria-label="anchor" href="#semantics-63"></a></h4>
<p>Writes <code>inputs</code> to the outfeed and produces a <code>result</code> token.</p>
<p>Semantics of <code>outfeed_config</code> is implementation-defined.</p>
</div>
<div class="section level4">
<h4 id="inputs-61">Inputs<a class="anchor" aria-label="anchor" href="#inputs-61"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="24%"><col width="66%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or quantized tensors</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>token</code></td>
<td><code>token</code></td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>outfeed_config</code></td>
<td>constant of type <code>string</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-63">Outputs<a class="anchor" aria-label="anchor" href="#outputs-63"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td><code>token</code></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="examples-63">Examples<a class="anchor" aria-label="anchor" href="#examples-63"></a></h4>
<pre class="mlir"><code>%result = "stablehlo.outfeed"(%input0, %token) {
  outfeed_config = ""
} : (tensor&lt;2x2x2xi64&gt;, !stablehlo.token) -&gt; !stablehlo.token</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/outfeed.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="pad">pad<a class="anchor" aria-label="anchor" href="#pad"></a></h3>
<div class="section level4">
<h4 id="semantics-64">Semantics<a class="anchor" aria-label="anchor" href="#semantics-64"></a></h4>
<p>Expands <code>operand</code> by padding around the tensor as well as between the elements of the tensor with the given <code>padding_value</code>.</p>
<p><code>edge_padding_low</code> and <code>edge_padding_high</code> specify the amount of padding added at the low-end (next to index 0) and the high-end (next to the highest index) of each dimension respectively. The amount of padding can be negative, where the absolute value of negative padding indicates the number of elements to remove from the specified dimension.</p>
<p><code>interior_padding</code> specifies the amount of padding added between any two elements in each dimension which may not be negative. Interior padding occurs before edge padding such that negative edge padding will remove elements from the interior-padded operand.</p>
<p>More formally, <code>result[result_index]</code> is defined as:</p>
<ul><li>
<code>operand[operand_index]</code> if <code>result_index = edge_padding_low + operand_index * (interior_padding + 1)</code>.</li>
<li>
<code>padding_value</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-62">Inputs<a class="anchor" aria-label="anchor" href="#inputs-62"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="21%"><col width="53%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C2), (C4)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>padding_value</code></td>
<td>0-dimensional tensor or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>edge_padding_low</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C4)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>edge_padding_high</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1), (C4)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>interior_padding</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-64">Outputs<a class="anchor" aria-label="anchor" href="#outputs-64"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C3-C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-61">Constraints<a class="anchor" aria-label="anchor" href="#constraints-61"></a></h4>
<ul><li>(C1) <code>element_type(operand) = element_type(padding_value) =   element_type(result)</code>.</li>
<li>(C2) <code>size(edge_padding_low) = size(edge_padding_high) =   size(interior_padding) = rank(operand)</code>.</li>
<li>(C3) <code>0 &lt;= interior_padding</code>.</li>
<li>(C4) <code>shape(result) = shape(operand) + edge_padding_low +   max(shape(operand) - 1, 0) * interior_padding + edge_padding_high</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-64">Examples<a class="anchor" aria-label="anchor" href="#examples-64"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [1, 2, 3],
//            [4, 5, 6]
//           ]
// %padding_value: 0
%result = "stablehlo.pad"(%operand, %padding_value) {
  edge_padding_low = array&lt;i64: 0, 1&gt;,
  edge_padding_high = array&lt;i64: 2, 1&gt;,
  interior_padding = array&lt;i64: 1, 2&gt;
} : (tensor&lt;2x3xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;5x9xi32&gt;
// %result: [
//           [0, 1, 0, 0, 2, 0, 0, 3, 0],
//           [0, 0, 0, 0, 0, 0, 0, 0, 0],
//           [0, 4, 0, 0, 5, 0, 0, 6, 0],
//           [0, 0, 0, 0, 0, 0, 0, 0, 0],
//           [0, 0, 0, 0, 0, 0, 0, 0, 0]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/pad.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="partition_id">partition_id<a class="anchor" aria-label="anchor" href="#partition_id"></a></h3>
<div class="section level4">
<h4 id="semantics-65">Semantics<a class="anchor" aria-label="anchor" href="#semantics-65"></a></h4>
<p>Produces <code>partition_id</code> of the current process.</p>
</div>
<div class="section level4">
<h4 id="outputs-65">Outputs<a class="anchor" aria-label="anchor" href="#outputs-65"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>0-dimensional tensor of type <code>ui32</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="examples-65">Examples<a class="anchor" aria-label="anchor" href="#examples-65"></a></h4>
<pre class="mlir"><code>%result = "stablehlo.partition_id"() : () -&gt; tensor&lt;ui32&gt;</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/partition_id.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="popcnt">popcnt<a class="anchor" aria-label="anchor" href="#popcnt"></a></h3>
<div class="section level4">
<h4 id="semantics-66">Semantics<a class="anchor" aria-label="anchor" href="#semantics-66"></a></h4>
<p>Performs element-wise count of the number of bits set in the <code>operand</code> tensor and produces a <code>result</code> tensor.</p>
</div>
<div class="section level4">
<h4 id="inputs-63">Inputs<a class="anchor" aria-label="anchor" href="#inputs-63"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-66">Outputs<a class="anchor" aria-label="anchor" href="#outputs-66"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-62">Constraints<a class="anchor" aria-label="anchor" href="#constraints-62"></a></h4>
<ul><li>(C1) <code>type(operand) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-66">Examples<a class="anchor" aria-label="anchor" href="#examples-66"></a></h4>
<pre class="mlir"><code>// %operand: [0, 1, 2, 127]
%result = "stablehlo.popcnt"(%operand) : (tensor&lt;4xi64&gt;) -&gt; tensor&lt;4xi64&gt;
// %result: [0, 1, 1, 7]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/popcnt.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="power">power<a class="anchor" aria-label="anchor" href="#power"></a></h3>
<div class="section level4">
<h4 id="semantics-67">Semantics<a class="anchor" aria-label="anchor" href="#semantics-67"></a></h4>
<p>Performs element-wise exponentiation of <code>lhs</code> tensor by <code>rhs</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For integers: integer exponentiation.</li>
<li>For floats: <code>pow</code> from IEEE-754.</li>
<li>For complex numbers: complex exponentiation.</li>
<li>For quantized types: <code>dequantize_op_quantize(power, lhs, rhs, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-64">Inputs<a class="anchor" aria-label="anchor" href="#inputs-64"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="6%"><col width="75%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-67">Outputs<a class="anchor" aria-label="anchor" href="#outputs-67"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-63">Constraints<a class="anchor" aria-label="anchor" href="#constraints-63"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-67">Examples<a class="anchor" aria-label="anchor" href="#examples-67"></a></h4>
<pre class="mlir"><code>// %lhs: [-2.0, -0.0, -36.0, 5.0, 3.0, 10000.0]
// %rhs: [2.0, 2.0, 1.1, 2.0, -1.0, 10.0]
%result = "stablehlo.power"(%lhs, %rhs) : (tensor&lt;6xf64&gt;, tensor&lt;6xf64&gt;) -&gt; tensor&lt;6xf64&gt;
// %result: [4.0, 0.0, -nan, 25.0, 0.333333343, inf]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/power.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="real">real<a class="anchor" aria-label="anchor" href="#real"></a></h3>
<div class="section level4">
<h4 id="semantics-68">Semantics<a class="anchor" aria-label="anchor" href="#semantics-68"></a></h4>
<p>Extracts the real part, element-wise, from the <code>operand</code> and produces a <code>result</code> tensor. More formally, for each element <code>x</code>: <code>real(x) = is_complex(x) ? real_part(x) : x</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-65">Inputs<a class="anchor" aria-label="anchor" href="#inputs-65"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="15%"><col width="57%"><col width="17%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-68">Outputs<a class="anchor" aria-label="anchor" href="#outputs-68"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-64">Constraints<a class="anchor" aria-label="anchor" href="#constraints-64"></a></h4>
<ul><li>(C1) <code>shape(result) = shape(operand)</code>.</li>
<li>(C2) <code>element_type(result)</code> is defined as:
<ul><li>
<code>complex_element_type(element_type(operand))</code> if <code>is_complex(operand)</code>.</li>
<li>
<code>element_type(operand)</code> otherwise.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-68">Examples<a class="anchor" aria-label="anchor" href="#examples-68"></a></h4>
<pre class="mlir"><code>// %operand: [(1.0, 2.0), (3.0, 4.0)]
%result = "stablehlo.real"(%operand) : (tensor&lt;2xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;2xf32&gt;
// %result: [1.0, 3.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/real.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="recv">recv<a class="anchor" aria-label="anchor" href="#recv"></a></h3>
<div class="section level4">
<h4 id="semantics-69">Semantics<a class="anchor" aria-label="anchor" href="#semantics-69"></a></h4>
<p>Receives data from a channel with <code>channel_id</code> and produces <code>results</code>.</p>
<p>If <code>is_host_transfer</code> is <code>true</code>, then the operation transfers data from the host. Otherwise, it transfers data from another device based on the values of <code>source_target_pairs</code>. This flag duplicates the information provided in <code>channel_type</code>, so in the future we are planning to only keep one of them (<a href="https://github.com/openxla/stablehlo/issues/666" class="external-link">#666</a>). If <code>is_host_transfer</code> = <code>false</code> and <code>source_target_pairs</code> is <code>None</code> or empty, it is considered undefined behavior.</p>
<p><code>results</code> consist of payload values which come first and a token which comes last. In the future, we are planning to split the payload and the token into two separate outputs to improve clarity (<a href="https://github.com/openxla/stablehlo/issues/670" class="external-link">#670</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-66">Inputs<a class="anchor" aria-label="anchor" href="#inputs-66"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="24%"><col width="52%"><col width="15%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>token</code></td>
<td><code>token</code></td>
<td></td>
</tr><tr class="even"><td>(I2)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td></td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>channel_type</code></td>
<td>enum of <code>DEVICE_TO_DEVICE</code> and <code>DEVICE_TO_HOST</code>
</td>
<td>(C5)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>is_host_transfer</code></td>
<td>constant of type <code>i1</code>
</td>
<td>(C5-C6)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>source_target_pairs</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1-C4), (C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-69">Outputs<a class="anchor" aria-label="anchor" href="#outputs-69"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="70%"><col width="16%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors, quantized tensors or tokens</td>
<td>(C2-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-65">Constraints<a class="anchor" aria-label="anchor" href="#constraints-65"></a></h4>
<ul><li>(C1) <code>dim(source_target_pairs, 1) = 2</code>.</li>
<li>(C2) <code>is_unique(source_target_pairs[:, 0])</code>.</li>
<li>(C3) <code>is_unique(source_target_pairs[:, 1])</code>.</li>
<li>(C4) <code>0 &lt;= source_target_pairs &lt; N</code>, where <code>N</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_partitions</code> if <code>cross_partition</code> is used.</li>
</ul></li>
<li>(C5) <code>channel_type</code> is defined as:
<ul><li>
<code>DEVICE_TO_HOST</code> if <code>is_host_transfer = true</code>,</li>
<li>
<code>DEVICE_TO_DEVICE</code> otherwise.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-69">Examples<a class="anchor" aria-label="anchor" href="#examples-69"></a></h4>
<pre class="mlir"><code>%results0, %results1 = "stablehlo.recv"(%token) {
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 1&gt;,
  is_host_transfer = false,
  source_target_pairs = dense&lt;[[0, 1], [1, 2]]&gt; : tensor&lt;2x2xi64&gt;
} : (!stablehlo.token) -&gt; (tensor&lt;2x2xi64&gt;, !stablehlo.token)</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/send_recv.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="reduce">reduce<a class="anchor" aria-label="anchor" href="#reduce"></a></h3>
<div class="section level4">
<h4 id="semantics-70">Semantics<a class="anchor" aria-label="anchor" href="#semantics-70"></a></h4>
<p>Applies a reduction function <code>body</code> to <code>inputs</code> and <code>init_values</code> along the <code>dimensions</code> and produces <code>results</code> tensors.</p>
<p>The order of reductions is implementation-defined, which means that <code>body</code> and <code>init_values</code> must form a monoid to guarantee that the operation produces the same results for all inputs on all implementations. However, this condition doesn’t hold for many popular reductions. E.g. floating-point addition for <code>body</code> and zero for <code>init_values</code> don’t actually form a monoid because floating-point addition is not associative.</p>
<p>More formally, <code>results...[j0, ..., jR-1] = reduce(input_slices_converted)</code> where:</p>
<ul><li>
<code>input_slices = inputs...[j0, ..., :, ..., jR-1]</code>, where <code>:</code> are inserted at <code>dimensions</code>.</li>
<li>
<code>input_slices_converted = to_destination_type(input_slices...,   type(func_inputs(body)[:len(func_inputs(body))//2])...)</code>.</li>
<li>
<code>init_values_converted = to_destination_type(init_values...,   type(func_inputs(body)[len(func_inputs(body))//2:])...)</code>.</li>
<li>
<code>reduce(input_slices_converted) = exec(schedule)</code> for some binary tree <code>schedule</code> where:
<ul><li>
<code>exec(node) = body(exec(node.left), exec(node.right))</code>.</li>
<li>
<code>exec(leaf) = leaf.value</code>.</li>
</ul></li>
<li>
<code>schedule</code> is an implementation-defined full binary tree whose in-order traversal consists of:
<ul><li>
<code>input_slices_converted...[index]</code> values, for all <code>index</code> in <code>index_space(input_slices_converted)</code> in the ascending lexicographic order of <code>index</code>.</li>
<li>Interspersed with an implementation-defined amount of <code>init_values_converted</code> at implementation-defined positions.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-67">Inputs<a class="anchor" aria-label="anchor" href="#inputs-67"></a></h4>
<table class="table"><colgroup><col width="5%"><col width="12%"><col width="63%"><col width="17%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1-C4), (C6), (C7)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>init_values</code></td>
<td>variadic number of 0-dimensional tensors or per-tensor quantized tensors</td>
<td>(C2), (C3)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C4), (C5), (C7)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>body</code></td>
<td>function</td>
<td>(C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-70">Outputs<a class="anchor" aria-label="anchor" href="#outputs-70"></a></h4>
<table class="table"><colgroup><col width="12%"><col width="67%"><col width="20%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C3), (C7), (C8)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-66">Constraints<a class="anchor" aria-label="anchor" href="#constraints-66"></a></h4>
<ul><li>(C1) <code>same(shape(inputs...))</code>.</li>
<li>(C2) <code>element_type(inputs...) = element_type(init_values...)</code>.</li>
<li>(C3) <code>0 &lt; size(inputs) = size(init_values) = size(results) = N</code>.</li>
<li>(C4) <code>0 &lt;= dimensions &lt; rank(inputs[0])</code>.</li>
<li>(C5) <code>is_unique(dimensions)</code>.</li>
<li>(C6) <code>body</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;E0&gt;, ...,</code> <code>tensor&lt;EN-1&gt;) -&gt; (tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;)</code> where <code>is_promotable(element_type(inputs[i]), Ei)</code>.</li>
<li>(C7) <code>shape(results...) = shape(inputs...)</code> except that the dimension sizes of <code>inputs...</code> corresponding to <code>dimensions</code> are not included.</li>
<li>(C8) <code>element_type(results[i]) = Ei</code> for all <code>i</code> in <code>[0,N)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-70">Examples<a class="anchor" aria-label="anchor" href="#examples-70"></a></h4>
<pre class="mlir"><code>// %input = [[0, 1, 2, 3, 4, 5]]
// %init_value = 0
%result = "stablehlo.reduce"(%input, %init_value) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i64&gt;
    "stablehlo.return"(%0) : (tensor&lt;i64&gt;) -&gt; ()
}) {
  dimensions = array&lt;i64: 1&gt;
} : (tensor&lt;1x6xi64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;1xi64&gt;
// %result = [15]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/reduce.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="reduce_precision">reduce_precision<a class="anchor" aria-label="anchor" href="#reduce_precision"></a></h3>
<div class="section level4">
<h4 id="semantics-71">Semantics<a class="anchor" aria-label="anchor" href="#semantics-71"></a></h4>
<p>Performs element-wise conversion of <code>operand</code> to another floating-point type that uses <code>exponent_bits</code> and <code>mantissa_bits</code> and back to the original floating-point type and produces an <code>output</code> tensor.</p>
<p>More formally:</p>
<ul><li>The mantissa bits of the original value are updated to round the original value to the nearest value representable with <code>mantissa_bits</code> using <code>roundToIntegralTiesToEven</code> semantics.</li>
<li>Then, if <code>mantissa_bits</code> are smaller than the number of mantissa bits of the original value, the mantissa bits are truncated to <code>mantissa_bits</code>.</li>
<li>Then, if the exponent bits of the intermediate result don’t fit into the range provided by <code>exponent_bits</code>, the intermediate result overflows to infinity using the original sign or underflows to zero using the original sign.</li>
<li>For quantized types, performs <code>dequantize_op_quantize(     lambda operand: reduce_precision(operand, exponent_bits, mantissa_bits),     operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-68">Inputs<a class="anchor" aria-label="anchor" href="#inputs-68"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="17%"><col width="62%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>exponent_bits</code></td>
<td>constant of type <code>si32</code>
</td>
<td>(C2)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>mantissa_bits</code></td>
<td>constant of type <code>si32</code>
</td>
<td>(C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-71">Outputs<a class="anchor" aria-label="anchor" href="#outputs-71"></a></h4>
<table class="table"><colgroup><col width="11%"><col width="72%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>output</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-67">Constraints<a class="anchor" aria-label="anchor" href="#constraints-67"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(output)</code>.</li>
<li>(C2) <code>1 &lt;= exponent_bits</code>.</li>
<li>(C3) <code>0 &lt;= mantissa_bits</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-71">Examples<a class="anchor" aria-label="anchor" href="#examples-71"></a></h4>
<pre class="mlir"><code>// Logical values: +Inf, NaN, +Denormal, 0.0, 65519.0, 65520.0
// %operand: [0x7FF0000000000000, 0x7FFFFFFFFFFFFFFF, 0x0000000000000001, 0.0, 65519.0, 65520.0]
%output = "stablehlo.reduce_precision"(%operand) {
  exponent_bits = 5 : i32,
  mantissa_bits = 10 : i32
} : (tensor&lt;6xf64&gt;) -&gt; tensor&lt;6xf64&gt;
// Logical values: +Inf, NaN, 0.0, 0.0, 65504.0, +Inf
// %output: [0x7FF0000000000000, 0x7FFFFFFFFFFFFFFF, 0.0, 0.0, 65504.0, 0x7FF0000000000000]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/reduce_precision.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="reduce_scatter">reduce_scatter<a class="anchor" aria-label="anchor" href="#reduce_scatter"></a></h3>
<div class="section level4">
<h4 id="semantics-72">Semantics<a class="anchor" aria-label="anchor" href="#semantics-72"></a></h4>
<div class="float">
<img src="images/spec/reduce_scatter.svg" alt="reduce_scatter"><div class="figcaption">reduce_scatter</div>
</div>
<p>Within each process group in the StableHLO process grid, performs reduction, using <code>computations</code>, over the values of the <code>operand</code> tensor from each process, splits the reduction result along <code>scatter_dimension</code> into parts, and scatters the split parts between the processes to produce the <code>result</code>.</p>
<p>The operation splits the StableHLO process grid into <code>process_groups</code> which is defined as follows:</p>
<ul><li>
<code>cross_replica(replica_groups)</code> if <code>channel_id &lt;= 0 and use_global_device_ids = false</code>.</li>
<li>
<code>cross_replica_and_partition(replica_groups)</code> if <code>channel_id &gt; 0 and use_global_device_ids = false</code>.</li>
<li>
<code>flattened_ids(replica_groups)</code> if <code>channel_id &gt; 0 and use_global_device_ids = true</code>.</li>
</ul><p>Afterwards, within each <code>process_group</code>:</p>
<ul><li>
<code>reduced_value = all_reduce(operand, replica_groups, channel_id,   use_global_device_ids, computation)</code>.</li>
<li>
<code>parts@sender = split(reduced_value@sender, dim(process_groups, 1),   scatter_dimension)</code>.</li>
<li>
<code>result@receiver = parts@sender[receiver_index]</code> for all <code>sender</code> in <code>process_group</code>, where <code>receiver_index = process_group.index(receiver)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-69">Inputs<a class="anchor" aria-label="anchor" href="#inputs-69"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="24%"><col width="45%"><col width="23%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C2), (C7), (C8)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>scatter_dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C1), (C2), (C8)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>replica_groups</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C3-C5)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C6)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>use_global_device_ids</code></td>
<td>constant of type <code>i1</code>
</td>
<td>(C6)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>computation</code></td>
<td>function</td>
<td>(C7)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-72">Outputs<a class="anchor" aria-label="anchor" href="#outputs-72"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C8-C9)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-68">Constraints<a class="anchor" aria-label="anchor" href="#constraints-68"></a></h4>
<ul><li>(C1) <code>dim(operand, scatter_dimension) % dim(process_groups, 1) = 0</code>.</li>
<li>(C2) <code>0 &lt;= scatter_dimension &lt; rank(operand)</code>.</li>
<li>(C3) <code>is_unique(replica_groups)</code>.</li>
<li>(C4) <code>size(replica_groups)</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_replicas</code> if <code>cross_replica_and_partition</code> is used.</li>
<li>
<code>num_processes</code> if <code>flattened_ids</code> is used.</li>
</ul></li>
<li>(C5) <code>0 &lt;= replica_groups &lt; size(replica_groups)</code>.</li>
<li>(C6) If <code>use_global_device_ids = true</code>, then <code>channel_id &gt; 0</code>.</li>
<li>(C7) <code>computation</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; (tensor&lt;E&gt;)</code> where <code>is_promotable(element_type(operand), E)</code>.</li>
<li>(C8) <code>shape(result) = shape(operand)</code> except:
<ul><li>
<code>dim(result, scatter_dimension) = dim(operand, scatter_dimension) /   dim(process_groups, 1)</code>.</li>
</ul></li>
<li>(C9) <code>element_type(result) = E</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-72">Examples<a class="anchor" aria-label="anchor" href="#examples-72"></a></h4>
<pre class="mlir"><code>// num_replicas: 2
// num_partitions: 1
// %operand@(0, 0): [[1, 2, 3, 4],
//                   [5, 6, 7, 8]]
// %operand@(1, 0): [[9, 10, 11, 12],
//                   [13, 14, 15, 16]]
%result = "stablehlo.reduce_scatter"(%operand) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
  %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i64&gt;
  "stablehlo.return"(%0) : (tensor&lt;i64&gt;) -&gt; ()
}) {
  scatter_dimension = 1 : i64,
  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;,
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;
} : (tensor&lt;2x4xi64&gt;) -&gt; tensor&lt;2x2xi64&gt;
//
// %result@(0, 0): [[10, 12],
//                  [18, 20]]
// %result@(1, 0): [[14, 16],
//                  [22, 24]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/reduce_scatter.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="reduce_window">reduce_window<a class="anchor" aria-label="anchor" href="#reduce_window"></a></h3>
<div class="section level4">
<h4 id="semantics-73">Semantics<a class="anchor" aria-label="anchor" href="#semantics-73"></a></h4>
<p>Applies a reduction function <code>body</code> to windows of <code>inputs</code> and <code>init_values</code> and produces <code>results</code>.</p>
<p>The following diagram shows how elements in <code>results...</code> are computed from <code>inputs...</code> using a concrete example.</p>
<div class="float">
<img src="images/spec/reduce_window.svg" alt="reduce_window"><div class="figcaption">reduce_window</div>
</div>
<p>More formally, <code>results...[result_index] = reduce(windows, init_values, axes(inputs...), body)</code> (see <a href="#reduce">reduce</a>) where:</p>
<ul><li>
<code>padded_inputs = pad(inputs..., init_values..., padding[:, 0], padding[:, 1],   base_dilations - 1)</code>.</li>
<li>
<code>window_start = result_index * window_strides</code>.</li>
<li>
<code>window_end = window_start + (window_dimensions - 1) * window_dilations + 1</code>.</li>
<li>
<code>windows = slice(padded_inputs..., window_start, window_end,   window_dilations)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-70">Inputs<a class="anchor" aria-label="anchor" href="#inputs-70"></a></h4>
<table class="table"><colgroup><col width="4%"><col width="13%"><col width="49%"><col width="32%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1-C4), (C6), (C8), (C10), (C12), (C13), (C15)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>init_values</code></td>
<td>variadic number of 0-dimensional tensors or per-tensor quantized tensors</td>
<td>(C1), (C13)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>window_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C4), (C5), (C15)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>window_strides</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C6), (C7), (C15)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>base_dilations</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C8), (C9), (C15)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>window_dilations</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C10), (C11), (C15)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>padding</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C12), (C15)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>body</code></td>
<td>function</td>
<td>(C13)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-73">Outputs<a class="anchor" aria-label="anchor" href="#outputs-73"></a></h4>
<table class="table"><colgroup><col width="12%"><col width="68%"><col width="19%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1), (C14-C16)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-69">Constraints<a class="anchor" aria-label="anchor" href="#constraints-69"></a></h4>
<!-- markdownlint-disable line-length -->
<ul><li>(C1) <code>0 &lt; size(inputs) = size(init_values) = size(results) = N</code>.</li>
<li>(C2) <code>same(shape(inputs...))</code>.</li>
<li>(C3) <code>element_type(inputs...) = element_type(init_values...)</code>.</li>
<li>(C4) <code>size(window_dimensions) = rank(inputs[0])</code>.</li>
<li>(C5) <code>0 &lt; window_dimensions</code>.</li>
<li>(C6) <code>size(window_strides) = rank(inputs[0])</code>.</li>
<li>(C7) <code>0 &lt; window_strides</code>.</li>
<li>(C8) <code>size(base_dilations) = rank(inputs[0])</code>.</li>
<li>(C9) <code>0 &lt; base_dilations</code>.</li>
<li>(C10) <code>size(window_dilations) = rank(inputs[0])</code>.</li>
<li>(C11) <code>0 &lt; window_dilations</code>.</li>
<li>(C12) <code>shape(padding) = [rank(inputs[0]), 2]</code>.</li>
<li>(C13) <code>body</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;E0&gt;, ...,</code> <code>tensor&lt;EN-1&gt;) -&gt; (tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;)</code> where <code>is_promotable(element_type(inputs[i]), Ei)</code>.</li>
<li>(C14) <code>same(shape(results...))</code>.</li>
<li>(C15) <code>shape(results[0]) = num_windows</code> where:
<ul><li>
<code>dilated_input_shape = shape(inputs[0]) = 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1</code>.</li>
<li>
<code>padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1]</code>.</li>
<li>
<code>dilated_window_shape = (window_dimensions - 1) * window_dilations + 1</code>.</li>
<li>
<code>is_empty_window = padded_input_shape = 0 || dilated_window_shape &gt; padded_input_shape</code>.</li>
<li>
<code>num_windows = is_empty_window ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1</code>.</li>
</ul></li>
<li>(C16) <code>element_type(results[i]) = Ei</code> for all <code>i</code> in <code>[0,N)</code>. <!-- markdownlint-enable line-length -->
</li>
</ul></div>
<div class="section level4">
<h4 id="examples-73">Examples<a class="anchor" aria-label="anchor" href="#examples-73"></a></h4>
<pre class="mlir"><code>// %input = [[1, 2], [3, 4], [5, 6]]
// %init_value = 0
%result = "stablehlo.reduce_window"(%input, %init_value) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i64&gt;
    "stablehlo.return"(%0) : (tensor&lt;i64&gt;) -&gt; ()
}) {
  window_dimensions = array&lt;i64: 2, 1&gt;,
  window_strides = array&lt;i64: 4, 1&gt;,
  base_dilations = array&lt;i64: 2, 1&gt;,
  window_dilations = array&lt;i64: 3, 1&gt;,
  padding = dense&lt;[[2, 1], [0, 0]]&gt; : tensor&lt;2x2xi64&gt;
} : (tensor&lt;3x2xi64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;2x2xi64&gt;
// %result = [[0, 0], [3, 4]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/reduce_window.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="remainder">remainder<a class="anchor" aria-label="anchor" href="#remainder"></a></h3>
<div class="section level4">
<h4 id="semantics-74">Semantics<a class="anchor" aria-label="anchor" href="#semantics-74"></a></h4>
<p>Performs element-wise remainder of dividend <code>lhs</code> and divisor <code>rhs</code> tensors and produces a <code>result</code> tensor.</p>
<p>More formally, the sign of the result is taken from the dividend, and the absolute value of the result is always less than the divisor’s absolute value. The remainder is calculated as <code>lhs - d * rhs</code>, where <code>d</code> is given by:</p>
<ul><li>For integers: <code>stablehlo.divide(lhs, rhs)</code>.</li>
<li>For floats: <code>division(lhs, rhs)</code> from IEEE-754 with rounding attribute <code>roundTowardZero</code>.</li>
<li>For complex numbers: TBD (<a href="https://github.com/openxla/stablehlo/issues/997" class="external-link">#997</a>).</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(remainder, lhs, rhs, type(result))</code>.</li>
</ul></li>
</ul><p>For floating-point element types, this operation is in contrast with the <code>remainder</code> operation from IEEE-754 specification where <code>d</code> is an integral value nearest to the exact value of <code>lhs/rhs</code> with ties to even.</p>
</div>
<div class="section level4">
<h4 id="inputs-71">Inputs<a class="anchor" aria-label="anchor" href="#inputs-71"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="6%"><col width="75%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer, floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer, floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-74">Outputs<a class="anchor" aria-label="anchor" href="#outputs-74"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-70">Constraints<a class="anchor" aria-label="anchor" href="#constraints-70"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-74">Examples<a class="anchor" aria-label="anchor" href="#examples-74"></a></h4>
<pre class="mlir"><code>// %lhs: [17, -17, 17, -17]
// %rhs: [3, 3, -3, -3]
%result = "stablehlo.remainder"(%lhs, %rhs) : (tensor&lt;4xi64&gt;, tensor&lt;4xi64&gt;) -&gt; tensor&lt;4xi64&gt;
// %result: [2, -2, 2, -2]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/remainder.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="replica_id">replica_id<a class="anchor" aria-label="anchor" href="#replica_id"></a></h3>
<div class="section level4">
<h4 id="semantics-75">Semantics<a class="anchor" aria-label="anchor" href="#semantics-75"></a></h4>
<p>Produces <code>replica_id</code> of the current process.</p>
</div>
<div class="section level4">
<h4 id="outputs-75">Outputs<a class="anchor" aria-label="anchor" href="#outputs-75"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>0-dimensional tensor of type <code>ui32</code>
</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="examples-75">Examples<a class="anchor" aria-label="anchor" href="#examples-75"></a></h4>
<pre class="mlir"><code>%result = "stablehlo.replica_id"() : () -&gt; tensor&lt;ui32&gt;</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/replica_id.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="reshape">reshape<a class="anchor" aria-label="anchor" href="#reshape"></a></h3>
<div class="section level4">
<h4 id="semantics-76">Semantics<a class="anchor" aria-label="anchor" href="#semantics-76"></a></h4>
<p>Performs reshape of <code>operand</code> tensor to a <code>result</code> tensor. Conceptually, it amounts to keeping the same canonical representation but potentially changing the shape, e.g. from <code>tensor&lt;2x3xf32&gt;</code> to <code>tensor&lt;3x2xf32&gt;</code> or <code>tensor&lt;6xf32&gt;</code>.</p>
<p>More formally, <code>result[result_index] = operand[operand_index]</code> where <code>result_index</code> and <code>operand_index</code> have the same position in the lexicographic ordering of <code>index_space(result)</code> and <code>index_space(operand)</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-72">Inputs<a class="anchor" aria-label="anchor" href="#inputs-72"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-76">Outputs<a class="anchor" aria-label="anchor" href="#outputs-76"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-71">Constraints<a class="anchor" aria-label="anchor" href="#constraints-71"></a></h4>
<ul><li>(C1) <code>element_type(result)</code> is given by:
<ul><li>
<code>element_type(operand)</code>, if <code>!is_per_axis_quantized(operand)</code>.</li>
<li>
<code>element_type(operand)</code> except that <code>quantization_dimension(operand)</code> and <code>quantization_dimension(result)</code> may differ, otherwise.</li>
</ul></li>
<li>(C2) <code>size(operand) = size(result)</code>.</li>
<li>(C3) If <code>is_per_axis_quantized(operand)</code>:
<ul><li>
<code>reduce(dims(operand, [0, 1, ..., quantization_dimension(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result, [0, 1, ..., quantization_dimension(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y)</code>.</li>
<li>
<code>dim(operand, quantization_dimension(operand)) =   dim(result, quantization_dimension(result))</code>.</li>
<li>
<code>reduce(dims(operand,   [quantization_dimension(operand) + 1, ..., rank(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result,   [quantization_dimension(result) + 1, ..., rank(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y)</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-76">Examples<a class="anchor" aria-label="anchor" href="#examples-76"></a></h4>
<pre class="mlir"><code>// %operand: [[1, 2, 3], [4, 5, 6]]
%result = "stablehlo.reshape"(%operand) : (tensor&lt;2x3xi32&gt;) -&gt; tensor&lt;3x2xi32&gt;
// %result: [[1, 2], [3, 4], [5, 6]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/reshape.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="reverse">reverse<a class="anchor" aria-label="anchor" href="#reverse"></a></h3>
<div class="section level4">
<h4 id="semantics-77">Semantics<a class="anchor" aria-label="anchor" href="#semantics-77"></a></h4>
<p>Reverses the order of elements in the <code>operand</code> along the specified <code>dimensions</code> and produces a <code>result</code> tensor. More formally, <code>result[result_index] = operand[operand_index]</code> where:</p>
<ul><li>
<code>operand_index[d] = dim(result, d) - result_index[d] - 1</code> if <code>d</code> in <code>dimensions</code>.</li>
<li>
<code>operand_index[d] = result_index[d]</code> otherwise.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-73">Inputs<a class="anchor" aria-label="anchor" href="#inputs-73"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="8%"><col width="17%"><col width="57%"><col width="16%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-77">Outputs<a class="anchor" aria-label="anchor" href="#outputs-77"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-72">Constraints<a class="anchor" aria-label="anchor" href="#constraints-72"></a></h4>
<ul><li>(C1) <code>type(operand) = type(result)</code>.</li>
<li>(C2) <code>is_unique(dimensions)</code>.</li>
<li>(C3) <code>0 &lt;= dimensions &lt; rank(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-77">Examples<a class="anchor" aria-label="anchor" href="#examples-77"></a></h4>
<pre class="mlir"><code>// %operand = [[1, 2], [3, 4], [5, 6]]
%result = "stablehlo.reverse"(%operand) {
  dimensions = array&lt;i64: 1&gt;
} : (tensor&lt;3x2xi32&gt;) -&gt; tensor&lt;3x2xi32&gt;
// %result: [[2, 1], [4, 3], [6, 5]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/reverse.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="rng">rng<a class="anchor" aria-label="anchor" href="#rng"></a></h3>
<blockquote>
<p>Note: Per <a href="https://github.com/openxla/stablehlo/pull/2283" class="external-link">StableHLO v1.0 Cleanup #2283</a>, this op is being explored for deprecation as it appears to be unused by both frameworks and compilers. As such, it has limited compatibility guarantees (6 months).</p>
</blockquote>
<div class="section level4">
<h4 id="semantics-78">Semantics<a class="anchor" aria-label="anchor" href="#semantics-78"></a></h4>
<p>Generates random numbers using the <code>rng_distribution</code> algorithm and produces a <code>result</code> tensor of a given shape <code>shape</code>.</p>
<p>If <code>rng_distribution = UNIFORM</code>, then the random numbers are generated following the uniform distribution over the interval <code>[a, b)</code>. If <code>a &gt;= b</code>, the behavior is undefined.</p>
<p>If <code>rng_distribution = NORMAL</code>, then the random numbers are generated following the normal distribution with mean = <code>a</code> and standard deviation = <code>b</code>. If <code>b &lt; 0</code>, the behavior is undefined.</p>
<p>The exact way how random numbers are generated is implementation-defined. For example, they may or may not be deterministic, and they may or may not use hidden state.</p>
<p>In conversations with many stakeholders, this op has come up as effectively deprecated, so in the future we are planning to explore removing it (<a href="https://github.com/openxla/stablehlo/issues/597" class="external-link">#597</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-74">Inputs<a class="anchor" aria-label="anchor" href="#inputs-74"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="6%"><col width="18%"><col width="62%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>a</code></td>
<td>0-dimensional tensor of integer, boolean, or floating-point type</td>
<td>(C1), (C2)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>b</code></td>
<td>0-dimensional tensor of integer, boolean, or floating-point type</td>
<td>(C1), (C2)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>shape</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C3)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>rng_distribution</code></td>
<td>enum of <code>UNIFORM</code> and <code>NORMAL</code>
</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-78">Outputs<a class="anchor" aria-label="anchor" href="#outputs-78"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="69%"><col width="17%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, boolean, or floating-point type</td>
<td>(C1-C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-73">Constraints<a class="anchor" aria-label="anchor" href="#constraints-73"></a></h4>
<ul><li>(C1) <code>element_type(a) = element_type(b) = element_type(result)</code>.</li>
<li>(C2) If <code>rng_distribution = NORMAL</code>, then <code>is_float(a)</code>.</li>
<li>(C3) <code>shape(result) = shape</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-78">Examples<a class="anchor" aria-label="anchor" href="#examples-78"></a></h4>
<pre class="mlir"><code>// %a = 0
// %b = 2
// %shape = [3, 3]
%result = "stablehlo.rng"(%a, %b, %shape) {
  rng_distribution = #stablehlo&lt;rng_distribution UNIFORM&gt;
} : (tensor&lt;i32&gt;, tensor&lt;i32&gt;, tensor&lt;2xi64&gt;) -&gt; tensor&lt;3x3xi32&gt;
// %result: [
//           [1, 0, 1],
//           [1, 1, 1],
//           [0, 0, 0]
//          ]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="rng_bit_generator">rng_bit_generator<a class="anchor" aria-label="anchor" href="#rng_bit_generator"></a></h3>
<div class="section level4">
<h4 id="semantics-79">Semantics<a class="anchor" aria-label="anchor" href="#semantics-79"></a></h4>
<p>Returns an <code>output</code> filled with uniform random bits and an updated output state <code>output_state</code> using the pseudorandom number generator algorithm <code>rng_algorithm</code> given an initial state <code>initial_state</code>. The output is guaranteed to be deterministic function of <code>initial_state</code>, but it is not guaranteed to be deterministic between implementations.</p>
<p><code>rng_algorithm</code> is one of the following:</p>
<ul><li>
<code>DEFAULT</code>: Implementation-defined algorithm.</li>
<li>
<code>THREE_FRY</code>: Implementation-defined variant of the Threefry algorithm.*</li>
<li>
<code>PHILOX</code>: Implementation-defined variant of the Philox algorithm.*</li>
</ul><p>* See: <a href="http://www.thesalmons.org/john/random123/papers/random123sc11.pdf" class="external-link">Salmon et al. SC 2011. Parallel random numbers: as easy as 1, 2, 3.</a></p>
</div>
<div class="section level4">
<h4 id="inputs-75">Inputs<a class="anchor" aria-label="anchor" href="#inputs-75"></a></h4>
<table class="table"><colgroup><col width="8%"><col width="20%"><col width="55%"><col width="15%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>rng_algorithm</code></td>
<td>enum of <code>DEFAULT</code>, <code>THREE_FRY</code>, and <code>PHILOX</code>
</td>
<td>(C2)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>initial_state</code></td>
<td>1-dimensional tensor of type <code>ui64</code>
</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-79">Outputs<a class="anchor" aria-label="anchor" href="#outputs-79"></a></h4>
<table class="table"><colgroup><col width="22%"><col width="59%"><col width="18%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>output_state</code></td>
<td>1-dimensional tensor of type <code>ui64</code>
</td>
<td>(C1)</td>
</tr><tr class="even"><td><code>output</code></td>
<td>tensor of integer or floating-point type</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-74">Constraints<a class="anchor" aria-label="anchor" href="#constraints-74"></a></h4>
<ul><li>(C1) <code>type(initial_state) = type(output_state)</code>.</li>
<li>(C2) <code>size(initial_state)</code> is defined as:
<ul><li>implementation-defined if <code>rng_algorithm = DEFAULT</code>.</li>
<li>
<code>2</code> if <code>rng_algorithm = THREE_FRY</code>.</li>
<li>
<code>2</code> or <code>3</code> if <code>rng_algorithm = PHILOX</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-79">Examples<a class="anchor" aria-label="anchor" href="#examples-79"></a></h4>
<pre class="mlir"><code>// %initial_state: [1, 2]
%output_state, %output = "stablehlo.rng_bit_generator"(%initial_state) {
  rng_algorithm = #stablehlo&lt;rng_algorithm THREE_FRY&gt;
} : (tensor&lt;2xui64&gt;) -&gt; (tensor&lt;2xui64&gt;, tensor&lt;2x2xui64&gt;)
// %output_state: [1, 6]
// %output: [
//           [9236835810183407956, 16087790271692313299],
//           [18212823393184779219, 2658481902456610144]
//          ]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="round_nearest_afz">round_nearest_afz<a class="anchor" aria-label="anchor" href="#round_nearest_afz"></a></h3>
<div class="section level4">
<h4 id="semantics-80">Semantics<a class="anchor" aria-label="anchor" href="#semantics-80"></a></h4>
<p>Performs element-wise rounding towards the nearest integer, breaking ties away from zero, on the <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTiesToAway</code> operation from the IEEE-754 specification. For quantized types, performs <code>dequantize_op_quantize(round_nearest_afz, operand, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-76">Inputs<a class="anchor" aria-label="anchor" href="#inputs-76"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="7%"><col width="11%"><col width="66%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-80">Outputs<a class="anchor" aria-label="anchor" href="#outputs-80"></a></h4>
<table class="table"><colgroup><col width="11%"><col width="72%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-75">Constraints<a class="anchor" aria-label="anchor" href="#constraints-75"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-80">Examples<a class="anchor" aria-label="anchor" href="#examples-80"></a></h4>
<pre class="mlir"><code>// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5]
%result = "stablehlo.round_nearest_afz"(%operand) : (tensor&lt;5xf64&gt;) -&gt; tensor&lt;5xf64&gt;
// %result: [-3.0, 0.0, 1.0, 1.0, 3.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/round_nearest_afz.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="round_nearest_even">round_nearest_even<a class="anchor" aria-label="anchor" href="#round_nearest_even"></a></h3>
<div class="section level4">
<h4 id="semantics-81">Semantics<a class="anchor" aria-label="anchor" href="#semantics-81"></a></h4>
<p>Performs element-wise rounding towards the nearest integer, breaking ties towards the even integer, on the <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTiesToEven</code> operation from the IEEE-754 specification. For quantized types, performs <code>dequantize_op_quantize(round_nearest_even, operand, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-77">Inputs<a class="anchor" aria-label="anchor" href="#inputs-77"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="7%"><col width="11%"><col width="66%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-81">Outputs<a class="anchor" aria-label="anchor" href="#outputs-81"></a></h4>
<table class="table"><colgroup><col width="11%"><col width="72%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-76">Constraints<a class="anchor" aria-label="anchor" href="#constraints-76"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-81">Examples<a class="anchor" aria-label="anchor" href="#examples-81"></a></h4>
<pre class="mlir"><code>// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5]
%result = "stablehlo.round_nearest_even"(%operand) : (tensor&lt;5xf64&gt;) -&gt; tensor&lt;5xf64&gt;
// %result: [-2.0, 0.0, 0.0, 1.0, 2.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/round_nearest_even.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="rsqrt">rsqrt<a class="anchor" aria-label="anchor" href="#rsqrt"></a></h3>
<div class="section level4">
<h4 id="semantics-82">Semantics<a class="anchor" aria-label="anchor" href="#semantics-82"></a></h4>
<p>Performs element-wise reciprocal square root operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>rSqrt</code> from IEEE-754.</li>
<li>For complex numbers: complex reciprocal square root.</li>
<li>For quantized types: <code>dequantize_op_quantize(rsqrt, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-78">Inputs<a class="anchor" aria-label="anchor" href="#inputs-78"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-82">Outputs<a class="anchor" aria-label="anchor" href="#outputs-82"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-77">Constraints<a class="anchor" aria-label="anchor" href="#constraints-77"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-82">Examples<a class="anchor" aria-label="anchor" href="#examples-82"></a></h4>
<pre class="mlir"><code>// %operand: [[1.0, 4.0], [9.0, 25.0]]
%result = "stablehlo.rsqrt"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;
// %result: [[1.0, 0.5], [0.33333343, 0.2]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/rsqrt.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="scatter">scatter<a class="anchor" aria-label="anchor" href="#scatter"></a></h3>
<div class="section level4">
<h4 id="semantics-83">Semantics<a class="anchor" aria-label="anchor" href="#semantics-83"></a></h4>
<p>Produces <code>results</code> tensors which are equal to <code>inputs</code> tensors except that several slices specified by <code>scatter_indices</code> are updated with the values <code>updates</code> using <code>update_computation</code>.</p>
<p>The following diagram shows how elements in <code>updates...</code> map on elements in <code>results...</code> using a concrete example. The diagram picks a few example <code>updates...</code> indices and explains in detail which <code>results...</code> indices they correspond to.</p>
<div class="float">
<img src="images/spec/scatter.svg" alt="scatter"><div class="figcaption">scatter</div>
</div>
<p>More formally, for all <code>update_index</code> in <code>index_space(updates[0])</code>:</p>
<ul><li>
<code>update_scatter_dims = [d for d in axes(updates[0]) and d not in   update_window_dims]</code>.</li>
<li>
<code>update_scatter_index = update_index[update_scatter_dims...]</code>.</li>
<li>
<code>start_index</code> is defined as:
<ul><li>
<code>scatter_indices[si0, ..., :, ..., siN]</code> where <code>si</code> are individual elements in <code>update_scatter_index</code> and <code>:</code> is inserted at the <code>index_vector_dim</code> index, if <code>index_vector_dim</code> &lt; <code>rank(scatter_indices)</code>.</li>
<li>
<code>[scatter_indices[update_scatter_index]]</code> otherwise.</li>
</ul></li>
<li>For <code>d_input</code> in <code>axes(inputs[0])</code>,
<ul><li>
<code>full_start_index[d_input] = start_index[d_start]</code> if <code>d_input = scatter_dims_to_operand_dims[d_start]</code>.</li>
<li>
<code>full_start_index[d_input] = 0</code> otherwise.</li>
</ul></li>
<li>For <code>d_input</code> in <code>axes(inputs[0])</code>,
<ul><li>
<code>full_batching_index[d_input] =   update_scatter_index[d_start - (d_start &lt; index_vector_dim ? 0 : 1)]</code> if <code>d_input = input_batching_dims[i_batching]</code> and <code>d_start = scatter_indices_batching_dims[i_batching]</code>.</li>
<li>
<code>full_batching_index[d_input] = 0</code> otherwise.</li>
</ul></li>
<li>
<code>update_window_index = update_index[update_window_dims...]</code>.</li>
<li>
<code>full_window_index = [wi0, ..., 0, ..., wiN]</code> where <code>wi</code> are individual elements in <code>update_window_index</code>, and <code>0</code> is inserted at indices from <code>inserted_window_dims</code> and <code>input_batching_dims</code>.</li>
<li>
<code>result_index = full_start_index + full_batching_index + full_window_index</code>.</li>
</ul><p>Given that, <code>results = exec(schedule, inputs)</code>, where:</p>
<ul><li>
<code>schedule</code> is an implementation-defined permutation of <code>index_space(updates[0])</code>.</li>
<li>
<code>exec([update_index, ...], results) = exec([...], updated_results)</code> where:
<ul><li>If <code>result_index</code> is in bounds for <code>shape(results...)</code>
<ul><li><code>updates_converted = to_destination_type(   updates...[update_index], type(func_inputs(update_computation)   [len(func_inputs(update_computation))//2:])... )</code></li>
<li><code>updated_values = update_computation(results...[result_index],   updates_converted)</code></li>
<li>
<code>updated_results</code> is a copy of <code>results</code> with <code>results...[result_index]</code> set to <code>updated_values...</code>.</li>
</ul></li>
<li>Otherwise
<ul><li>
<code>updated_results = results</code>.</li>
</ul></li>
</ul></li>
<li>
<code>exec([], results) = results</code>.</li>
</ul><p>If <code>indices_are_sorted</code> is <code>true</code> then the implementation can assume that <code>scatter_indices</code> are sorted with respect to <code>scatter_dims_to_operand_dims</code>, otherwise the behavior is undefined. More formally, for all <code>i1 &lt; i2</code> from <code>indices(result)</code>, <code>full_start_index(i1)</code> &lt;= <code>full_start_index(i2)</code>.</p>
<p>If <code>unique_indices</code> is <code>true</code> then the implementation can assume that all <code>result_index</code> indices being scattered to are unique. If <code>unique_indices</code> is <code>true</code> but the indices being scattered to are not unique then the behavior is undefined.</p>
</div>
<div class="section level4">
<h4 id="inputs-79">Inputs<a class="anchor" aria-label="anchor" href="#inputs-79"></a></h4>
<table class="table"><colgroup><col width="4%"><col width="23%"><col width="36%"><col width="36%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1), (C2), (C4-C6), (C11), (C13), (C18), (C21), (C23-C24)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>scatter_indices</code></td>
<td>tensor of integer type</td>
<td>(C4), (C15), (C19), (C22)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>updates</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C3-C6), (C8)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>update_window_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C7-C8)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>inserted_window_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C9-C11)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>input_batching_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C9), (C12-13), (C17-18), (C20)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>scatter_indices_batching_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C14-C18)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>scatter_dims_to_operand_dims</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C19-C21)</td>
</tr><tr class="odd"><td>(I9)</td>
<td><code>index_vector_dim</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C4), (C16), (C19), (C22)</td>
</tr><tr class="even"><td>(I10)</td>
<td><code>indices_are_sorted</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr><tr class="odd"><td>(I11)</td>
<td><code>unique_indices</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr><tr class="even"><td>(I12)</td>
<td><code>update_computation</code></td>
<td>function</td>
<td>(C23)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-83">Outputs<a class="anchor" aria-label="anchor" href="#outputs-83"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="71%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C24-C25)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-78">Constraints<a class="anchor" aria-label="anchor" href="#constraints-78"></a></h4>
<!-- markdownlint-disable line-length -->
<ul><li>(C1) <code>same(shape(inputs...))</code>.</li>
<li>(C2) <code>rank(inputs[0]) = size(update_window_dims) + size(inserted_window_dims) + size(input_batching_dims)</code>.</li>
<li>(C3) <code>same(shape(updates...))</code>.</li>
<li>(C4) <code>shape(updates[0]) = combine(update_scatter_dim_sizes,        update_window_dim_sizes)</code> where:
<ul><li>
<code>update_scatter_dim_sizes = shape(scatter_indices)</code> except that the dimension size of <code>scatter_indices</code> corresponding to <code>index_vector_dim</code> is not included.</li>
<li>
<code>update_window_dim_sizes &lt;= shape(inputs[0])</code> except that the dimension sizes in <code>inputs[0]</code> corresponding to <code>inserted_window_dims</code> and <code>input_batching_dims</code> are not included.</li>
<li>
<code>combine</code> puts <code>update_scatter_dim_sizes</code> at axes corresponding to <code>update_scatter_dims</code> and <code>update_window_dim_sizes</code> at axes corresponding to <code>update_window_dims</code>.</li>
</ul></li>
<li>(C5) <code>0 &lt; size(inputs) = size(updates) = N</code>.</li>
<li>(C6) <code>element_type(updates...) = element_type(inputs...)</code>.</li>
<li>(C7) <code>is_unique(update_window_dims) and is_sorted(update_window_dims)</code>.</li>
<li>(C8) <code>0 &lt;= update_window_dims &lt; rank(updates[0])</code>.</li>
<li>(C9) <code>is_unique(concatenate(inserted_window_dims, input_batching_dims))</code>
</li>
<li>(C10) <code>is_sorted(inserted_window_dims)</code>.</li>
<li>(C11) <code>0 &lt;= inserted_window_dims &lt; rank(inputs[0])</code>.</li>
<li>(C12) <code>is_sorted(input_batching_dims)</code>.</li>
<li>(C13) <code>0 &lt;= input_batching_dims &lt; rank(inputs[0]))</code>.</li>
<li>(C14) <code>is_unique(scatter_indices_batching_dims)</code>.</li>
<li>(C15) <code>0 &lt;= scatter_indices_batching_dims &lt; rank(scatter_indices)</code>.</li>
<li>(C16) <code>index_vector_dim not in scatter_indices_batching_dims</code>.</li>
<li>(C17) <code>size(input_batching_dims) == size(scatter_indices_batching_dims)</code>.</li>
<li>(C18) <code>dim(inputs[0], input_batching_dims...) =         dim(scatter_indices, scatter_indices_batching_dims...)</code>.</li>
<li>(C19) <code>size(scatter_dims_to_operand_dims) =         index_vector_dim &lt; rank(scatter_indices) ?         dim(scatter_indices, index_vector_dim) : 1</code>.</li>
<li>(C20) <code>is_unique(concatenate(scatter_dims_to_operand_dims,         input_batching_dims))</code>.</li>
<li>(C21) <code>0 &lt;= scatter_dims_to_operand_dims &lt; rank(inputs[0])</code>.</li>
<li>(C22) <code>0 &lt;= index_vector_dim &lt;= rank(scatter_indices)</code>.</li>
<li>(C23) <code>update_computation</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;,   tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;) -&gt; (tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;)</code>, where <code>is_promotable(element_type(inputs[i]), Ei)</code>.</li>
<li>(C24) <code>shape(inputs...) = shape(results...)</code>.</li>
<li>(C25) <code>element_type(results[i]) = Ei</code> for all <code>i</code> in <code>[0,N)</code>. <!-- markdownlint-enable line-length -->
</li>
</ul></div>
<div class="section level4">
<h4 id="examples-83">Examples<a class="anchor" aria-label="anchor" href="#examples-83"></a></h4>
<pre class="mlir"><code>// %input: [
//          [
//           [[1, 2], [3, 4], [5, 6], [7, 8]],
//           [[9, 10],[11, 12], [13, 14], [15, 16]],
//           [[17, 18], [19, 20], [21, 22], [23, 24]]
//          ],
//          [
//           [[25, 26], [27, 28], [29, 30], [31, 32]],
//           [[33, 34], [35, 36], [37, 38], [39, 40]],
//           [[41, 42], [43, 44], [45, 46], [47, 48]]
//          ]
//         ]
// %scatter_indices: [
//                    [
//                     [[0, 0], [1, 0], [2, 1]],
//                     [[0, 1], [1, 1], [0, 9]]
//                    ],
//                    [
//                     [[0, 0], [2, 1], [2, 2]],
//                     [[1, 2], [0, 1], [1, 0]]
//                    ]
//                   ]
// %update: [
//           [
//            [[1, 1], [1, 1], [1, 1]],
//            [[1, 1], [1, 1], [1, 1]]
//           ],
//           [
//            [[1, 1], [1, 1], [1, 1]],
//            [[1, 1], [1, 1], [1, 1]]
//           ]
//          ]
%result = "stablehlo.scatter"(%input, %scatter_indices, %update) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i64&gt;
    "stablehlo.return"(%0) : (tensor&lt;i64&gt;) -&gt; ()
}) {
  scatter_dimension_numbers = #stablehlo.scatter&lt;
    update_window_dims = [3, 4],
    inserted_window_dims = [1],
    input_batching_dims = [0],
    scatter_indices_batching_dims = [1],
    scatter_dims_to_operand_dims = [2, 1],
    index_vector_dim = 3&gt;,
  indices_are_sorted = false,
  unique_indices = false
} : (tensor&lt;2x3x4x2xi64&gt;, tensor&lt;2x2x3x2xi64&gt;, tensor&lt;2x2x3x2x2xi64&gt;) -&gt; tensor&lt;2x3x4x2xi64&gt;
// %result: [
//           [
//            [[3, 4], [6, 7], [6, 7], [7, 8]],
//            [[9, 10],[11, 12], [15, 16], [17, 18]],
//            [[17, 18], [19, 20], [22, 23], [24, 25]]
//           ],
//           [
//            [[25, 26], [28, 29], [30, 31], [31, 32]],
//            [[35, 36], [38, 39], [38, 39], [39, 40]],
//            [[41, 42], [44, 45], [46, 47], [47, 48]]
//           ]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/scatter.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="select">select<a class="anchor" aria-label="anchor" href="#select"></a></h3>
<div class="section level4">
<h4 id="semantics-84">Semantics<a class="anchor" aria-label="anchor" href="#semantics-84"></a></h4>
<p>Produces a <code>result</code> tensor where each element is selected from <code>on_true</code> or <code>on_false</code> tensor based on the value of the corresponding element of <code>pred</code>. More formally, <code>result[result_index] = pred_element ? on_true[result_index] : on_false[result_index]</code>, where <code>pred_element = rank(pred) = 0 ? pred[] : pred[result_index]</code>. For quantized types, performs <code>dequantize_select_quantize(pred, on_true, on_false, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-80">Inputs<a class="anchor" aria-label="anchor" href="#inputs-80"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="16%"><col width="54%"><col width="18%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>pred</code></td>
<td>tensor of type <code>i1</code>
</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>on_true</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C2)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>on_false</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-84">Outputs<a class="anchor" aria-label="anchor" href="#outputs-84"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-79">Constraints<a class="anchor" aria-label="anchor" href="#constraints-79"></a></h4>
<ul><li>(C1) <code>rank(pred) = 0 or shape(pred) = shape(on_true)</code>.</li>
<li>(C2) <code>baseline_type(on_true) = baseline_type(on_false) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-84">Examples<a class="anchor" aria-label="anchor" href="#examples-84"></a></h4>
<pre class="mlir"><code>// %pred: [[false, true], [true, false]]
// %on_true: [[1, 2], [3, 4]]
// %on_false: [[5, 6], [7, 8]]
%result = "stablehlo.select"(%pred, %on_true, %on_false) : (tensor&lt;2x2xi1&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[5, 2], [3, 8]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/select.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="select_and_scatter">select_and_scatter<a class="anchor" aria-label="anchor" href="#select_and_scatter"></a></h3>
<div class="section level4">
<h4 id="semantics-85">Semantics<a class="anchor" aria-label="anchor" href="#semantics-85"></a></h4>
<p>Scatters the values from the <code>source</code> tensor using <code>scatter</code> based on the outcome of <code>reduce_window</code> of the <code>input</code> tensor using <code>select</code> and produces a <code>result</code> tensor.</p>
<p>The following diagram shows how elements in <code>result</code> are computed from <code>operand</code> and <code>source</code> using a concrete example.</p>
<div class="float">
<img src="images/spec/select_and_scatter.svg" alt="select_and_scatter"><div class="figcaption">select_and_scatter</div>
</div>
<p>More formally:</p>
<ul><li>
<code>selected_values = reduce_window_without_init(...)</code> with the following inputs:
<ul><li><code>inputs = [operand].</code></li>
<li>
<code>window_dimensions</code>, <code>window_strides</code>, and <code>padding</code> which are used as is.</li>
<li>
<code>base_dilations = windows_dilations = 1</code>.</li>
<li>
<code>body</code> is defined as:</li>
</ul><div class="sourceCode" id="cb116"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" tabindex="-1"></a><span class="kw">def</span> body(arg0: tensor<span class="op">&lt;</span>E<span class="op">&gt;</span>, arg1: tensor<span class="op">&lt;</span>E<span class="op">&gt;</span>) <span class="op">-&gt;</span> tensor<span class="op">&lt;</span>E<span class="op">&gt;</span>:</span>
<span id="cb116-2"><a href="#cb116-2" tabindex="-1"></a>  <span class="cf">return</span> select(arg0, arg1) ? arg0 : arg1<span class="op">;</span></span></code></pre></div>
where <code>E = element_type(operand)</code>, and <code>reduce_window_without_init</code> works exactly like <code>reduce_window</code>, except that the <code>schedule</code> of the underlying <code>reduce</code> (see <a href="#reduce">reduce</a>) doesn’t include init values. It is currently unspecified what happens if the corresponding window doesn’t have values (<a href="https://github.com/openxla/stablehlo/issues/731" class="external-link">#731</a>).</li>
<li>
<code>result[result_index] = reduce([source_values], [init_value], [0], scatter)</code> where:
<ul><li>
<code>source_values = [source[source_index] for source_index in  source_indices]</code>.</li>
<li>
<code>selected_index(source_index) = operand_index</code> if <code>selected_values[source_index]</code> has the <code>operand</code> element from <code>operand_index</code>.</li>
<li>
<code>source_indices = [source_index for source_index in  indices(source) if selected_index(source_index) = result_index]</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-81">Inputs<a class="anchor" aria-label="anchor" href="#inputs-81"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="19%"><col width="50%"><col width="23%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C4), (C6), (C8-C11)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>source</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C2)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>init_value</code></td>
<td>0-dimensional tensor or per-tensor quantized tensor</td>
<td>(C3)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>window_dimensions</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4), (C5)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>window_strides</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C6), (C7)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>padding</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C8)</td>
</tr><tr class="odd"><td>(I7)</td>
<td><code>select</code></td>
<td>function</td>
<td>(C9)</td>
</tr><tr class="even"><td>(I8)</td>
<td><code>scatter</code></td>
<td>function</td>
<td>(C10)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-85">Outputs<a class="anchor" aria-label="anchor" href="#outputs-85"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C11-C12)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-80">Constraints<a class="anchor" aria-label="anchor" href="#constraints-80"></a></h4>
<!-- markdownlint-disable line-length -->
<ul><li>(C1) <code>element_type(operand) = element_type(source)</code>.</li>
<li>(C2) <code>shape(source) = num_windows</code> where:
<ul><li>
<code>padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1]</code>.</li>
<li>
<code>is_empty_window = padded_operand_shape = 0 || window_dimensions &gt; padded_operand_shape</code>.</li>
<li>
<code>num_windows = is_empty_window ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1</code>.</li>
</ul></li>
<li>(C3) <code>element_type(init_value) = element_type(operand)</code>.</li>
<li>(C4) <code>size(window_dimensions) = rank(operand)</code>.</li>
<li>(C5) <code>0 &lt; window_dimensions</code>.</li>
<li>(C6) <code>size(window_strides) = rank(operand)</code>.</li>
<li>(C7) <code>0 &lt; window_strides</code>.</li>
<li>(C8) <code>shape(padding) = [rank(operand), 2]</code>.</li>
<li>(C9) <code>select</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; tensor&lt;i1&gt;</code> where <code>E = element_type(operand)</code>.</li>
<li>(C10) <code>scatter</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; tensor&lt;E&gt;</code> where <code>is_promotable(element_type(operand), E)</code>.</li>
<li>(C11) <code>shape(operand) = shape(result)</code>.</li>
<li>(C12) <code>element_type(result) = E</code>. <!-- markdownlint-enable line-length -->
</li>
</ul></div>
<div class="section level4">
<h4 id="examples-85">Examples<a class="anchor" aria-label="anchor" href="#examples-85"></a></h4>
<pre class="mlir"><code>// %operand: [[1, 5], [2, 5], [3, 6], [4, 4]]
// %source: [[5, 6], [7, 8]]
// %init_value: 0
%result = "stablehlo.select_and_scatter"(%operand, %source, %init_value) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = "stablehlo.compare"(%arg0, %arg1) {
      comparison_direction = #stablehlo&lt;comparison_direction GE&gt;
    } : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i1&gt;
    "stablehlo.return"(%0) : (tensor&lt;i1&gt;) -&gt; ()
}, {
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %0 = "stablehlo.add"(%arg0, %arg1) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i64&gt;
    "stablehlo.return"(%0) : (tensor&lt;i64&gt;) -&gt; ()
}) {
  window_dimensions = array&lt;i64: 3, 1&gt;,
  window_strides = array&lt;i64: 2, 1&gt;,
  padding = dense&lt;[[0, 1], [0, 0]]&gt; : tensor&lt;2x2xi64&gt;
} : (tensor&lt;4x2xi64&gt;, tensor&lt;2x2xi64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;4x2xi64&gt;
// %result: [[0, 0], [0, 0], [5, 14], [7, 0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/select_and_scatter.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="send">send<a class="anchor" aria-label="anchor" href="#send"></a></h3>
<div class="section level4">
<h4 id="semantics-86">Semantics<a class="anchor" aria-label="anchor" href="#semantics-86"></a></h4>
<p>Sends <code>inputs</code> to a channel <code>channel_id</code>. Inputs are then sent to other devices in the order specified by <code>source_target_pairs</code>. The operation produces a <code>result</code> token.</p>
<p>If <code>is_host_transfer</code> is <code>true</code>, then the operation transfers data to the host. Otherwise, it transfers data to another device based on the values of <code>source_target_pairs</code>. This flag duplicates the information provided in <code>channel_type</code>, so in the future we are planning to only keep one of them (<a href="https://github.com/openxla/stablehlo/issues/666" class="external-link">#666</a>). If <code>is_host_transfer</code> = <code>false</code> and <code>source_target_pairs</code> is <code>None</code> or empty, it is considered undefined behavior.</p>
</div>
<div class="section level4">
<h4 id="inputs-82">Inputs<a class="anchor" aria-label="anchor" href="#inputs-82"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="24%"><col width="52%"><col width="15%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or quantized tensors</td>
<td></td>
</tr><tr class="even"><td>(I2)</td>
<td><code>token</code></td>
<td><code>token</code></td>
<td></td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>channel_id</code></td>
<td>constant of type <code>si64</code>
</td>
<td></td>
</tr><tr class="even"><td>(I4)</td>
<td><code>channel_type</code></td>
<td>enum of <code>DEVICE_TO_DEVICE</code> and <code>DEVICE_TO_HOST</code>
</td>
<td>(C5)</td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>is_host_transfer</code></td>
<td>constant of type <code>i1</code>
</td>
<td>(C5-C6)</td>
</tr><tr class="even"><td>(I6)</td>
<td><code>source_target_pairs</code></td>
<td>2-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C1-C4), (C6)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-86">Outputs<a class="anchor" aria-label="anchor" href="#outputs-86"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td><code>token</code></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-81">Constraints<a class="anchor" aria-label="anchor" href="#constraints-81"></a></h4>
<ul><li>(C1) <code>dim(source_target_pairs, 1) = 2</code>.</li>
<li>(C2) <code>is_unique(source_target_pairs[:, 0])</code>.</li>
<li>(C3) <code>is_unique(source_target_pairs[:, 1])</code>.</li>
<li>(C4) <code>0 &lt;= source_target_pairs &lt; N</code>, where <code>N</code> is defined as:
<ul><li>
<code>num_replicas</code> if <code>cross_replica</code> is used.</li>
<li>
<code>num_partitions</code> if <code>cross_partition</code> is used.</li>
</ul></li>
<li>(C5) <code>channel_type</code> is defined as:
<ul><li>
<code>DEVICE_TO_HOST</code> if <code>is_host_transfer = true</code>,</li>
<li>
<code>DEVICE_TO_DEVICE</code> otherwise.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="examples-86">Examples<a class="anchor" aria-label="anchor" href="#examples-86"></a></h4>
<pre class="mlir"><code>%result = "stablehlo.send"(%operand, %token) {
  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 1&gt;,
  is_host_transfer = false,
  source_target_pairs = dense&lt;[[0, 1], [1, 2]]&gt; : tensor&lt;2x2xi64&gt;
} : (tensor&lt;2x2xi64&gt;, !stablehlo.token) -&gt; !stablehlo.token</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/send_recv.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="shift_left">shift_left<a class="anchor" aria-label="anchor" href="#shift_left"></a></h3>
<div class="section level4">
<h4 id="semantics-87">Semantics<a class="anchor" aria-label="anchor" href="#semantics-87"></a></h4>
<p>Performs element-wise left-shift operation on the <code>lhs</code> tensor by <code>rhs</code> number of bits and produces a <code>result</code> tensor.</p>
</div>
<div class="section level4">
<h4 id="inputs-83">Inputs<a class="anchor" aria-label="anchor" href="#inputs-83"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-87">Outputs<a class="anchor" aria-label="anchor" href="#outputs-87"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-82">Constraints<a class="anchor" aria-label="anchor" href="#constraints-82"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-87">Examples<a class="anchor" aria-label="anchor" href="#examples-87"></a></h4>
<pre class="mlir"><code>// %lhs: [-1, 0, 1]
// %rhs: [1, 2, 3]
%result = "stablehlo.shift_left"(%lhs, %rhs): (tensor&lt;3xi64&gt;, tensor&lt;3xi64&gt;) -&gt; tensor&lt;3xi64&gt;
// %result: [-2, 0, 8]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/shift_left.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="shift_right_arithmetic">shift_right_arithmetic<a class="anchor" aria-label="anchor" href="#shift_right_arithmetic"></a></h3>
<div class="section level4">
<h4 id="semantics-88">Semantics<a class="anchor" aria-label="anchor" href="#semantics-88"></a></h4>
<p>Performs element-wise arithmetic right-shift operation on the <code>lhs</code> tensor by <code>rhs</code> number of bits and produces a <code>result</code> tensor.</p>
</div>
<div class="section level4">
<h4 id="inputs-84">Inputs<a class="anchor" aria-label="anchor" href="#inputs-84"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-88">Outputs<a class="anchor" aria-label="anchor" href="#outputs-88"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-83">Constraints<a class="anchor" aria-label="anchor" href="#constraints-83"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-88">Examples<a class="anchor" aria-label="anchor" href="#examples-88"></a></h4>
<pre class="mlir"><code>// %lhs: [-1, 0, 8]
// %rhs: [1, 2, 3]
%result = "stablehlo.shift_right_arithmetic"(%lhs, %rhs): (tensor&lt;3xi64&gt;, tensor&lt;3xi64&gt;) -&gt; tensor&lt;3xi64&gt;
// %result: [-1, 0, 1]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/shift_right_arithmetic.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="shift_right_logical">shift_right_logical<a class="anchor" aria-label="anchor" href="#shift_right_logical"></a></h3>
<div class="section level4">
<h4 id="semantics-89">Semantics<a class="anchor" aria-label="anchor" href="#semantics-89"></a></h4>
<p>Performs element-wise logical right-shift operation on the <code>lhs</code> tensor by <code>rhs</code> number of bits and produces a <code>result</code> tensor.</p>
</div>
<div class="section level4">
<h4 id="inputs-85">Inputs<a class="anchor" aria-label="anchor" href="#inputs-85"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-89">Outputs<a class="anchor" aria-label="anchor" href="#outputs-89"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-84">Constraints<a class="anchor" aria-label="anchor" href="#constraints-84"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-89">Examples<a class="anchor" aria-label="anchor" href="#examples-89"></a></h4>
<pre class="mlir"><code>// %lhs: [-1, 0, 8]
// %rhs: [1, 2, 3]
%result = "stablehlo.shift_right_logical"(%lhs, %rhs): (tensor&lt;3xi64&gt;, tensor&lt;3xi64&gt;) -&gt; tensor&lt;3xi64&gt;
// %result: [9223372036854775807, 0, 1]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/shift_right_logical.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="sign">sign<a class="anchor" aria-label="anchor" href="#sign"></a></h3>
<div class="section level4">
<h4 id="semantics-90">Semantics<a class="anchor" aria-label="anchor" href="#semantics-90"></a></h4>
<p>Returns the sign of the <code>operand</code> element-wise and produces a <code>result</code> tensor. More formally, for each element <code>x</code>, the semantics can be expressed using Python syntax as follows:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" tabindex="-1"></a><span class="kw">def</span> sign(x):</span>
<span id="cb122-2"><a href="#cb122-2" tabindex="-1"></a>  <span class="cf">if</span> is_integer(x):</span>
<span id="cb122-3"><a href="#cb122-3" tabindex="-1"></a>    <span class="cf">if</span> compare(x, <span class="dv">0</span>, LT, SIGNED): <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb122-4"><a href="#cb122-4" tabindex="-1"></a>    <span class="cf">if</span> compare(x, <span class="dv">0</span>, EQ, SIGNED): <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb122-5"><a href="#cb122-5" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb122-6"><a href="#cb122-6" tabindex="-1"></a>  <span class="cf">elif</span> is_float(x):</span>
<span id="cb122-7"><a href="#cb122-7" tabindex="-1"></a>    <span class="cf">if</span> is_nan(x): <span class="cf">return</span> NaN</span>
<span id="cb122-8"><a href="#cb122-8" tabindex="-1"></a>    <span class="cf">if</span> compare(x, <span class="op">-</span><span class="fl">0.0</span>, EQ, FLOAT): <span class="cf">return</span> <span class="op">-</span><span class="fl">0.0</span></span>
<span id="cb122-9"><a href="#cb122-9" tabindex="-1"></a>    <span class="cf">if</span> compare(x, <span class="op">+</span><span class="fl">0.0</span>, EQ, FLOAT): <span class="cf">return</span> <span class="op">+</span><span class="fl">0.0</span></span>
<span id="cb122-10"><a href="#cb122-10" tabindex="-1"></a>    <span class="cf">if</span> compare(x, <span class="fl">0.0</span>, LT, FLOAT): <span class="cf">return</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb122-11"><a href="#cb122-11" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span></span>
<span id="cb122-12"><a href="#cb122-12" tabindex="-1"></a>  <span class="cf">elif</span> is_complex(x):</span>
<span id="cb122-13"><a href="#cb122-13" tabindex="-1"></a>    <span class="cf">if</span> is_nan(real(x)) <span class="kw">or</span> is_nan(imag(x)): <span class="cf">return</span> (NaN, NaN)</span>
<span id="cb122-14"><a href="#cb122-14" tabindex="-1"></a>    <span class="cf">if</span> compare(x, (<span class="fl">0.0</span>, <span class="fl">0.0</span>), EQ, FLOAT): <span class="cf">return</span> (<span class="fl">0.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb122-15"><a href="#cb122-15" tabindex="-1"></a>    <span class="cf">return</span> divide(x, convert(<span class="bu">abs</span>(x), <span class="bu">type</span>(x)))</span></code></pre></div>
<p>For quantized types, performs <code>dequantize_op_quantize(sign, operand, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-86">Inputs<a class="anchor" aria-label="anchor" href="#inputs-86"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="5%"><col width="9%"><col width="74%"><col width="10%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of signed integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-90">Outputs<a class="anchor" aria-label="anchor" href="#outputs-90"></a></h4>
<table class="table"><colgroup><col width="8%"><col width="79%"><col width="11%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of signed integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-85">Constraints<a class="anchor" aria-label="anchor" href="#constraints-85"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-90">Examples<a class="anchor" aria-label="anchor" href="#examples-90"></a></h4>
<pre class="mlir"><code>// Logical values: +NaN, -1.0, -0.0, +0.0, 1.0
// operand: [0x7FFFFFFFFFFFFFFF, -1.0, -0.0, 0.0, 1.0]
%result = "stablehlo.sign"(%operand) : (tensor&lt;5xf64&gt;) -&gt; tensor&lt;5xf64&gt;
// Logical values: +NaN, -1.0, -0.0, +0.0, 1.0
// %result: [0x7FFFFFFFFFFFFFFF, -1.0, -0.0, 0.0, 1.0]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/sign.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="sine">sine<a class="anchor" aria-label="anchor" href="#sine"></a></h3>
<div class="section level4">
<h4 id="semantics-91">Semantics<a class="anchor" aria-label="anchor" href="#semantics-91"></a></h4>
<p>Performs element-wise sine operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>sin</code> from IEEE-754.</li>
<li>For complex numbers: complex sine.</li>
<li>For quantized types: <code>dequantize_op_quantize(sine, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-87">Inputs<a class="anchor" aria-label="anchor" href="#inputs-87"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-91">Outputs<a class="anchor" aria-label="anchor" href="#outputs-91"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-86">Constraints<a class="anchor" aria-label="anchor" href="#constraints-86"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-91">Examples<a class="anchor" aria-label="anchor" href="#examples-91"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [0.0, 1.57079632],       // [0, pi/2]
//            [3.14159265, 4.71238898] // [pi, 3pi/2]
//           ]
%result = "stablehlo.sine"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;
// %result: [[0.0, 1.0], [0.0, -1.0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/sine.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="slice">slice<a class="anchor" aria-label="anchor" href="#slice"></a></h3>
<div class="section level4">
<h4 id="semantics-92">Semantics<a class="anchor" aria-label="anchor" href="#semantics-92"></a></h4>
<p>Extracts a slice from the <code>operand</code> using statically-computed starting indices and produces a <code>result</code> tensor. <code>start_indices</code> contain the starting indices of the slice for each dimension, <code>limit_indices</code> contain the ending indices (exclusive) for the slice for each dimension, and <code>strides</code> contain the strides for each dimension.</p>
<p>More formally, <code>result[result_index] = operand[operand_index]</code> where <code>operand_index = start_indices + result_index * strides</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-88">Inputs<a class="anchor" aria-label="anchor" href="#inputs-88"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="19%"><col width="52%"><col width="20%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1-C3), (C5)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>start_indices</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C3), (C5)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>limit_indices</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C3), (C5)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>strides</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2), (C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-92">Outputs<a class="anchor" aria-label="anchor" href="#outputs-92"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or per-tensor quantized tensor</td>
<td>(C1), (C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-87">Constraints<a class="anchor" aria-label="anchor" href="#constraints-87"></a></h4>
<ul><li>(C1) <code>element_type(operand) = element_type(result)</code>.</li>
<li>(C2) <code>size(start_indices) = size(limit_indices) = size(strides) =   rank(operand)</code>.</li>
<li>(C3) <code>0 &lt;= start_indices &lt;= limit_indices &lt;= shape(operand)</code>.</li>
<li>(C4) <code>0 &lt; strides</code>.</li>
<li>(C5) <code>shape(result) = ceil((limit_indices - start_indices) / strides)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-92">Examples<a class="anchor" aria-label="anchor" href="#examples-92"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [0, 0, 0, 0],
//            [0, 0, 1, 1],
//            [0, 0, 1, 1]
//           ]
%result = "stablehlo.slice"(%operand) {
  start_indices = array&lt;i64: 1, 2&gt;,
  limit_indices = array&lt;i64: 3, 4&gt;,
  strides = array&lt;i64: 1, 1&gt;
} : (tensor&lt;3x4xi64&gt;) -&gt; tensor&lt;2x2xi64&gt;
// % result: [
//            [1, 1],
//            [1, 1]
//           ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/slice.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="sort">sort<a class="anchor" aria-label="anchor" href="#sort"></a></h3>
<div class="section level4">
<h4 id="semantics-93">Semantics<a class="anchor" aria-label="anchor" href="#semantics-93"></a></h4>
<p>Sorts 1-dimensional slices of <code>inputs</code> along the dimension <code>dimension</code> together, according to a <code>comparator</code> and produces <code>results</code>.</p>
<p>Unlike similar inputs in other operations, <code>dimension</code> allows negative values, with the semantics described below. In the future, this may be disallowed for consistency reasons (<a href="https://github.com/openxla/stablehlo/issues/1377" class="external-link">#1377</a>).</p>
<p>If <code>is_stable</code> is true, then the sorting is stable, that is, relative order of elements considered to be equal by the comparator is preserved. For the case where there is a single input, two elements <code>e1</code> and <code>e2</code> are considered to be equal by the comparator if and only if <code>comparator(e1, e2) = comparator(e2, e1) = false</code>. See the formalization below for how this generalizes to multiple inputs.</p>
<p>More formally, for all <code>result_index</code> in <code>index_space(results[0])</code>:</p>
<ul><li><p><code>adjusted_dimension = dimension &gt;= 0 ? dimension : rank(inputs[0]) + dimension</code>.</p></li>
<li><p><code>result_slice = [ri0, ..., :, ..., riR-1]</code> where <code>riN</code> are individual elements in <code>result_index</code>, and <code>:</code> is inserted at <code>adjusted_dimension</code>.</p></li>
<li><p><code>inputs_together = (inputs[0]..., ..., inputs[N-1]...)</code>.</p></li>
<li><p><code>results_together[result_slice] = sort(inputs_together[result_slice], comparator_together)</code>.</p></li>
<li><p>where <code>sort</code> sorts a 1-dimensional slice in non-descending order expecting that <code>comparator_together</code> returns <code>true</code> if the left-hand side argument is less than the right-hand second argument.</p></li>
<li>
<p> </p>
<div class="sourceCode" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" tabindex="-1"></a><span class="kw">def</span> comparator_together(lhs_together, rhs_together):</span>
<span id="cb126-2"><a href="#cb126-2" tabindex="-1"></a>  args <span class="op">=</span> []</span>
<span id="cb126-3"><a href="#cb126-3" tabindex="-1"></a>  <span class="cf">for</span> (lhs_el, rhs_el) <span class="kw">in</span> <span class="bu">zip</span>(lhs_together, rhs_together):</span>
<span id="cb126-4"><a href="#cb126-4" tabindex="-1"></a>    args.append(lhs_el)</span>
<span id="cb126-5"><a href="#cb126-5" tabindex="-1"></a>    args.append(rhs_el)</span>
<span id="cb126-6"><a href="#cb126-6" tabindex="-1"></a>  <span class="cf">return</span> comparator(<span class="op">*</span>args)</span></code></pre></div>
</li>
<li><p><code>(results[0]..., ..., results[N-1]...) = results_together</code>.</p></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-89">Inputs<a class="anchor" aria-label="anchor" href="#inputs-89"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="14%"><col width="63%"><col width="13%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>inputs</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C1-C5)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>dimension</code></td>
<td>constant of type <code>si64</code>
</td>
<td>(C4)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>is_stable</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr><tr class="even"><td>(I4)</td>
<td><code>comparator</code></td>
<td>function</td>
<td>(C5)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-93">Outputs<a class="anchor" aria-label="anchor" href="#outputs-93"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="71%"><col width="15%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors or per-tensor quantized tensors</td>
<td>(C2), (C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-88">Constraints<a class="anchor" aria-label="anchor" href="#constraints-88"></a></h4>
<ul><li>(C1) <code>0 &lt; size(inputs)</code>.</li>
<li>(C2) <code>type(inputs...) = type(results...)</code>.</li>
<li>(C3) <code>same(shape(inputs...) + shape(results...))</code>.</li>
<li>(C4) <code>-R &lt;= dimension &lt; R</code>, where <code>R = rank(inputs[0])</code>.</li>
<li>(C5) <code>comparator</code> has type <code>(tensor&lt;E1&gt;, tensor&lt;E1&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;EN-1&gt;) -&gt; tensor&lt;i1&gt;</code>, where <code>Ei = element_type(inputs[i])</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-93">Examples<a class="anchor" aria-label="anchor" href="#examples-93"></a></h4>
<pre class="mlir"><code>// %input0 = [[1, 2, 3], [3, 2, 1]]
// %input1 = [[3, 2, 1], [1, 2, 3]]
%result0, %result1 = "stablehlo.sort"(%input0, %input1) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;, %arg2: tensor&lt;i64&gt;, %arg3: tensor&lt;i64&gt;):
    %predicate = "stablehlo.compare"(%arg0, %arg1) {
      comparison_direction = #stablehlo&lt;comparison_direction GT&gt;
    } : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i1&gt;
    "stablehlo.return"(%predicate) : (tensor&lt;i1&gt;) -&gt; ()
}) {
  dimension = 0 : i64,
  is_stable = true
} : (tensor&lt;2x3xi64&gt;, tensor&lt;2x3xi64&gt;) -&gt; (tensor&lt;2x3xi64&gt;, tensor&lt;2x3xi64&gt;)
// %result0 = [[3, 2, 3], [1, 2, 1]]
// %result1 = [[1, 2, 1], [3, 2, 3]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/sort.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="sqrt">sqrt<a class="anchor" aria-label="anchor" href="#sqrt"></a></h3>
<div class="section level4">
<h4 id="semantics-94">Semantics<a class="anchor" aria-label="anchor" href="#semantics-94"></a></h4>
<p>Performs element-wise square root operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>squareRoot</code> from IEEE-754.</li>
<li>For complex numbers: complex square root.</li>
<li>For quantized types: <code>dequantize_op_quantize(sqrt, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-90">Inputs<a class="anchor" aria-label="anchor" href="#inputs-90"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-94">Outputs<a class="anchor" aria-label="anchor" href="#outputs-94"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-89">Constraints<a class="anchor" aria-label="anchor" href="#constraints-89"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-94">Examples<a class="anchor" aria-label="anchor" href="#examples-94"></a></h4>
<pre class="mlir"><code>// %operand: [[0.0, 1.0], [4.0, 9.0]]
%result = "stablehlo.sqrt"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;
// %result: [[0.0, 1.0], [2.0, 3.0]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/sqrt.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="subtract">subtract<a class="anchor" aria-label="anchor" href="#subtract"></a></h3>
<div class="section level4">
<h4 id="semantics-95">Semantics<a class="anchor" aria-label="anchor" href="#semantics-95"></a></h4>
<p>Performs element-wise subtraction of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For integers: integer subtraction.</li>
<li>For floats: <code>subtraction</code> from IEEE-754.</li>
<li>For complex numbers: complex subtraction.</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(subtract, lhs, rhs, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-91">Inputs<a class="anchor" aria-label="anchor" href="#inputs-91"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="6%"><col width="75%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-95">Outputs<a class="anchor" aria-label="anchor" href="#outputs-95"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="78%"><col width="12%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of integer, floating-point, or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-90">Constraints<a class="anchor" aria-label="anchor" href="#constraints-90"></a></h4>
<ul><li>(C1) <code>baseline_type(lhs) = baseline_type(rhs) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-95">Examples<a class="anchor" aria-label="anchor" href="#examples-95"></a></h4>
<pre class="mlir"><code>// %lhs: [[6, 8], [10, 12]]
// %rhs: [[5, 6], [7, 8]]
%result = "stablehlo.subtract"(%lhs, %rhs) : (tensor&lt;2x2xf32&gt;, tensor&lt;2x2xf32&gt;) -&gt; (tensor&lt;2x2xf32&gt;)
// %result: [[1, 2], [3, 4]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/subtract.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="tan">tan<a class="anchor" aria-label="anchor" href="#tan"></a></h3>
<div class="section level4">
<h4 id="semantics-96">Semantics<a class="anchor" aria-label="anchor" href="#semantics-96"></a></h4>
<p>Performs element-wise tangent operation on the <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>tan</code> from IEEE-754.</li>
<li>For complex numbers: complex tangent.</li>
<li>For quantized types: <code>dequantize_op_quantize(tan, operand, type(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-92">Inputs<a class="anchor" aria-label="anchor" href="#inputs-92"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-96">Outputs<a class="anchor" aria-label="anchor" href="#outputs-96"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-91">Constraints<a class="anchor" aria-label="anchor" href="#constraints-91"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-96">Examples<a class="anchor" aria-label="anchor" href="#examples-96"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [0.0, 1.57079632],       // [0, pi/2]
//            [3.14159265, 4.71238898] // [pi, 3pi/2]
//           ]
%result = "stablehlo.tan"(%operand) : (tensor&lt;2x2xf64&gt;) -&gt; tensor&lt;2x2xf64&gt;
// %result: [
//           [0.0, 1.63312e+16],
//           [0.0, 5.44375e+15]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/tan.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="tanh">tanh<a class="anchor" aria-label="anchor" href="#tanh"></a></h3>
<div class="section level4">
<h4 id="semantics-97">Semantics<a class="anchor" aria-label="anchor" href="#semantics-97"></a></h4>
<p>Performs element-wise hyperbolic tangent operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For floats: <code>tanh</code> from IEEE-754.</li>
<li>For complex numbers: complex hyperbolic tangent.</li>
<li>For quantized types:
<ul><li>
<code>dequantize_op_quantize(tanh, operand, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-93">Inputs<a class="anchor" aria-label="anchor" href="#inputs-93"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="10%"><col width="70%"><col width="12%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-97">Outputs<a class="anchor" aria-label="anchor" href="#outputs-97"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-92">Constraints<a class="anchor" aria-label="anchor" href="#constraints-92"></a></h4>
<ul><li>(C1) <code>baseline_type(operand) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-97">Examples<a class="anchor" aria-label="anchor" href="#examples-97"></a></h4>
<pre class="mlir"><code>// %operand: [-1.0, 0.0, 1.0]
%result = "stablehlo.tanh"(%operand) : (tensor&lt;3xf32&gt;) -&gt; tensor&lt;3xf32&gt;
// %result: [-0.76159416, 0.0, 0.76159416]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/tanh.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="transpose">transpose<a class="anchor" aria-label="anchor" href="#transpose"></a></h3>
<div class="section level4">
<h4 id="semantics-98">Semantics<a class="anchor" aria-label="anchor" href="#semantics-98"></a></h4>
<p>Permutes the dimensions of <code>operand</code> tensor using <code>permutation</code> and produces a <code>result</code> tensor. More formally, <code>result[result_index] = operand[operand_index]</code> where <code>result_index[d] = operand_index[permutation[d]]</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-94">Inputs<a class="anchor" aria-label="anchor" href="#inputs-94"></a></h4>
<table class="table"><colgroup><col width="8%"><col width="18%"><col width="56%"><col width="16%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor or quantized tensor</td>
<td>(C1-C4)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>permutation</code></td>
<td>1-dimensional tensor constant of type <code>si64</code>
</td>
<td>(C2-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-98">Outputs<a class="anchor" aria-label="anchor" href="#outputs-98"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor or quantized tensor</td>
<td>(C1), (C3-C4)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-93">Constraints<a class="anchor" aria-label="anchor" href="#constraints-93"></a></h4>
<ul><li>(C1) <code>element_type(result)</code> is given by:
<ul><li>
<code>element_type(operand)</code>, if <code>!is_per_axis_quantized(operand)</code>.</li>
<li>
<code>element_type(operand)</code> except that <code>quantization_dimension(operand)</code> and <code>quantization_dimension(result)</code> may differ, otherwise.</li>
</ul></li>
<li>(C2) <code>permutation</code> is a permutation of <code>range(rank(operand))</code>.</li>
<li>(C3) <code>shape(result) = dim(operand, permutation...)</code>.</li>
<li>(C4) If <code>is_per_axis_quantized(result)</code>, then <code>quantization_dimension(operand) =   permutation(quantization_dimension(result))</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-98">Examples<a class="anchor" aria-label="anchor" href="#examples-98"></a></h4>
<pre class="mlir"><code>// %operand: [
//            [[1,2], [3,4], [5,6]],
//            [[7,8], [9,10], [11,12]]
//           ]
%result = "stablehlo.transpose"(%operand) {
  permutation = array&lt;i64: 2, 1, 0&gt;
} : (tensor&lt;2x3x2xi32&gt;) -&gt; tensor&lt;2x3x2xi32&gt;
// %result: [
//           [[1,7], [3,9], [5,11]],
//           [[2,8], [4,10], [6,12]]
//          ]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/transpose.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="triangular_solve">triangular_solve<a class="anchor" aria-label="anchor" href="#triangular_solve"></a></h3>
<div class="section level4">
<h4 id="semantics-99">Semantics<a class="anchor" aria-label="anchor" href="#semantics-99"></a></h4>
<p>Solves batches of systems of linear equations with lower or upper triangular coefficient matrices.</p>
<p>More formally, given <code>a</code> and <code>b</code>, <code>result[i0, ..., iR-3, :, :]</code> is the solution to <code>op(a[i0, ..., iR-3, :, :]) * x = b[i0, ..., iR-3, :, :]</code> when <code>left_side</code> is <code>true</code> or <code>x * op(a[i0, ..., iR-3, :, :]) = b[i0, ..., iR-3, :, :]</code> when <code>left_side</code> is <code>false</code>, solving for the variable <code>x</code> where <code>op(a)</code> is determined by <code>transpose_a</code>, which can be one of the following:</p>
<ul><li>
<code>NO_TRANSPOSE</code>: Perform operation using <code>a</code> as-is.</li>
<li>
<code>TRANSPOSE</code>: Perform operation on transpose of <code>a</code>.</li>
<li>
<code>ADJOINT</code>: Perform operation on conjugate transpose of <code>a</code>.</li>
</ul><p>Input data is read only from the lower triangle of <code>a</code>, if <code>lower</code> is <code>true</code> or upper triangle of <code>a</code>, otherwise. Output data is returned in the same triangle; the values in the other triangle are implementation-defined.</p>
<p>If <code>unit_diagonal</code> is true, then the implementation can assume that the diagonal elements of <code>a</code> are equal to 1, otherwise the behavior is undefined.</p>
<p>For quantized types, performs <code>dequantize_op_quantize(lambda x, y: triangular_solve(x, y, left_side, lower, unit_diagonal, transpose_a), a, b, type(result))</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-95">Inputs<a class="anchor" aria-label="anchor" href="#inputs-95"></a></h4>
<table class="table"><colgroup><col width="6%"><col width="15%"><col width="66%"><col width="11%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>a</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1-C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>b</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1-C4)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>left_side</code></td>
<td>constant of type <code>i1</code>
</td>
<td>(C3)</td>
</tr><tr class="even"><td>(I4)</td>
<td><code>lower</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr><tr class="odd"><td>(I5)</td>
<td><code>unit_diagonal</code></td>
<td>constant of type <code>i1</code>
</td>
<td></td>
</tr><tr class="even"><td>(I6)</td>
<td><code>transpose_a</code></td>
<td>enum of <code>NO_TRANSPOSE</code>, <code>TRANSPOSE</code>, and <code>ADJOINT</code>
</td>
<td></td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-99">Outputs<a class="anchor" aria-label="anchor" href="#outputs-99"></a></h4>
<table style="width:100%;" class="table"><colgroup><col width="10%"><col width="76%"><col width="13%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point or complex type or per-tensor quantized tensor</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-94">Constraints<a class="anchor" aria-label="anchor" href="#constraints-94"></a></h4>
<ul><li>(C1) <code>baseline_element_type(a) = baseline_element_type(b)</code>.</li>
<li>(C2) <code>2 &lt;= rank(a) = rank(b) = R</code>.</li>
<li>(C3) The relationship between <code>shape(a)</code> and <code>shape(b)</code> is defined as follows:
<ul><li>
<code>shape(a)[:-3] = shape(b)[:-3]</code>.</li>
<li>
<code>dim(a, -2) = dim(a, -1) = dim(b, left_side ? -2 : -1)</code>.</li>
</ul></li>
<li>(C4) <code>baseline_type(b) = baseline_type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-99">Examples<a class="anchor" aria-label="anchor" href="#examples-99"></a></h4>
<pre class="mlir"><code>// %a = [
//       [1.0, 0.0, 0.0],
//       [2.0, 4.0, 0.0],
//       [3.0, 5.0, 6.0]
//      ]
// %b = [
//       [2.0, 0.0, 0.0],
//       [4.0, 8.0, 0.0],
//       [6.0, 10.0, 12.0]
//      ]
%result = "stablehlo.triangular_solve"(%a, %b) {
  left_side = true,
  lower = true,
  unit_diagonal = false,
  transpose_a = #stablehlo&lt;transpose NO_TRANSPOSE&gt;
} : (tensor&lt;3x3xf32&gt;, tensor&lt;3x3xf32&gt;) -&gt; tensor&lt;3x3xf32&gt;
// %result: [
//           [2.0, 0.0, 0.0],
//           [0.0, 2.0, 0.0],
//           [0.0, 0.0, 2.0]
//          ]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="tuple">tuple<a class="anchor" aria-label="anchor" href="#tuple"></a></h3>
<blockquote>
<p>Note: Per <a href="https://github.com/openxla/stablehlo/pull/2283" class="external-link">StableHLO v1.0 Cleanup #2283</a>, this op is being explored for deprecation as it appears to be unused by both frameworks and compilers. As such, it has limited compatibility guarantees (6 months).</p>
</blockquote>
<div class="section level4">
<h4 id="semantics-100">Semantics<a class="anchor" aria-label="anchor" href="#semantics-100"></a></h4>
<p>Produces a <code>result</code> tuple from values <code>val</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-96">Inputs<a class="anchor" aria-label="anchor" href="#inputs-96"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>val</code></td>
<td>variadic number of values</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-100">Outputs<a class="anchor" aria-label="anchor" href="#outputs-100"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tuple</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-95">Constraints<a class="anchor" aria-label="anchor" href="#constraints-95"></a></h4>
<ul><li>(C1) <code>result</code> has type <code>tuple&lt;E0, ..., EN-1&gt;</code> where <code>Ei = type(val[i])</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-100">Examples<a class="anchor" aria-label="anchor" href="#examples-100"></a></h4>
<pre class="mlir"><code>// %val0: [1.0, 2.0]
// %val1: (3)
%result = "stablehlo.tuple"(%val0, %val1) : (tensor&lt;2xf32&gt;, tuple&lt;tensor&lt;i32&gt;&gt;) -&gt; tuple&lt;tensor&lt;2xf32&gt;, tuple&lt;tensor&lt;i32&gt;&gt;&gt;
// %result: ([1.0, 2.0], (3))</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/tuple_and_get_tuple_element.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="uniform_dequantize">uniform_dequantize<a class="anchor" aria-label="anchor" href="#uniform_dequantize"></a></h3>
<div class="section level4">
<h4 id="semantics-101">Semantics<a class="anchor" aria-label="anchor" href="#semantics-101"></a></h4>
<p>Performs element-wise conversion of quantized tensor <code>operand</code> to a floating-point tensor <code>result</code> according to the quantization parameters defined by the <code>operand</code> type.</p>
<p>More formally, <code>result = dequantize(operand)</code>.</p>
</div>
<div class="section level4">
<h4 id="inputs-97">Inputs<a class="anchor" aria-label="anchor" href="#inputs-97"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>quantized tensor</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-101">Outputs<a class="anchor" aria-label="anchor" href="#outputs-101"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of floating-point type</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-96">Constraints<a class="anchor" aria-label="anchor" href="#constraints-96"></a></h4>
<ul><li>(C1) <code>shape(operand) = shape(result)</code>.</li>
<li>(C2) <code>element_type(result) = expressed_type(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-101">Examples<a class="anchor" aria-label="anchor" href="#examples-101"></a></h4>
<pre class="mlir"><code>// %operand: [10, 10]
%result = "stablehlo.uniform_dequantize"(%operand) : (tensor&lt;2x!quant.uniform&lt;i8:f32:0, {0.1:-30,0.5:-20}&gt;&gt;) -&gt; tensor&lt;2xf32&gt;
// %result: [4.0, 15.0]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="uniform_quantize">uniform_quantize<a class="anchor" aria-label="anchor" href="#uniform_quantize"></a></h3>
<div class="section level4">
<h4 id="semantics-102">Semantics<a class="anchor" aria-label="anchor" href="#semantics-102"></a></h4>
<p>Performs element-wise conversion of floating-point tensor or quantized tensor <code>operand</code> to a quantized tensor <code>result</code> according to the quantization parameters defined by the <code>result</code> type.</p>
<p>More formally,</p>
<ul><li>If <code>is_float(operand)</code>:
<ul><li>
<code>result = quantize(operand, type(result))</code>.</li>
</ul></li>
<li>If <code>is_quantized(operand)</code>:
<ul><li>
<code>float_result = dequantize(operand)</code>.</li>
<li>
<code>result = quantize(float_result, type(result))</code>.</li>
</ul></li>
</ul></div>
<div class="section level4">
<h4 id="inputs-98">Inputs<a class="anchor" aria-label="anchor" href="#inputs-98"></a></h4>
<table class="table"><colgroup><col width="9%"><col width="14%"><col width="58%"><col width="17%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>tensor of floating-point or quantized type</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-102">Outputs<a class="anchor" aria-label="anchor" href="#outputs-102"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>quantized tensor</td>
<td>(C1), (C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-97">Constraints<a class="anchor" aria-label="anchor" href="#constraints-97"></a></h4>
<ul><li>(C1) <code>shape(operand) = shape(result)</code>.</li>
<li>(C2) <code>expressed_type(result) = is_float(operand) ? element_type(operand) :   expressed_type(operand)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-102">Examples<a class="anchor" aria-label="anchor" href="#examples-102"></a></h4>
<pre class="mlir"><code>// %operand: [4.0, 15.0]
%result = "stablehlo.uniform_quantize"(%operand) : (tensor&lt;2xf32&gt;) -&gt; tensor&lt;2x!quant.uniform&lt;i8:f32:0, {0.1:-30,0.5:-20}&gt;&gt;
// %result: [10, 10]

// %operand: [10, 10]
%result = "stablehlo.uniform_quantize"(%operand) : (tensor&lt;2x!quant.uniform&lt;i8:f32:0, {0.1:-30,0.5:-20}&gt;&gt;) -&gt; tensor&lt;2x!quant.uniform&lt;i8:f32:0, {0.1:-20,0.2:-30}&gt;&gt;
// %result: [20, 45]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="while">while<a class="anchor" aria-label="anchor" href="#while"></a></h3>
<div class="section level4">
<h4 id="semantics-103">Semantics<a class="anchor" aria-label="anchor" href="#semantics-103"></a></h4>
<p>Produces the output from executing <code>body</code> function 0 or more times while the <code>cond</code> function outputs <code>true</code>. More formally, the semantics can be expressed using Python syntax as follows:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" tabindex="-1"></a>internal_state <span class="op">=</span> operand</span>
<span id="cb137-2"><a href="#cb137-2" tabindex="-1"></a><span class="cf">while</span> cond(<span class="op">*</span>internal_state):</span>
<span id="cb137-3"><a href="#cb137-3" tabindex="-1"></a>  internal_state <span class="op">=</span> body(<span class="op">*</span>internal_state)</span>
<span id="cb137-4"><a href="#cb137-4" tabindex="-1"></a>results <span class="op">=</span> internal_state</span></code></pre></div>
<p>The behavior of an infinite loop is TBD (<a href="https://github.com/openxla/stablehlo/issues/383" class="external-link">#383</a>).</p>
</div>
<div class="section level4">
<h4 id="inputs-99">Inputs<a class="anchor" aria-label="anchor" href="#inputs-99"></a></h4>
<table class="table"><colgroup><col width="7%"><col width="12%"><col width="64%"><col width="14%"></colgroup><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>operand</code></td>
<td>variadic number of tensors, quantized tensors or tokens</td>
<td>(C1-C3)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>cond</code></td>
<td>function</td>
<td>(C1)</td>
</tr><tr class="odd"><td>(I3)</td>
<td><code>body</code></td>
<td>function</td>
<td>(C2)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-103">Outputs<a class="anchor" aria-label="anchor" href="#outputs-103"></a></h4>
<table class="table"><colgroup><col width="13%"><col width="70%"><col width="16%"></colgroup><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>results</code></td>
<td>variadic number of tensors, quantized tensors or tokens</td>
<td>(C3)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-98">Constraints<a class="anchor" aria-label="anchor" href="#constraints-98"></a></h4>
<ul><li>(C1) <code>cond</code> has type <code>(T0, ..., TN-1) -&gt; tensor&lt;i1&gt;</code>, where <code>Ti = type(operand[i])</code>.</li>
<li>(C2) <code>body</code> has type <code>(T0, ..., TN-1) -&gt; (T0, ..., TN-1)</code>, where <code>Ti = type(operand[i])</code>.</li>
<li>(C3) <code>type(results...) = type(operand...)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-103">Examples<a class="anchor" aria-label="anchor" href="#examples-103"></a></h4>
<pre class="mlir"><code>// %init_i: 1
// %init_sum: 0
// %one: 1
// %ten: 10
%results0, %results1 = "stablehlo.while"(%init_i, %init_sum) ({
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %cond = "stablehlo.compare"(%arg0, %ten) {
      comparison_direction = #stablehlo&lt;comparison_direction LT&gt;
    } : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;i1&gt;
    stablehlo.return %cond : tensor&lt;i1&gt;
  }, {
  ^bb0(%arg0: tensor&lt;i64&gt;, %arg1: tensor&lt;i64&gt;):
    %new_sum = stablehlo.add %arg1, %one : tensor&lt;i64&gt;
    %new_i = stablehlo.add %arg0, %one : tensor&lt;i64&gt;
    stablehlo.return %new_i, %new_sum : tensor&lt;i64&gt;, tensor&lt;i64&gt;
}) : (tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; (tensor&lt;i64&gt;, tensor&lt;i64&gt;)
// %results0: 10
// %results1: 10</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/while.mlir" class="external-link">More Examples</a></p>
</div>
</div>
<div class="section level3">
<h3 id="xor">xor<a class="anchor" aria-label="anchor" href="#xor"></a></h3>
<div class="section level4">
<h4 id="semantics-104">Semantics<a class="anchor" aria-label="anchor" href="#semantics-104"></a></h4>
<p>Performs element-wise XOR of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p>
<ul><li>For booleans: logical XOR.</li>
<li>For integers: bitwise XOR.</li>
</ul></div>
<div class="section level4">
<h4 id="inputs-100">Inputs<a class="anchor" aria-label="anchor" href="#inputs-100"></a></h4>
<table class="table"><thead><tr class="header"><th>Label</th>
<th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td>(I1)</td>
<td><code>lhs</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr><tr class="even"><td>(I2)</td>
<td><code>rhs</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="outputs-104">Outputs<a class="anchor" aria-label="anchor" href="#outputs-104"></a></h4>
<table class="table"><thead><tr class="header"><th>Name</th>
<th>Type</th>
<th>Constraints</th>
</tr></thead><tbody><tr class="odd"><td><code>result</code></td>
<td>tensor of boolean or integer type</td>
<td>(C1)</td>
</tr></tbody></table></div>
<div class="section level4">
<h4 id="constraints-99">Constraints<a class="anchor" aria-label="anchor" href="#constraints-99"></a></h4>
<ul><li>(C1) <code>type(lhs) = type(rhs) = type(result)</code>.</li>
</ul></div>
<div class="section level4">
<h4 id="examples-104">Examples<a class="anchor" aria-label="anchor" href="#examples-104"></a></h4>
<pre class="mlir"><code>// Bitwise operation with with integer tensors
// %lhs: [[1, 2], [3, 4]]
// %rhs: [[5, 6], [7, 8]]
%result = "stablehlo.xor"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;
// %result: [[4, 4], [4, 12]]

// Logical operation with with boolean tensors
// %lhs: [[false, false], [true, true]]
// %rhs: [[false, true], [false, true]]
%result = "stablehlo.xor"(%lhs, %rhs) : (tensor&lt;2x2xi1&gt;, tensor&lt;2x2xi1&gt;) -&gt; tensor&lt;2x2xi1&gt;
// %result: [[false, true], [true, false]]</code></pre>
<p> <a href="https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/xor.mlir" class="external-link">More Examples</a></p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="dialect-interop">Dialect Interop<a class="anchor" aria-label="anchor" href="#dialect-interop"></a></h2>
<p>At the moment, StableHLO programs in the wild sometimes contain operations that are not defined by StableHLO.</p>
<div class="section level3">
<h3 id="module-function-call-and-return">Module, Function, Call and Return<a class="anchor" aria-label="anchor" href="#module-function-call-and-return"></a></h3>
<p>StableHLO uses upstream MLIR operations for ModuleOp, FuncOp, CallOp, and ReturnOp. This was done for better interop with existing MLIR machinery, as many useful passes are written targeting FuncOp and ModuleOp, and many compilation pipelines expect these ops to be present. Full compatibility guarantees are applied to these ops. If anything ever changes about these ops in an incompatible way (i.e. removal), StableHLO equivalents will be added to preserve compatibility.</p>
</div>
<div class="section level3">
<h3 id="chlo">CHLO<a class="anchor" aria-label="anchor" href="#chlo"></a></h3>
<p>The CHLO opset contains higher level operations that decompose to StableHLO. Currently there are no compatibility guarantees for CHLO. For compatibility guarantees, the <a href="https://github.com/openxla/stablehlo/blob/12fd0a9e7b3c6f3dea3defc513870c962e62726d/stablehlo/transforms/Passes.td#L119" class="external-link">chlo-legalize-to-stablehlo pass</a> must be used prior to serialization.</p>
</div>
<div class="section level3">
<h3 id="shape-operations">Shape Operations<a class="anchor" aria-label="anchor" href="#shape-operations"></a></h3>
<p>It is a common use case in the community to use certain operations from core MLIR dialects in dynamic StableHLO programs to perform shape computations. Most commonly, these include <a href="https://mlir.llvm.org/docs/Dialects/ShapeDialect/" class="external-link"><code>shape</code> dialect</a> ops like <code>shape_of</code> or <code>num_elements</code>, <a href="https://mlir.llvm.org/docs/Dialects/TensorOps/" class="external-link"><code>tensor</code> dialect</a> ops like <code>dim</code> or <code>from_elements</code>, and the builtin <code>index</code> type.</p>
<p>The <a href="https://github.com/openxla/stablehlo/blob/main/rfcs/20230704-dynamism-101.md#o2" class="external-link">Dynamism RFC &gt; O2</a> denotes these as out of scope, however some support for <code>index</code> types is included for interop purposes. There are no compatibility guarantees for these ops or types. The <a href="https://github.com/openxla/stablehlo/blob/12fd0a9e7b3c6f3dea3defc513870c962e62726d/stablehlo/transforms/Passes.td#L136" class="external-link">shape-legalize-to-stablehlo</a> pass can be used to convert these operations to fully supported StableHLO ops.</p>
</div>
</div>
<div class="section level2">
<h2 id="deprecated-operations">Deprecated Operations<a class="anchor" aria-label="anchor" href="#deprecated-operations"></a></h2>
<p>There are several StableHLO operations that were inherited from <a href="https://github.com/openxla/xla/blob/d63deb9250b9c212445290bd08c6effb5b6d0a2b/xla/mlir_hlo/mhlo/IR/hlo_ops.td" class="external-link">MHLO</a> which are deprecated and on the way out of StableHLO. The full details on these removals can be found in the <a href="https://github.com/openxla/stablehlo/pull/2283" class="external-link">StableHLO v1.0 Cleanup #2283</a>. The tracker issue for these deprecations is <a href="https://github.com/openxla/stablehlo/issues/2340" class="external-link">#2340</a>.</p>
<p>These operations fall into a few categories:</p>
<ul><li>“Not in HLO” category of StableHLO operations - they were initially part of the StableHLO opset but have been later deemed to not fit it well: <code>broadcast</code>, <code>create_token</code>, <code>cross-replica-sum</code>, <code>dot</code>, <code>einsum</code>, <code>torch_index_select</code>, <code>unary_einsum</code> (<a href="https://github.com/openxla/stablehlo/issues/3" class="external-link">#3</a>).</li>
<li>Unused ops - These operations may have been useful at some point, but the ops were either underdeveloped, or the pipelines using these ops have been refactored to not require them anymore. This includes <code>map</code>, <code>tuple</code> (<a href="https://github.com/openxla/stablehlo/issues/598" class="external-link">#598</a>), <code>get_tuple_element</code>, <code>rng</code>, <code>complex</code> comparisons <a href="https://github.com/openxla/stablehlo/issues/560" class="external-link">#560</a>, and convolution <code>window_reversal</code> (<a href="https://github.com/openxla/stablehlo/issues/1181" class="external-link">#1181</a>).</li>
</ul><p>Some of these ops can be removed easily given that they can be expressed using existing ops (<code>broadcast</code>, <code>create_token</code>, <code>cross-replica-sum</code>, <code>dot</code>, <code>unary_einsum</code>) and will be removed after the existing compatibilty window passes (6 months). Others are still being explored for removal (<code>einsum</code>, <code>get_tuple_element</code>, <code>map</code>, <code>rng</code> <code>torch_index_select</code>, <code>tuple</code>, <code>complex</code> comparisons, <code>window_reversal</code>). Pending community feedback, these ops will either be removed, or added to the spec with full support. Until these ops futures are known, they are only guaranteed 6 months of compatibility.</p>
</div>
<div class="section level2">
<h2 id="execution">Execution<a class="anchor" aria-label="anchor" href="#execution"></a></h2>
<div class="section level3">
<h3 id="sequential-execution">Sequential execution<a class="anchor" aria-label="anchor" href="#sequential-execution"></a></h3>
<p>A StableHLO program is executed by providing input values to the <code>main</code> function and computing output values. Output values of a function are computed by executing the graph of ops rooted in the corresponding <code>return</code> op.</p>
<p>The execution order is implementation-defined as long as it is aligned with dataflow, i.e. if ops are executed before their uses. In StableHLO, all side-effecting ops consume one token and produce one token (multiple tokens can be multiplexed into one token via <code>after_all</code>), so the execution order of side effects is also aligned with dataflow. For example, in the below program there are two possible execution orders: <code>%0</code> → <code>%1</code> → <code>%2</code> → <code>return</code> and <code>%1</code> → <code>%0</code> → <code>%2</code> → <code>return</code>.</p>
<pre class="mlir"><code>func.func @main() -&gt; tensor&lt;f64&gt; {
  %0 = stablehlo.constant dense&lt;1.0&gt; : tensor&lt;f64&gt;
  %1 = stablehlo.constant dense&lt;2.0&gt; : tensor&lt;f64&gt;
  %2 = stablehlo.add %0, %1 : tensor&lt;f64&gt;
  return %2 : tensor&lt;f64&gt;
}</code></pre>
<p>More formally, a <strong>StableHLO process</strong> is a combination of: 1) a StableHLO program, 2) operation statuses (not executed yet, already executed), and 3) intermediate values that the process is working on. The process starts with input values to the <code>main</code> function, progresses through the graph of ops updating operation statuses and intermediate values and finishes with output values. Further formalization is TBD (<a href="https://github.com/openxla/stablehlo/issues/484" class="external-link">#484</a>).</p>
</div>
<div class="section level3">
<h3 id="parallel-execution">Parallel execution<a class="anchor" aria-label="anchor" href="#parallel-execution"></a></h3>
<p>StableHLO programs can be executed in parallel, organized into a 2D process grid of <code>num_replicas</code> by <code>num_partitions</code> which both have type <code>ui32</code>.</p>
<p>In the <strong>StableHLO process grid</strong>, <code>num_replicas * num_partitions</code> of StableHLO processes are executing at the same time. Each process has a unique <code>process_id = (replica_id, partition_id)</code>, where <code>replica_id</code> in <code>replica_ids = range(num_replicas)</code> and <code>partition_id</code> in <code>partition_ids = range(num_partitions)</code> which both have type <code>ui32</code>.</p>
<p>The size of the process grid is known statically for every program (in the future, we are planning to make it an explicit part of StableHLO programs <a href="https://github.com/openxla/stablehlo/issues/650" class="external-link">#650</a>), and the position within the process grid is known statically for every process. Each process has access to its position within the process grid via the <code>replica_id</code> and <code>partition_id</code> ops.</p>
<p>Within the process grid, the programs can all be the same (in the “Single Program, Multiple Data” style), can all be different (in the “Multiple Program, Multiple Data” style) or something in between. In the future, we are planning to introduce support for other idioms of defining parallel StableHLO programs, including GSPMD (<a href="https://github.com/openxla/stablehlo/issues/619" class="external-link">#619</a>).</p>
<p>Within the process grid, the processes are mostly independent from each other - they have separate operation statuses, separate input/intermediate/output values and most of the ops are executed separately between processes, with the exception of a small number of collective ops described below.</p>
<p>Given that execution of most of the ops is only using values from the same process, it is usually unambiguous to refer to these values by their names. However, when describing semantics of collective ops, that is insufficient, and that gives rise to the notation <code>name@process_id</code> to refer to the value <code>name</code> within a particular process. (From that perspective, unqualified <code>name</code> can be viewed as a shorthand for <code>name@(replica_id(), partition_id())</code>).</p>
<p>The execution order across processes is implementation-defined, except for the synchronization introduced by point-to-point communication and collective ops as described below.</p>
</div>
<div class="section level3">
<h3 id="point-to-point-communication">Point-to-point communication<a class="anchor" aria-label="anchor" href="#point-to-point-communication"></a></h3>
<p>StableHLO processes can communicate with each other through <strong>StableHLO channels</strong>. A channel is represented by a positive id of type <code>si64</code>. Through various ops, it is possible to send values to channels and receive them from channels.</p>
<p>Further formalization, e.g. where these channel ids are coming from, how processes programs become aware of them and what kind of synchronization is introduced by them, is TBD (<a href="https://github.com/openxla/stablehlo/issues/484" class="external-link">#484</a>).</p>
</div>
<div class="section level3">
<h3 id="streaming-communication">Streaming communication<a class="anchor" aria-label="anchor" href="#streaming-communication"></a></h3>
<p>Every StableHLO process has access to two streaming interfaces:</p>
<ul><li>
<strong>Infeed</strong> that can be read from.</li>
<li>
<strong>Outfeed</strong> that can be written to.</li>
</ul><p>Unlike channels, which are used to communicate between processes and therefore have processes at both of their ends, infeeds and outfeeds have their other end implementation-defined.</p>
<p>Further formalization, e.g. how streaming communication influences execution order and what kind of synchronization is introduced by it, is TBD (<a href="https://github.com/openxla/stablehlo/issues/484" class="external-link">#484</a>).</p>
</div>
<div class="section level3">
<h3 id="collective-ops">Collective ops<a class="anchor" aria-label="anchor" href="#collective-ops"></a></h3>
<p>There are six collective ops in StableHLO: <code>all_gather</code>, <code>all_reduce</code>, <code>all_to_all</code>, <code>collective_broadcast</code>, <code>collective_permute</code>, and <code>reduce_scatter</code>. All these ops split the processes in the StableHLO process grid into <strong>StableHLO process groups</strong> and execute a joint computation within each process group, independently from other process groups.</p>
<p>Within each process group, collective ops may introduce a synchronization barrier. Further formalization, e.g. elaborating on when exactly this synchronization happens, how exactly the processes arrive at this barrier, and what happens if they don’t, is TBD (<a href="https://github.com/openxla/stablehlo/issues/484" class="external-link">#484</a>).</p>
<p>If the process group involves cross-partition communication, i.e. there are processes in the process group whose partition ids are different, then execution of the collective op needs a channel, and the collective op must provide a positive <code>channel_id</code> of type <code>si64</code>. Cross-replica communication doesn’t need channels.</p>
<p>The computations performed by the collective ops are specific to individual ops and are described in individual op sections above. However, the strategies by which the process grid is split into process groups are shared between these ops and are described in this section. More formally, StableHLO supports the following four strategies.</p>
<div class="section level4">
<h4 id="cross_replica">cross_replica<a class="anchor" aria-label="anchor" href="#cross_replica"></a></h4>
<p>Only cross-replica communications happen within each process group. This strategy takes <code>replica_groups</code> - a list of lists of replica ids - and computes a Cartesian product of <code>replica_groups</code> by <code>partition_ids</code>. <code>replica_groups</code> must have unique elements and cover all <code>replica_ids</code>. More formally, using Python syntax:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" tabindex="-1"></a><span class="kw">def</span> cross_replica(replica_groups: List[List[ReplicaId]]) <span class="op">-&gt;</span> List[List[ProcessId]]:</span>
<span id="cb141-2"><a href="#cb141-2" tabindex="-1"></a>  <span class="cf">for</span> replica_group <span class="kw">in</span> replica_groups:</span>
<span id="cb141-3"><a href="#cb141-3" tabindex="-1"></a>    <span class="cf">for</span> partition_id <span class="kw">in</span> partition_ids:</span>
<span id="cb141-4"><a href="#cb141-4" tabindex="-1"></a>      process_group <span class="op">=</span> []</span>
<span id="cb141-5"><a href="#cb141-5" tabindex="-1"></a>      <span class="cf">for</span> replica_id <span class="kw">in</span> replica_group:</span>
<span id="cb141-6"><a href="#cb141-6" tabindex="-1"></a>        process_group.append((replica_id, partition_id))</span>
<span id="cb141-7"><a href="#cb141-7" tabindex="-1"></a>      <span class="cf">yield</span> process_group</span></code></pre></div>
<p>For example, for <code>replica_groups = [[0, 1], [2, 3]]</code> and <code>num_partitions = 2</code>, <code>cross_replica</code> will produce <code>[[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(2, 0), (3, 0)], [(2, 1), (3, 1)]]</code>.</p>
</div>
<div class="section level4">
<h4 id="cross_partition">cross_partition<a class="anchor" aria-label="anchor" href="#cross_partition"></a></h4>
<p>Only cross-partition communications happen within each process group. This strategy takes <code>partition_groups</code> - a list of lists of partition ids - and computes a Cartesian product of <code>partition_groups</code> by <code>replica_ids</code>. <code>partition_groups</code> must have unique elements and cover all <code>partition_ids</code>. More formally, using Python syntax:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" tabindex="-1"></a><span class="kw">def</span> cross_partition(partition_groups: List[List[PartitionId]]) <span class="op">-&gt;</span> List[List[ProcessId]]:</span>
<span id="cb142-2"><a href="#cb142-2" tabindex="-1"></a>  <span class="cf">for</span> partition_group <span class="kw">in</span> partition_groups:</span>
<span id="cb142-3"><a href="#cb142-3" tabindex="-1"></a>    <span class="cf">for</span> replica_id <span class="kw">in</span> replica_ids:</span>
<span id="cb142-4"><a href="#cb142-4" tabindex="-1"></a>      process_group <span class="op">=</span> []</span>
<span id="cb142-5"><a href="#cb142-5" tabindex="-1"></a>      <span class="cf">for</span> partition_id <span class="kw">in</span> partition_group:</span>
<span id="cb142-6"><a href="#cb142-6" tabindex="-1"></a>        process_group.append((replica_id, partition_id))</span>
<span id="cb142-7"><a href="#cb142-7" tabindex="-1"></a>      <span class="cf">yield</span> process_group</span></code></pre></div>
<p>For example, for <code>partition_groups = [[0, 1]]</code> and <code>num_replicas = 4</code>, <code>cross_partition</code> will produce <code>[[(0, 0), (0, 1)], [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)]]</code>.</p>
</div>
<div class="section level4">
<h4 id="cross_replica_and_partition">cross_replica_and_partition<a class="anchor" aria-label="anchor" href="#cross_replica_and_partition"></a></h4>
<p>Both cross-replica and cross-partition communications may happen within each process group. This strategy takes <code>replica_groups</code> - a list of lists of replica ids - and computes Cartesian products of each <code>replica_group</code> by <code>partition_ids</code>. <code>replica_groups</code> must have unique elements and cover all <code>replica_ids</code>. More formally, using Python syntax:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" tabindex="-1"></a><span class="kw">def</span> cross_replica_and_partition(replica_groups: List[List[ReplicaId]]) <span class="op">-&gt;</span> List[List[ProcessId]]:</span>
<span id="cb143-2"><a href="#cb143-2" tabindex="-1"></a>  <span class="cf">for</span> replica_group <span class="kw">in</span> replica_groups:</span>
<span id="cb143-3"><a href="#cb143-3" tabindex="-1"></a>    process_group <span class="op">=</span> []</span>
<span id="cb143-4"><a href="#cb143-4" tabindex="-1"></a>    <span class="cf">for</span> partition_id <span class="kw">in</span> partition_ids:</span>
<span id="cb143-5"><a href="#cb143-5" tabindex="-1"></a>      <span class="cf">for</span> replica_id <span class="kw">in</span> replica_group:</span>
<span id="cb143-6"><a href="#cb143-6" tabindex="-1"></a>        process_group.append((replica_id, partition_id))</span>
<span id="cb143-7"><a href="#cb143-7" tabindex="-1"></a>    <span class="cf">yield</span> process_group</span></code></pre></div>
<p>For example, for <code>replica_groups = [[0, 1], [2, 3]]</code> and <code>num_partitions = 2</code>, <code>cross_replica_and_partition</code> will produce <code>[[(0, 0), (1, 0), (0, 1), (1, 1)], [(2, 0), (3, 0), (2, 1), (3, 1)]]</code>.</p>
</div>
<div class="section level4">
<h4 id="flattened_ids">flattened_ids<a class="anchor" aria-label="anchor" href="#flattened_ids"></a></h4>
<p>This strategy takes <code>flattened_id_groups</code> - a list of lists of “flattened” process ids in the form of <code>replica_id * num_partitions + partition_id</code> - and turns them into process ids. <code>flattened_id_groups</code> must have unique elements and cover all <code>process_ids</code>. More formally, using Python syntax:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" tabindex="-1"></a><span class="kw">def</span> flattened_ids(flattened_id_groups: List[List[ui32]]) <span class="op">-&gt;</span> List[List[ProcessId]]:</span>
<span id="cb144-2"><a href="#cb144-2" tabindex="-1"></a>  <span class="cf">for</span> flattened_id_group <span class="kw">in</span> flattened_id_groups:</span>
<span id="cb144-3"><a href="#cb144-3" tabindex="-1"></a>    process_group <span class="op">=</span> []</span>
<span id="cb144-4"><a href="#cb144-4" tabindex="-1"></a>    <span class="cf">for</span> flattened_id <span class="kw">in</span> flattened_id_group:</span>
<span id="cb144-5"><a href="#cb144-5" tabindex="-1"></a>      replica_id <span class="op">=</span> flattened_id <span class="op">//</span> num_partitions</span>
<span id="cb144-6"><a href="#cb144-6" tabindex="-1"></a>      partition_id <span class="op">=</span> flattened_id <span class="op">%</span> num_partitions</span>
<span id="cb144-7"><a href="#cb144-7" tabindex="-1"></a>      process_group.append((replica_id, partition_id))</span>
<span id="cb144-8"><a href="#cb144-8" tabindex="-1"></a>    <span class="cf">yield</span> process_group</span></code></pre></div>
<p>For example, for <code>flattened_id_groups = [[0, 1, 2, 3], [4, 5, 6, 7]]</code>, <code>num_replicas = 4</code> and <code>num_partitions = 2</code>, <code>flattened_ids</code> will produce <code>[[(0, 0), (0, 1), (1, 0), (1, 1)], [(2, 0), (2, 1), (3, 0), (3, 1)]]</code>.</p>
</div>
</div>
<div class="section level3">
<h3 id="accuracy">Accuracy<a class="anchor" aria-label="anchor" href="#accuracy"></a></h3>
<p>At the moment, StableHLO does not provide guarantees about numerical accuracy, but this may change in the future (<a href="https://github.com/openxla/stablehlo/issues/1156" class="external-link">#1156</a>).</p>
</div>
<div class="section level3">
<h3 id="execution-semantics-of-quantized-operation">Execution semantics of quantized operation<a class="anchor" aria-label="anchor" href="#execution-semantics-of-quantized-operation"></a></h3>
<p>The interpretation of quantized StableHLO operations may vary depending on the hardware requirements and capabilities. For instance, some hardware may opt to interpret quantized operations using a “dequantize, perform floating-point operation, and finally quantize” strategy. Others may perform the entire computation with integer arithmetic. Consequently, the interpretation of quantized StableHLO operations is exclusively determined by the specific implementation. The interpretation of hybrid quantization (<a href="https://github.com/openxla/stablehlo/issues/1575" class="external-link">#1575</a>) should be based on the it’s semantics as prescribed in the specification (via <a href="https://github.com/openxla/stablehlo/pull/1792" class="external-link">1792</a>).</p>
</div>
<div class="section level3">
<h3 id="errors">Errors<a class="anchor" aria-label="anchor" href="#errors"></a></h3>
<p>StableHLO programs are validated through an extensive set of constraints for individual ops, which rules out many classes of errors prior to run time. However, error conditions are still possible, e.g. through integer overflows, out-of-bounds accesses, etc. Unless explicitly called out, all these errors result in implementation-defined behavior, but this may change in the future (<a href="https://github.com/openxla/stablehlo/issues/1157" class="external-link">#1157</a>).</p>
<div class="section level4">
<h4 id="floating-point-exceptions">Floating-point exceptions<a class="anchor" aria-label="anchor" href="#floating-point-exceptions"></a></h4>
<p>As an exception to this rule, floating-point exceptions in StableHLO programs have well-defined behavior. Operations which result in exceptions defined by the IEEE-754 standard (invalid operation, division-by-zero, overflow, underflow, or inexact exceptions) produce default results (as defined in the standard) and continue execution without raising the corresponding status flag; similar to <code>raiseNoFlag</code> exception handling from the standard. Exceptions for nonstandard operations (e.g. complex arithmetic and certain transcendental functions) are implementation-defined.</p>
</div>
<div class="section level4">
<h4 id="shape-mismatches">Shape mismatches<a class="anchor" aria-label="anchor" href="#shape-mismatches"></a></h4>
<p>StableHLO supports dynamically-shaped tensors. However, shapes have to agree at runtime, otherwise the behavior is undefined. StableHLO does not explicitly provide an op that can assert that a tensor has a given shape at runtime. Generating correct code is the responsibility of the producer.</p>
<p>As a specific example, the below program is valid. However, at runtime, the exact shapes of <code>%arg0</code> and <code>%arg1</code> will have to be the same, otherwise the behavior of the program is undefined:</p>
<pre class="mlir"><code>func.func @foo(%arg0: tensor&lt;?xi32&gt;, %arg1: tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xi32&gt; {
    %0 = stablehlo.add %arg0, %arg1 : tensor&lt;?xi32&gt;
    return %0 : tensor&lt;?xi32&gt;
}</code></pre>
</div>
</div>
</div>
<div class="section level2">
<h2 id="notation">Notation<a class="anchor" aria-label="anchor" href="#notation"></a></h2>
<p>For describing syntax, this document is using the modified ISO flavor of EBNF syntax (<a href="https://www.iso.org/standard/26153.html" class="external-link">ISO/IEC 14977:1996</a>, <a href="https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form" class="external-link">Wikipedia</a>), with two modifications: 1) rules are defined using <code>::=</code> rather than <code>=</code>, 2) concatenation is expressed using juxtaposition rather than <code>,</code>.</p>
<p>For describing semantics (i.e. within “Types”, “Constants” and “Ops” sections), we are using formulas which are based on Python syntax extended with support for concisely expressing array operations as described below. This works well for small snippets of code, but in rare cases when larger snippets of code are needed, we use vanilla Python syntax which is always introduced explicitly.</p>
<div class="section level3">
<h3 id="formulas">Formulas<a class="anchor" aria-label="anchor" href="#formulas"></a></h3>
<p>Let’s explore how formulas work based on an example from the <code>dot_general</code> specification. One of the constraints for this operation looks as follows: <code>dim(lhs, lhs_batching_dimensions...) = dim(rhs, rhs_batching_dimensions...)</code>.</p>
<p>The names used in this formula come from two sources: 1) global functions, i.e. <code>dim</code>, 2) member definitions of the corresponding program element, i.e. <code>lhs</code>, <code>lhs_batching_dimensions</code>, <code>rhs</code> and <code>rhs_batching_dimensions</code> inputs defined in the “Inputs” section of <code>dot_general</code>.</p>
<p>As mentioned above, the syntax of this formula is Python-based with some conciseness-oriented extensions. To make sense of the formula, let’s transform it into vanilla Python syntax.</p>
<ol style="list-style-type: upper-alpha"><li><p>In these formulas, we are using <code>=</code> to represent equality, so the first step towards obtaining Python syntax is replacing <code>=</code> with <code>==</code>, as follows: <code>dim(lhs, lhs_batching_dimensions...) == dim(rhs, rhs_batching_dimensions...)</code>.</p></li>
<li><p>Also, these formulas support ellipses (<code>...</code>) which turn scalar expressions into tensor expressions. In a nutshell, <code>f(xs...)</code> roughly means “for each scalar <code>x</code> in the tensor <code>xs</code>, compute a scalar <code>f(x)</code> and then return all these scalar results together as a tensor result”. In vanilla Python syntax, our example formula turns into: <code>[dim(lhs, dim1) for dim1 in lhs_batching_dimensions] == [dim(rhs, dim2) for dim2 in rhs_batching_dimensions]</code>.</p></li>
</ol><p>Thanks to ellipses, it is often possible to avoid working at the level of individual scalars. However, in some tricky cases, lower-level semi-informal syntax may be used like in the <code>start_indices[bi0, ..., :, ..., biN]</code> formula from the <code>gather</code> specification. In the service of conciseness, we don’t provide an exact formalism for translating such syntax to vanilla Python, in hopes that it is still intuitively understandable on case-by-case basis. Please let us know if some specific formulas look opaque, and we’ll try to improve them.</p>
<p>Also, you will notice that formulas use ellipses to expand all sorts of lists, including tensors, lists of tensors (which e.g. can arise from a variadic number of tensors), etc. This is another area where we don’t provide an exact formalism (e.g. lists are not even part of the StableHLO type system) and instead rely on intuitive understandability.</p>
<ol start="3" style="list-style-type: upper-alpha"><li>The final noteworthy notational vehicle that we employ is implicit broadcasting. While the StableHLO opset doesn’t support implicit broadcasting, the formulas do, also in the service of conciseness. In a nutshell, if a scalar is used in a context where a tensor is expected, the scalar is broadcasted to the expected shape.</li>
</ol><p>To continue the <code>dot_general</code> example, here’s another constraint: <code>0 &lt;= lhs_batching_dimensions &lt; rank(lhs)</code>. As defined in the <code>dot_general</code> specification, <code>lhs_batching_dimensions</code> is a tensor, however both <code>0</code> and <code>rank(lhs)</code> are scalars. After we apply implicit broadcasting, the formula will become <code>[0, ..., 0] &lt;= lhs_batching_dimensions &lt; [rank(lhs), ..., rank(lhs)]</code>.</p>
<p>When applied to a particular <code>dot_general</code> operation, this formula will evaluate to a tensor of booleans. When formulas are used as constraints, the constraint holds if the formula evaluates to either <code>true</code> or to a tensor which only has <code>true</code> elements.</p>
</div>
<div class="section level3">
<h3 id="names">Names<a class="anchor" aria-label="anchor" href="#names"></a></h3>
<p>In formulas, lexical scope includes: 1) global functions, 2) member definitions, 3) local definitions. The list of global functions is provided below. The list of element definitions depends on the program element that the notation is applied to:</p>
<ul><li>For operations, member definitions include names introduced in “Inputs” and “Outputs” sections.</li>
<li>For everything else, member definitions include structural parts of the program element, named after the corresponding EBNF non-terminals. Most of the time, the names of these structural parts are obtained by converting the names of the non-terminals to snake case (e.g. <code>IntegerLiteral</code> =&gt; <code>integer_literal</code>), but sometimes names get abbreviated in the process (e.g. <code>QuantizationStorageType</code> =&gt; <code>storage_type</code>) in which case the names are introduced explicitly similarly to “Inputs” / “Outputs” sections in operation specifications.</li>
<li>Additionally, member definitions always include <code>self</code> to refer to the corresponding program element.</li>
</ul></div>
<div class="section level3">
<h3 id="values">Values<a class="anchor" aria-label="anchor" href="#values"></a></h3>
<p>When formulas are evaluated, they work with the following types of values: 1) <code>Value</code> (actual values, e.g. <code>dense&lt;[[1, 2], [3, 4]]&gt; : tensor&lt;2x2xi32&gt;</code>; they always know their types), 2) <code>Placeholder</code> (future values, e.g. <code>lhs</code>, <code>rhs</code> or <code>result</code>; their actual values are not known yet, only their types are known), 3) <code>Type</code> (types as defined in the “Types” section), 4) <code>Function</code> (global functions as defined in the “Functions” section).</p>
<p>Depending on the context, names may be referring to different values. More specifically, the “Semantics” section for ops (and equivalents for other program elements) defines runtime logic, so all inputs are available as <code>Value</code>. In contrast, the “Constraints” section for ops (and equivalents) defines “compile-time” logic, i.e. something that is typically executed before runtime, so only constant inputs are available as <code>Value</code> and other inputs are available only as <code>Placeholder</code>.</p>
<table class="table"><colgroup><col width="28%"><col width="36%"><col width="36%"></colgroup><thead><tr class="header"><th>Names</th>
<th>In “Semantics”</th>
<th>In “Constraints”</th>
</tr></thead><tbody><tr class="odd"><td>Global functions</td>
<td><code>Function</code></td>
<td><code>Function</code></td>
</tr><tr class="even"><td>Constant inputs</td>
<td><code>Value</code></td>
<td><code>Value</code></td>
</tr><tr class="odd"><td>Non-constant inputs</td>
<td><code>Value</code></td>
<td><code>Placeholder</code></td>
</tr><tr class="even"><td>Outputs</td>
<td><code>Value</code></td>
<td><code>Placeholder</code></td>
</tr><tr class="odd"><td>Local definitions</td>
<td>Depends on the definition</td>
<td>Depends on the definition</td>
</tr></tbody></table><p>Let’s consider an example <code>transpose</code> operation:</p>
<pre class="mlir"><code>%result = "stablehlo.transpose"(%operand) {
  permutation = dense&lt;[2, 1, 0]&gt; : tensor&lt;3xi64&gt;
} : (tensor&lt;2x3x2xi32&gt;) -&gt; tensor&lt;2x3x2xi32&gt;</code></pre>
<p>For this operation, <code>permutation</code> is a constant, so it’s available as a <code>Value</code> in both semantics and constraints. In contrast, <code>operand</code> and <code>result</code> are available as a <code>Value</code> in semantics but only as a <code>Placeholder</code> in constraints.</p>
</div>
<div class="section level3">
<h3 id="functions-1">Functions<a class="anchor" aria-label="anchor" href="#functions-1"></a></h3>
<div class="section level4">
<h4 id="construction-of-types">Construction of types<a class="anchor" aria-label="anchor" href="#construction-of-types"></a></h4>
<p>There are no functions that can be used to construct types. Instead, we directly use type syntax because it’s typically more concise. E.g. <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; (tensor&lt;E&gt;)</code> rather than <code>function_type( [tensor_type([], E), tensor_type([], E)], [tensor_type([], E)])</code>.</p>
</div>
<div class="section level4">
<h4 id="functions-on-types">Functions on types<a class="anchor" aria-label="anchor" href="#functions-on-types"></a></h4>
<ul><li>
<code>element_type</code> is defined on tensor types and quantized tensor types and returns, respectively, the <code>TensorElementType</code> or <code>QuantizedTensorElementType</code> part of the corresponding <code>TensorType</code> or <code>QuantizedTensorType</code>.</li>
</ul><div class="sourceCode" id="cb147"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" tabindex="-1"></a><span class="kw">def</span> element_type(x: Value <span class="op">|</span> Placeholder <span class="op">|</span> Type):</span>
<span id="cb147-2"><a href="#cb147-2" tabindex="-1"></a> <span class="cf">if</span> <span class="bu">type</span>(x) <span class="op">==</span> TensorType:</span>
<span id="cb147-3"><a href="#cb147-3" tabindex="-1"></a>    <span class="cf">return</span> tensor_element_type(x)</span>
<span id="cb147-4"><a href="#cb147-4" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(x) <span class="op">==</span> QuantizedTensorType:</span>
<span id="cb147-5"><a href="#cb147-5" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor_element_type(x)</span>
<span id="cb147-6"><a href="#cb147-6" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(x) <span class="kw">is</span> <span class="kw">not</span> Type:</span>
<span id="cb147-7"><a href="#cb147-7" tabindex="-1"></a>    <span class="cf">return</span> element_type(<span class="bu">type</span>(x))</span></code></pre></div>
<ul><li><p><code>is_per_axis_quantized(x: Value | Placeholder | Type) -&gt; Value</code> is a shortcut for <code>is_quantized(x) and quantization_dimension(x) is not None</code>.</p></li>
<li><p><code>is_per_tensor_quantized(x: Value | Placeholder | Type) -&gt; Value</code> is a shortcut for <code>is_quantized(x) and quantization_dimension(x) is None</code>.</p></li>
<li><p><code>is_promotable(x: Type, y: Type) -&gt; bool</code> checks if type <code>x</code> can be promoted to type <code>y</code>. When <code>x</code> and <code>y</code> are <code>QuantizedTensorElementType</code>s, the promotion is applied only to the <code>storage_type</code>. This specific version of promotion is currently used in context of reduction computation (refer to <a href="https://github.com/openxla/stablehlo/pull/1664" class="external-link">RFC</a> for more details).</p></li>
</ul><div class="sourceCode" id="cb148"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" tabindex="-1"></a><span class="kw">def</span> is_promotable(x: Type, y: Type) <span class="op">-&gt;</span> Value:</span>
<span id="cb148-2"><a href="#cb148-2" tabindex="-1"></a>  is_same_type <span class="op">=</span> (is_bool(x) <span class="kw">and</span> is_bool(y)) <span class="kw">or</span></span>
<span id="cb148-3"><a href="#cb148-3" tabindex="-1"></a>    (is_integer(x) <span class="kw">and</span> is_integer(y)) <span class="kw">or</span> (is_float(x) <span class="kw">and</span> is_float(y)) <span class="kw">or</span></span>
<span id="cb148-4"><a href="#cb148-4" tabindex="-1"></a>    (is_complex(x) <span class="kw">and</span> is_complex(y)) <span class="kw">or</span></span>
<span id="cb148-5"><a href="#cb148-5" tabindex="-1"></a>    (is_quantized(x) <span class="kw">and</span> is_quantized(y) <span class="kw">and</span> expressed_type(x) <span class="op">=</span> expressed_type(y))</span>
<span id="cb148-6"><a href="#cb148-6" tabindex="-1"></a></span>
<span id="cb148-7"><a href="#cb148-7" tabindex="-1"></a>  <span class="cf">if</span> is_same_type <span class="op">==</span> <span class="va">False</span>:</span>
<span id="cb148-8"><a href="#cb148-8" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb148-9"><a href="#cb148-9" tabindex="-1"></a></span>
<span id="cb148-10"><a href="#cb148-10" tabindex="-1"></a>  <span class="cf">if</span> is_integer(x) <span class="kw">or</span> is_float(x):</span>
<span id="cb148-11"><a href="#cb148-11" tabindex="-1"></a>    <span class="cf">return</span> bitwidth(x) <span class="op">&lt;=</span> bitwidth(y)</span>
<span id="cb148-12"><a href="#cb148-12" tabindex="-1"></a></span>
<span id="cb148-13"><a href="#cb148-13" tabindex="-1"></a>  <span class="cf">if</span> is_complex(x):</span>
<span id="cb148-14"><a href="#cb148-14" tabindex="-1"></a>    <span class="cf">return</span> bitwidth(element_type(x)) <span class="op">&lt;=</span> bitwidth(element_type(y))</span>
<span id="cb148-15"><a href="#cb148-15" tabindex="-1"></a></span>
<span id="cb148-16"><a href="#cb148-16" tabindex="-1"></a>  <span class="cf">if</span> is_quantized(x):</span>
<span id="cb148-17"><a href="#cb148-17" tabindex="-1"></a>    <span class="cf">return</span> bitwidth(storage_type(x)) <span class="op">&lt;=</span> bitwidth(storage_type(y))</span>
<span id="cb148-18"><a href="#cb148-18" tabindex="-1"></a></span>
<span id="cb148-19"><a href="#cb148-19" tabindex="-1"></a>  <span class="cf">return</span> false</span></code></pre></div>
<ul><li><p><code>is_quantized(x: Value | Placeholder | Type) -&gt; Value</code> is a shortcut for <code>is_quantized_tensor_element_type(x)</code>.</p></li>
<li><p><code>is_type_name(x: Value | Placeholder | Type) -&gt; Value</code>. Available for all types. For example, <code>is_float(x)</code> returns <code>true</code> if <code>x</code> is a <code>FloatType</code>. If <code>x</code> is a value or placeholder, this function is a shortcut for <code>is_type_name(type(x))</code>.</p></li>
<li><p><code>max_value(x: Type) -&gt; Value</code> returns the maximum value of an <code>TensorElementType</code>. If <code>x</code> is not an <code>TensorElementType</code>, returns <code>None</code>.</p></li>
<li><p><code>min_value(x: Type) -&gt; Value</code> returns the minimum possible value of an <code>TensorElementType</code>. If <code>x</code> is not an <code>TensorElementType</code>, returns <code>None</code>.</p></li>
<li><p><code>member_name(x: Value | Placeholder | Type) -&gt; Any</code>. Available for all member definitions <code>member_name</code> of all types. For example, <code>tensor_element_type(x)</code> returns the <code>TensorElementType</code> part of a corresponding <code>TensorType</code>. If <code>x</code> is a value or placeholder, this function is a shortcut for <code>member_name(type(x))</code>. If <code>x</code> is not a type that has an appropriate member, or a value or a placeholder of such a type, returns <code>None</code>.</p></li>
<li><p><code>is_empty_algorithm(*args: Type)</code> checks if all dot algorithm fields are set to <code>None</code>. This is needed since dot algorithms have implementation defined default behaviors, so specifying a default value would be incorrect.</p></li>
</ul></div>
<div class="section level4">
<h4 id="construction-of-values">Construction of values<a class="anchor" aria-label="anchor" href="#construction-of-values"></a></h4>
<ul><li>
<code>operation_name(*xs: Value | Type) -&gt; Value</code>. Available for all operations. For example, <code>add(lhs, rhs)</code> takes two tensor values <code>lhs</code> and <code>rhs</code> and returns the output of evaluating the <code>add</code> operation with these inputs. For some operations e.g. <code>broadcast_in_dim</code>, types of their outputs are “load-bearing”, i.e. needed to evaluate an operation. In this case, the function takes these types as arguments.</li>
</ul></div>
<div class="section level4">
<h4 id="functions-on-values">Functions on values<a class="anchor" aria-label="anchor" href="#functions-on-values"></a></h4>
<ul><li><p>All Python’s operators and functions are available. E.g. both <a href="https://docs.python.org/3/reference/expressions.html#subscriptions" class="external-link">subscription</a> and <a href="https://docs.python.org/3/reference/expressions.html#slicings" class="external-link">slicing</a> notations from Python are available to index into tensors, quantized tensors and tuples.</p></li>
<li><p><code>to_destination_type(x: Value, destination_type: Type) -&gt; Value</code> is defined on tensors and returns the converted value of <code>x</code> based on the <code>type(x)</code> and <code>destination_type</code> as follows:</p></li>
</ul><div class="sourceCode" id="cb149"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" tabindex="-1"></a><span class="kw">def</span> to_destination_type(x: Value, destination_type: Type) <span class="op">-&gt;</span> Value:</span>
<span id="cb149-2"><a href="#cb149-2" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(x) <span class="op">==</span> destination_type:</span>
<span id="cb149-3"><a href="#cb149-3" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb149-4"><a href="#cb149-4" tabindex="-1"></a></span>
<span id="cb149-5"><a href="#cb149-5" tabindex="-1"></a>  <span class="cf">if</span> is_quantized(destination_type):</span>
<span id="cb149-6"><a href="#cb149-6" tabindex="-1"></a>    <span class="cf">if</span> is_quantized(<span class="bu">type</span>(x)):</span>
<span id="cb149-7"><a href="#cb149-7" tabindex="-1"></a>      <span class="cf">return</span> quantize(x, destination_type)</span>
<span id="cb149-8"><a href="#cb149-8" tabindex="-1"></a>    <span class="cf">assert</span> is_float(<span class="bu">type</span>(x))</span>
<span id="cb149-9"><a href="#cb149-9" tabindex="-1"></a>    <span class="cf">return</span> quantize(x, destination_type)</span>
<span id="cb149-10"><a href="#cb149-10" tabindex="-1"></a></span>
<span id="cb149-11"><a href="#cb149-11" tabindex="-1"></a>  <span class="cf">if</span> is_quantized(<span class="bu">type</span>(x)):</span>
<span id="cb149-12"><a href="#cb149-12" tabindex="-1"></a>    <span class="cf">assert</span> destination_type <span class="op">=</span> expressed_type(<span class="bu">type</span>(x))</span>
<span id="cb149-13"><a href="#cb149-13" tabindex="-1"></a>    <span class="cf">return</span> dequantize(<span class="bu">type</span>(x))</span>
<span id="cb149-14"><a href="#cb149-14" tabindex="-1"></a></span>
<span id="cb149-15"><a href="#cb149-15" tabindex="-1"></a>  <span class="cf">return</span> convert(x, destination_type)</span></code></pre></div>
<p>There is early discussion on merging <code>convert</code>, <code>uniform_quantize</code> and <code>uniform_dequantize</code> operations (<a href="https://github.com/openxla/stablehlo/issues/1576" class="external-link">#1576</a>). After the merge we do not need the above function and can use the operation name for <code>convert</code> instead.</p>
<ul><li><p><code>is_nan(x: Value) -&gt; Value</code> is defined on tensors and returns <code>true</code> if all elements of <code>x</code> are <code>NaN</code> or <code>false</code> otherwise. If <code>x</code> is not a tensor, returns <code>None</code>.</p></li>
<li><p><code>is_sorted(x: Value) -&gt; Value</code> is defined on tensors and returns <code>true</code> if elements of <code>x</code> are sorted in ascending order with respect to the ascending lexicographical order of their indices or <code>false</code> otherwise. If <code>x</code> is not a tensor, returns <code>None</code>.</p></li>
<li><p><code>is_unique(x: Value) -&gt; Value</code> is defined on tensors and returns <code>true</code> if <code>x</code> doesn’t have duplicate elements or <code>false</code> otherwise. If <code>x</code> is not a tensor, returns <code>None</code>.</p></li>
<li><p><code>member_name(x: Value) -&gt; Any</code> is defined for all member definitions <code>member_name</code> of all values. For example, <code>real_part(x)</code> returns the <code>RealPart</code> part of a corresponding <code>ComplexConstant</code>. If <code>x</code> is not a value that has an appropriate member, returns <code>None</code>.</p></li>
<li><p><code>same(x: Value) -&gt; Value</code> is defined on tensors and returns <code>true</code> if elements of <code>x</code> are all equal to each other or <code>false</code> otherwise. If the tensor doesn’t have elements, that counts as “all equal to each other”, i.e. the function returns <code>true</code>. If <code>x</code> is not a tensor, returns <code>None</code>.</p></li>
<li><p><code>split(x: Value, num_results: Value, axis: Value) -&gt; Value</code> is defined on tensors and returns <code>num_results</code> slices of <code>x</code> along the axis <code>axis</code>. If <code>x</code> is not a tensor or <code>dim(x, axis) % num_results != 0</code>, returns <code>None</code>.</p></li>
<li><p><code>is_defined_in_parent_scope(x: Value) -&gt; Value</code> is defined on strings and returns <code>true</code> if <code>x</code> is the name of a function defined in the same scope as the parent function of the relevant op.</p></li>
<li><p><code>is_namespaced_op_name(x: Value) -&gt; Value</code> is defined on strings and returns <code>true</code> if <code>x</code> is a valid op name, that is it respects the following regular expression: <code>[a-zA-Z][a-zA-Z0-9_]*([.][a-zA-Z0-9_$]+)+</code></p></li>
</ul></div>
<div class="section level4">
<h4 id="shape-computations">Shape computations<a class="anchor" aria-label="anchor" href="#shape-computations"></a></h4>
<ul><li><p><code>axes(x: Value | Placeholder | Type) -&gt; Value</code> is a shortcut for <code>range(rank(x))</code>.</p></li>
<li><p><code>dim(x: Value | Placeholder | Type, axis: Value) -&gt; Value</code> is a shortcut for <code>shape(x)[axis]</code>.</p></li>
<li><p><code>dims(x: Value | Placeholder | Type, axes: List) -&gt; List</code> is a shortcut for <code>list(map(lambda axis: dim(x, axis), axes))</code>.</p></li>
<li><p><code>index_space(x: Value | Placeholder | Type) -&gt; Value</code> is defined on tensors and returns <code>size(x)</code> indices for the corresponding <code>TensorType</code> sorted in ascending lexicographical order, i.e. <code>[0, ..., 0]</code>, <code>[0, ..., 1]</code>, …, <code>shape(x) - 1</code>. If <code>x</code> is not a tensor type, a quantized tensor type, or a value or a placeholder of one of these types, returns <code>None</code>.</p></li>
<li><p><code>rank(x: Value | Placeholder | Type) -&gt; Value</code> is a shortcut for <code>size(shape(x))</code>.</p></li>
<li><p><code>shape(x: Value | Placeholder | Type) -&gt; Value</code> is defined in the “Functions on types” section via <code>member_name</code>.</p></li>
<li><p><code>size(x: Value | Placeholder | Type) -&gt; Value</code> is a shortcut for <code>reduce(lambda x, y: x * y, shape(x))</code>.</p></li>
</ul></div>
<div class="section level4">
<h4 id="quantization-computations">Quantization computations<a class="anchor" aria-label="anchor" href="#quantization-computations"></a></h4>
<ul><li><p><code>def baseline_element_type(x: Value | Placeholder | Type) -&gt; Type</code> is a shortcut for <code>element_type(baseline_type(x))</code>.</p></li>
<li><p><code>baseline_type</code> is defined on tensor types and quantized tensor types and transforms them to a “baseline”, i.e. a type with the same shape but with the quantization parameters of the element type reset to default values. This is used as a handy trick to compare both tensor and quantized tensor types uniformly, which is needed quite often. For quantized types, this enables comparing types ignoring the quantization parameters, that is, <code>shape</code>, <code>storage_type</code>, <code>expressed_type</code>, <code>storage_min</code>, <code>storage_max</code>, and <code>quantization_dimension</code> (for per-axis quantized type) must all match, but <code>scales</code> and <code>zero points</code> may differ.</p></li>
</ul><div class="sourceCode" id="cb150"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" tabindex="-1"></a><span class="kw">def</span> baseline_type(x: Value <span class="op">|</span> Placeholder <span class="op">|</span> Type) <span class="op">-&gt;</span> Type:</span>
<span id="cb150-2"><a href="#cb150-2" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(x) <span class="op">==</span> TensorType:</span>
<span id="cb150-3"><a href="#cb150-3" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb150-4"><a href="#cb150-4" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(x) <span class="op">==</span> QuantizedTensorType:</span>
<span id="cb150-5"><a href="#cb150-5" tabindex="-1"></a>    element_type <span class="op">=</span> quantized_tensor_element_type(x)</span>
<span id="cb150-6"><a href="#cb150-6" tabindex="-1"></a>    baseline_element_type <span class="op">=</span> QuantizedTensorElementType(</span>
<span id="cb150-7"><a href="#cb150-7" tabindex="-1"></a>      storage_type <span class="op">=</span> storage_type(element_type),</span>
<span id="cb150-8"><a href="#cb150-8" tabindex="-1"></a>      storage_min <span class="op">=</span> storage_min(element_type),</span>
<span id="cb150-9"><a href="#cb150-9" tabindex="-1"></a>      storage_max <span class="op">=</span> storage_max(element_type),</span>
<span id="cb150-10"><a href="#cb150-10" tabindex="-1"></a>      expressed_type <span class="op">=</span> expressed_type(element_type),</span>
<span id="cb150-11"><a href="#cb150-11" tabindex="-1"></a>      quantization_dimension <span class="op">=</span> quantization_dimension(element_type),</span>
<span id="cb150-12"><a href="#cb150-12" tabindex="-1"></a>      scales <span class="op">=</span> [constant(<span class="fl">1.0</span>, expressed_type(element_type))] <span class="op">*</span> dim(x, quantization_dimension(element_type)),</span>
<span id="cb150-13"><a href="#cb150-13" tabindex="-1"></a>      zero_points <span class="op">=</span> [constant(<span class="dv">0</span>, storage_type(element_type))] <span class="op">*</span> dim(x, quantization_dimension(element_type)))</span>
<span id="cb150-14"><a href="#cb150-14" tabindex="-1"></a>    <span class="cf">return</span> QuantizedTensorType(shape(x), baseline_element_type)</span>
<span id="cb150-15"><a href="#cb150-15" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(x) <span class="kw">is</span> <span class="kw">not</span> Type:</span>
<span id="cb150-16"><a href="#cb150-16" tabindex="-1"></a>    <span class="cf">return</span> baseline_element_type(<span class="bu">type</span>(x))</span></code></pre></div>
<ul><li>
<code>dequantize</code> is defined on quantized tensor types and turns them into floating-point tensor types. This happens via converting quantized elements which represent integer values of the storage type into corresponding floating-point values of the expressed type using the zero point and scale associated with the quantized element type.</li>
</ul><div class="sourceCode" id="cb151"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" tabindex="-1"></a><span class="kw">def</span> compute_zero_points(quantized_type, result_type):</span>
<span id="cb151-2"><a href="#cb151-2" tabindex="-1"></a>  <span class="cf">if</span> is_per_tensor_quantized(quantized_type):</span>
<span id="cb151-3"><a href="#cb151-3" tabindex="-1"></a>    <span class="cf">return</span> broadcast_in_dim(constant(zero_point(quantized_type), storage_type(quantized_type)), [], result_type)</span>
<span id="cb151-4"><a href="#cb151-4" tabindex="-1"></a>  <span class="cf">if</span> is_per_axis_quantized(quantized_type):</span>
<span id="cb151-5"><a href="#cb151-5" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> index_space(result_type):</span>
<span id="cb151-6"><a href="#cb151-6" tabindex="-1"></a>      d <span class="op">=</span> quantization_dimension(quantized_type)</span>
<span id="cb151-7"><a href="#cb151-7" tabindex="-1"></a>      zero_points[i] <span class="op">=</span> zero_points(quantized_type)[i[d]]</span>
<span id="cb151-8"><a href="#cb151-8" tabindex="-1"></a>    <span class="cf">return</span> zero_points</span>
<span id="cb151-9"><a href="#cb151-9" tabindex="-1"></a></span>
<span id="cb151-10"><a href="#cb151-10" tabindex="-1"></a><span class="kw">def</span> compute_scales(quantized_type, result_type):</span>
<span id="cb151-11"><a href="#cb151-11" tabindex="-1"></a>  <span class="cf">if</span> is_per_tensor_quantized(quantized_type):</span>
<span id="cb151-12"><a href="#cb151-12" tabindex="-1"></a>    <span class="cf">return</span> broadcast_in_dim(constant(scale(quantized_type), expressed_type(quantized_type)), [],</span>
<span id="cb151-13"><a href="#cb151-13" tabindex="-1"></a>            <span class="bu">type</span>(result_type))</span>
<span id="cb151-14"><a href="#cb151-14" tabindex="-1"></a>  <span class="cf">if</span> is_per_axis_quantized(quantized_type):</span>
<span id="cb151-15"><a href="#cb151-15" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> index_space(result_type):</span>
<span id="cb151-16"><a href="#cb151-16" tabindex="-1"></a>      d <span class="op">=</span> quantization_dimension(quantized_type)</span>
<span id="cb151-17"><a href="#cb151-17" tabindex="-1"></a>      scales[i] <span class="op">=</span> scales(quantized_type)[i[d]]</span>
<span id="cb151-18"><a href="#cb151-18" tabindex="-1"></a>    <span class="cf">return</span> scales</span>
<span id="cb151-19"><a href="#cb151-19" tabindex="-1"></a></span>
<span id="cb151-20"><a href="#cb151-20" tabindex="-1"></a><span class="kw">def</span> dequantize(x: Value) <span class="op">-&gt;</span> Value:</span>
<span id="cb151-21"><a href="#cb151-21" tabindex="-1"></a>  <span class="cf">assert</span> is_quantized(x)</span>
<span id="cb151-22"><a href="#cb151-22" tabindex="-1"></a>  x_storage <span class="op">=</span> bitcast_convert(x, storage_type(x))</span>
<span id="cb151-23"><a href="#cb151-23" tabindex="-1"></a>  x_storage_sub <span class="op">=</span> x_storage <span class="op">-</span> compute_zero_points(<span class="bu">type</span>(x), <span class="bu">type</span>(x_storage))</span>
<span id="cb151-24"><a href="#cb151-24" tabindex="-1"></a>  x_expressed_sub <span class="op">=</span> convert(x_storage_sub, expressed_type(x))</span>
<span id="cb151-25"><a href="#cb151-25" tabindex="-1"></a>  <span class="cf">return</span> x_expressed_sub <span class="op">*</span> compute_scales(<span class="bu">type</span>(x), <span class="bu">type</span>(x_expressed_sub))</span></code></pre></div>
<ul><li>
<code>quantize</code> is defined on floating-point tensor types and turns them into quantized tensor types. This happens via converting floating-point values of the expressed type into corresponding integer values of the storage type using the zero point and scale associated with the quantized element type.</li>
</ul><div class="sourceCode" id="cb152"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" tabindex="-1"></a><span class="kw">def</span> quantize(x: Value, result_type: Type) <span class="op">-&gt;</span> Value:</span>
<span id="cb152-2"><a href="#cb152-2" tabindex="-1"></a>  <span class="cf">assert</span> is_float(x) <span class="kw">and</span> is_quantized(result_type)</span>
<span id="cb152-3"><a href="#cb152-3" tabindex="-1"></a>  zero_points <span class="op">=</span> compute_zero_points(result_type, TensorType(shape(x), storage_type(result_type)))</span>
<span id="cb152-4"><a href="#cb152-4" tabindex="-1"></a>  converted_zero_points <span class="op">=</span> convert(zero_points, expressed_type(result_type))</span>
<span id="cb152-5"><a href="#cb152-5" tabindex="-1"></a>  converted_min <span class="op">=</span> convert(storage_min(result_type), expressed_type(result_type))</span>
<span id="cb152-6"><a href="#cb152-6" tabindex="-1"></a>  converted_max <span class="op">=</span> convert(storage_max(result_type), expressed_type(result_type))</span>
<span id="cb152-7"><a href="#cb152-7" tabindex="-1"></a></span>
<span id="cb152-8"><a href="#cb152-8" tabindex="-1"></a>  x_scaled <span class="op">=</span> x <span class="op">/</span> compute_scales(result_type, <span class="bu">type</span>(x))</span>
<span id="cb152-9"><a href="#cb152-9" tabindex="-1"></a>  x_scaled_add_zp <span class="op">=</span> x_scaled <span class="op">+</span> converted_zero_points</span>
<span id="cb152-10"><a href="#cb152-10" tabindex="-1"></a>  x_clamped <span class="op">=</span> clamp(converted_min, x_scaled_add_zp, converted_max)</span>
<span id="cb152-11"><a href="#cb152-11" tabindex="-1"></a>  x_rounded <span class="op">=</span> round_nearest_even(x_clamped)</span>
<span id="cb152-12"><a href="#cb152-12" tabindex="-1"></a>  <span class="cf">return</span> convert(x_rounded, result_type)</span></code></pre></div>
<ul><li>
<code>dequantize_op_quantize</code> is used to specify element-wise computations on quantized tensors. It dequantizes, i.e. turns quantized elements into their expressed types, then performs an operation, and then quantizes, i.e. turns the results back into their storage types. At the moment, this function only works for per-tensor quantization. Per-axis quantization is work in progress (<a href="https://github.com/openxla/stablehlo/issues/1574" class="external-link">#1574</a>).</li>
</ul><div class="sourceCode" id="cb153"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" tabindex="-1"></a><span class="kw">def</span> dequantize_op_quantize(op, <span class="op">*</span>inputs_and_output_type):</span>
<span id="cb153-2"><a href="#cb153-2" tabindex="-1"></a>  inputs <span class="op">=</span> inputs_and_output_type[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb153-3"><a href="#cb153-3" tabindex="-1"></a>  output_type <span class="op">=</span> inputs_and_output_type[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb153-4"><a href="#cb153-4" tabindex="-1"></a></span>
<span id="cb153-5"><a href="#cb153-5" tabindex="-1"></a>  float_inputs <span class="op">=</span> <span class="bu">map</span>(dequantize, inputs)</span>
<span id="cb153-6"><a href="#cb153-6" tabindex="-1"></a>  float_result <span class="op">=</span> op(<span class="op">*</span>float_inputs)</span>
<span id="cb153-7"><a href="#cb153-7" tabindex="-1"></a>  <span class="cf">return</span> quantize(float_result, output_type)</span>
<span id="cb153-8"><a href="#cb153-8" tabindex="-1"></a></span>
<span id="cb153-9"><a href="#cb153-9" tabindex="-1"></a><span class="kw">def</span> dequantize_batch_norm_grad_or_training_quantize(op, <span class="op">*</span>inputs_and_output_types):</span>
<span id="cb153-10"><a href="#cb153-10" tabindex="-1"></a>  inputs <span class="op">=</span> inputs_and_output_type[:<span class="op">-</span><span class="dv">3</span>]</span>
<span id="cb153-11"><a href="#cb153-11" tabindex="-1"></a>  float_inputs <span class="op">=</span> <span class="bu">map</span>(dequantize, inputs)</span>
<span id="cb153-12"><a href="#cb153-12" tabindex="-1"></a>  float_results <span class="op">=</span> op(<span class="op">*</span>float_inputs)</span>
<span id="cb153-13"><a href="#cb153-13" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">map</span>(quantize, float_results, inputs_and_output_type[<span class="op">-</span><span class="dv">3</span>:])</span>
<span id="cb153-14"><a href="#cb153-14" tabindex="-1"></a></span>
<span id="cb153-15"><a href="#cb153-15" tabindex="-1"></a><span class="kw">def</span> dequantize_compare(lhs, rhs, comparison_direction):</span>
<span id="cb153-16"><a href="#cb153-16" tabindex="-1"></a>  float_lhs <span class="op">=</span> dequantize(lhs)</span>
<span id="cb153-17"><a href="#cb153-17" tabindex="-1"></a>  float_rhs <span class="op">=</span> dequantize(rhs)</span>
<span id="cb153-18"><a href="#cb153-18" tabindex="-1"></a>  <span class="cf">return</span> compare(float_lhs, float_rhs, comparison_direction, FLOAT)</span>
<span id="cb153-19"><a href="#cb153-19" tabindex="-1"></a></span>
<span id="cb153-20"><a href="#cb153-20" tabindex="-1"></a><span class="kw">def</span> dequantize_select_quantize(pred, on_true, on_false, output_type):</span>
<span id="cb153-21"><a href="#cb153-21" tabindex="-1"></a>  float_on_true <span class="op">=</span> dequantize(on_true)</span>
<span id="cb153-22"><a href="#cb153-22" tabindex="-1"></a>  float_on_false <span class="op">=</span> dequantize(on_false)</span>
<span id="cb153-23"><a href="#cb153-23" tabindex="-1"></a>  float_result <span class="op">=</span> select(pred, float_on_true, float_on_false)</span>
<span id="cb153-24"><a href="#cb153-24" tabindex="-1"></a>  <span class="cf">return</span> quantize(float_result, output_type)</span></code></pre></div>
<ul><li>
<code>hybrid_dequantize_then_op</code> is used to specify weight-only quantization for hybrid op which accepts lhs in floating-point and rhs in quantized types. It dequantizes quantized inputs into their expressed types and performs computation in float. Element type of float lhs tensor and expressed type of quantized rhs tensor should be identical.</li>
</ul><div class="sourceCode" id="cb154"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" tabindex="-1"></a><span class="kw">def</span> hybrid_dequantize_then_op(op, lhs, rhs):</span>
<span id="cb154-2"><a href="#cb154-2" tabindex="-1"></a>  <span class="cf">assert</span>(is_float(lhs) <span class="kw">and</span> is_quantized(rhs) <span class="kw">and</span> element_type(lhs) <span class="op">==</span> expressed_type(rhs))</span>
<span id="cb154-3"><a href="#cb154-3" tabindex="-1"></a>  <span class="cf">return</span> op(lhs, dequantize(rhs))</span></code></pre></div>
</div>
<div class="section level4">
<h4 id="grid-computations">Grid computations<a class="anchor" aria-label="anchor" href="#grid-computations"></a></h4>
<ul><li><p><code>cross_partition(replica_groups: Value) -&gt; Value</code>. See the “cross_replica” section above.</p></li>
<li><p><code>cross_replica(replica_groups: Value) -&gt; Value</code>. See the “cross_replica” section above.</p></li>
<li><p><code>cross_replica_and_partition(replica_groups: Value) -&gt; Value</code>. See the “cross_replica_and_partition” section above.</p></li>
<li><p><code>flattened_ids(replica_groups: Value) -&gt; Value</code>. See the “flattened_ids” section above.</p></li>
</ul></div>
</div>
</div>
<div class="section level2">
<h2 id="dynamism">Dynamism<a class="anchor" aria-label="anchor" href="#dynamism"></a></h2>
<p>StableHLO values can have dynamic dimension sizes, e.g. <code>tensor&lt;?xi64&gt;</code>. However, StableHLO values cannot have a dynamic number of dimensions (unranked dynamism, e.g. <code>tensor&lt;*xi64&gt;</code>). Operands and results are allowed to use dynamic dimension sizes, even if there are constraints on the sizes. Constraints will be verified statically if possible, otherwise they are deferred to runtime and mismatches will result in undefined behavior. See below for examples.</p>
<div class="section level3">
<h3 id="shape-mismatches-for-unary-elementwise-operations">Shape mismatches for unary elementwise operations<a class="anchor" aria-label="anchor" href="#shape-mismatches-for-unary-elementwise-operations"></a></h3>
<p>Consider the following toy program:</p>
<pre class="mlir"><code>func.func @foo(%arg0: tensor&lt;?xf64&gt;) {
  %0 = stablehlo.abs %arg0 : (tensor&lt;?xf64&gt;) -&gt; tensor&lt;2xf64&gt;
  return
}</code></pre>
<p>Such a program is unusual, because it is not common to know the shape of the result but not the shape of the input. Nonetheless, this is a valid StableHLO program. It is not possible to statically validate the <code>abs</code> operation in this program, because the exact shape of the operand is unknown. However, the shapes are certainly compatible, and this can be checked statically: <code>?</code> could turn out to be <code>2</code> at runtime, and there would be no issue. However, <code>?</code> could also turn out to be some other integer, in which case the behavior is undefined.</p>
<p>Note that if a dimension size is dynamic in the result, there cannot be undefined behavior. Indeed, there is no “expected” size, so there cannot be a mismatch.</p>
</div>
<div class="section level3">
<h3 id="shape-mismatches-for-binary-elementwise-operations">Shape mismatches for binary elementwise operations<a class="anchor" aria-label="anchor" href="#shape-mismatches-for-binary-elementwise-operations"></a></h3>
<p>Consider the following toy program:</p>
<pre class="mlir"><code>func.func @foo(%arg0: tensor&lt;?xf64&gt;, %arg1: tensor&lt;?xf64&gt;) {
  %0 = stablehlo.add %arg0, %arg0 : (tensor&lt;?xf64&gt;, tensor&lt;?xf64&gt;) -&gt; tensor&lt;?xf64&gt;
  return
}</code></pre>
<p>When it comes to binary elementwise operations, the shapes of the inputs and the result must agree at runtime. At compile time, static dimensions must be equal, otherwise they merely need to be compatible. If <em>any</em> dimension is dynamic in the inputs, then there could be undefined behavior at runtime, because the dynamic size may not match the corresponding size in the other operand (be it static or dynamic). If all the inputs are static, then whether the result is dynamic or not does not matter: statically known dimensions will be checked statically, and dynamic dimensions do not impose any constraints.</p>
</div>
<div class="section level3">
<h3 id="shape-mismatches-for-ops-that-take-their-output-shape-as-an-operand">Shape mismatches for ops that take their output shape as an operand<a class="anchor" aria-label="anchor" href="#shape-mismatches-for-ops-that-take-their-output-shape-as-an-operand"></a></h3>
<p>Consider the following toy program:</p>
<pre class="mlir"><code>func.func @foo(%arg0: tensor&lt;2xi32&gt;) {
  %0 = stablehlo.dynamic_iota %arg0, dim = 0 : (tensor&lt;2xi32&gt;) -&gt; tensor&lt;3x4xi64&gt;
  return
}</code></pre>
<p>The values in the shape operand at runtime must match the shape of the result, otherwise the behavior is undefined. That is, at runtime <code>%arg0</code> must have a value of <code>dense&lt;[3, 4]&gt; : tensor&lt;2xi32&gt;</code>. If the shape operand is constant, this can be verified statically. If the result shape is fully dynamic, then there cannot be a mismatch.</p>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Falbel, Sebastian Fischer, Nikolai German.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

