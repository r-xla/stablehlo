[{"path":"/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"/CLAUDE.html","id":"package-overview","dir":"","previous_headings":"","what":"Package Overview","title":"CLAUDE.md","text":"stablehlo R package provides R bindings StableHLO, portable computation representation used machine learning. allows creating, manipulating, transforming StableHLO operations R.","code":""},{"path":[]},{"path":"/CLAUDE.html","id":"build-and-install","dir":"","previous_headings":"Development Commands","what":"Build and Install","title":"CLAUDE.md","text":"","code":"# Load the package for development devtools::load_all()  # Install the package devtools::install()  # Build the package (creates tar.gz file) devtools::build()"},{"path":"/CLAUDE.html","id":"testing","dir":"","previous_headings":"Development Commands","what":"Testing","title":"CLAUDE.md","text":"","code":"# Run all tests devtools::test()  # Run a specific test file testthat::test_file(\"tests/testthat/test-constant.R\")"},{"path":"/CLAUDE.html","id":"documentation","dir":"","previous_headings":"Development Commands","what":"Documentation","title":"CLAUDE.md","text":"","code":"# Generate documentation from roxygen comments devtools::document()"},{"path":"/CLAUDE.html","id":"check","dir":"","previous_headings":"Development Commands","what":"Check","title":"CLAUDE.md","text":"","code":"# Run checks for CRAN compliance devtools::check()"},{"path":"/CLAUDE.html","id":"package-structure-and-architecture","dir":"","previous_headings":"","what":"Package Structure and Architecture","title":"CLAUDE.md","text":"package implements representation StableHLO IR (Intermediate Representation) R. key components include: TensorType, TensorElementType, Shape, ValueType Various data types: BooleanType, IntegerType, FloatType, ComplexType OpMnemonic - Enumeration supported operations Op - Base class operations OpInputs, OpOutputs - Handle operation inputs outputs Func, FuncBody, FuncInputs, FuncOutputs ValueId - Identifier values Constant - Representation constant values Shape - Representation tensor shapes repr.R - Representation helpers list_of.R - Generic list type helpers enum.R - Enumeration type support tracer.R - Tracer function execution jit() - Function convert R functions StableHLO code","code":""},{"path":"/CLAUDE.html","id":"development-practices","dir":"","previous_headings":"","what":"Development Practices","title":"CLAUDE.md","text":"Use S7 (object-oriented system) defining types classes. Follow established pattern adding new operations types Add tests tests/testthat/ appropriate snapshots validation Document functions roxygen2 comments","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 stablehlo authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/SPEC.html","id":null,"dir":"","previous_headings":"","what":"StableHLO Specification","title":"StableHLO Specification","text":"StableHLO operation set high-level operations (HLO) machine learning (ML) models. StableHLO works portability layer different ML frameworks ML compilers: ML frameworks produce StableHLO programs compatible ML compilers consume StableHLO programs. goal simplify accelerate ML development creating interoperability various ML frameworks (TensorFlow, JAX PyTorch) ML compilers (XLA IREE). Towards end, document provides specification StableHLO programming language. specification contains three major sections. First, Programs section describes structure StableHLO programs consist StableHLO functions consist StableHLO ops. Within structure, Ops section specifies semantics individual ops. Execution section provides semantics ops executing together within program. Finally, Notation section discusses notation used throughout specification. view spec previous release StableHLO, open repo tagged release interest. example, StableHLO v0.19.0 Spec. view changes occurred minor version bump StableHLO, refer version log VhloDialect.td.","code":""},{"path":"/SPEC.html","id":"programs","dir":"","previous_headings":"","what":"Programs","title":"StableHLO Specification","text":"StableHLO programs consist arbitrary number StableHLO functions. example program function @main 3 inputs (%image, %weights %bias) 1 output. body function 6 ops.","code":"Program ::= {Func} func.func @main(   %image: tensor<28x28xf32>,   %weights: tensor<784x10xf32>,   %bias: tensor<1x10xf32> ) -> tensor<1x10xf32> {   %0 = \"stablehlo.reshape\"(%image) : (tensor<28x28xf32>) -> tensor<1x784xf32>   %1 = \"stablehlo.dot\"(%0, %weights) : (tensor<1x784xf32>, tensor<784x10xf32>) -> tensor<1x10xf32>   %2 = \"stablehlo.add\"(%1, %bias) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>   %3 = \"stablehlo.constant\"() {value = dense<0.0> : tensor<1x10xf32>} : () -> tensor<1x10xf32>   %4 = \"stablehlo.maximum\"(%2, %3) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>   \"func.return\"(%4): (tensor<1x10xf32>) -> () }"},{"path":"/SPEC.html","id":"functions","dir":"","previous_headings":"Programs","what":"Functions","title":"StableHLO Specification","text":"StableHLO functions (also called named functions) identifier, inputs/outputs body. future, planning introduce additional metadata functions achieve better compatibility HLO (#425, #626, #740, #744).","code":"Func        ::= 'func' '.' 'func' FuncId FuncInputs FuncOutputs '{' FuncBody '}' FuncInputs  ::= '(' [FuncInput {',' FuncInput}] `)` FuncInput   ::= ValueId ':' ValueType FuncOutputs ::= ['->' FuncOutput, {',' FuncOutput}] FuncOutput  ::= ValueType FuncBody    ::= {Op}"},{"path":"/SPEC.html","id":"identifiers","dir":"","previous_headings":"Programs","what":"Identifiers","title":"StableHLO Specification","text":"StableHLO identifiers similar identifiers many programming languages, two peculiarities: 1) identifiers sigils distinguish different kinds identifiers, 2) value identifiers can completely numeric simplify generation StableHLO programs.","code":"FuncId  ::= '@' letter {letter | digit} ValueId ::= '%' digit {digit}           | '%' letter {letter | digit} letter  ::= 'a' | ... | 'z' | 'A' | ... | 'Z' | '_' digit   ::= '0' | ... | '9'"},{"path":"/SPEC.html","id":"types","dir":"","previous_headings":"Programs","what":"Types","title":"StableHLO Specification","text":"StableHLO types categorized value types (also called first-class types) represent StableHLO values non-value types describe program elements. StableHLO types similar types many programming languages, main peculiarity StableHLO’s domain-specific nature results unusual outcomes (e.g. scalar types value types). Tensor types represent tensors, .e. multidimensional arrays. shape element type, shape represents non-negative unknown dimension sizes ascending order corresponding dimensions (also called axes) numbered 0 R-1. number dimensions R called rank. example, tensor<2x3xf32> tensor type shape 2x3 element type f32. two dimensions (, words, two axes) - 0th dimension 1st dimension - whose sizes 2 3. rank 2. Shapes can partially completely unknown (dynamic), e.g. tensor<?x2xf64> partially unknown tensor<?x?xf64> completely unknown. Dynamic dimension sizes represented using ?. Shapes unranked. future, planning explore extending tensor types beyond dimension sizes element types, example, include layouts (#629) sparsity (#1078). Quantized element types represent integer values storage type range storage_min storage_max (inclusive) correspond floating-point values expressed type. given integer value , corresponding floating-point value f can computed f = (- zero_point) * scale, scale zero_point called quantization parameters. storage_min storage_max optional grammar, default values min_value(storage_type) max_value(storage_type) respectively. Quantized element types following constraints: (C1) type(storage_min) = storage_type. (C2) type(storage_max) = storage_type. (C3) min_value(storage_type) <= storage_min < storage_max <= max_value(storage_type). (C4) type(scales...) = expressed_type. (C5) 0 < scales. (C6) is_finite(scales...). (C7) storage_min <= zero_points <= storage_max. (C8) type(zero_points...) = storage_type. (C9) size(scales) = size(zero_points). (C10) is_empty(quantization_dimension), size(scales) = 1. (C11) 0 <= quantization_dimension. moment, QuantizationScale floating-point constant, strong interest integer-based scales, represented multipliers shifts. planning explore near future (#1404). ongoing discussion semantics QuantizationZeroPoint, including type, values whether can just one potentially multiple zero points quantized tensor type. Based results discussion, specification around zero points may change future (#1405). Another ongoing discussion involves semantics QuantizationStorageMin QuantizationStorageMax determine whether constraints imposed values values quantized tensors (#1406). Finally, planning explore representing unknown scales zero points, similarly planning explore representing unknown dimension sizes (#1407). Quantized tensor types represent tensors quantized elements. tensors exactly regular tensors, except elements quantized element types, instead regular element types. quantized tensors, quantization can per-tensor, meaning, one scale zero_point entire tensor can per-axis, meaning, multiple scales zero_points, one pair per slice particular dimension quantization_dimension. formally, tensor t per-axis quantization, dim(t, quantization_dimension) slices quantization_dimension: t[:, ..., 0, ..., :], t[:, ..., 1, ..., :], etc. elements ith slice use scales[] zero_points[] quantization parameters. Quantized tensor types following constraints: additional constraints. (C12) quantization_dimension < rank(self). (C13) dim(self, quantization_dimension) = size(scales). Token types represent tokens, .e. opaque values produced consumed operations. Tokens used imposing execution order operations described Execution section. Tuple types represent tuples, .e. heterogeneous lists. Tuples legacy feature exists compatibility HLO. HLO, tuples used represent variadic inputs outputs. StableHLO, variadic inputs outputs supported natively, use tuples StableHLO comprehensively represent HLO ABI e.g. T, tuple<T> tuple<tuple<T>> may materially different depending particular implementation. future, planning make changes HLO ABI may allow us remove tuple types StableHLO (#598). Element types represent elements tensor types. Unlike many programming languages, types first class StableHLO. means StableHLO programs directly represent values types (result, idiomatic represent scalar values type T 0-dimensional tensor values type tensor<T>). Boolean type represents boolean values true false. Integer types can either signed (si) unsigned (ui) one supported bit widths (2, 4, 8, 16, 32 64). Signed siN types represent integer values -2^(N-1) 2^(N-1)-1 inclusive, unsigned uiN types represent integer values 0 2^N-1 inclusive. f8E3M4, f8E4M3 f8E5M2 8-bit floating point numbers following IEEE-754 conventions. f8E4M3FN f8E5M2 types corresponding respectively E4M3 E5M2 encodings FP8 format described FP8 Formats Deep Learning. f8E4M3FNUZ f8E5M2FNUZ types corresponding E4M3 E5M2 encodings FP8 formats described 8-bit Numerical Formats Deep Neural Networks. f8E4M3B11FNUZ type corresponding E4M3 encoding FP8 formats described Hybrid 8-bit Floating Point (HFP8) Training Inference Deep Neural Networks. bf16 type corresponding bfloat16 format described BFloat16: secret high performance Cloud TPUs. f16, f32 f64 types corresponding respectively binary16 (“half precision”), binary32 (“single precision”) binary64 (“double precision”) formats described IEEE 754 standard. tf32 type corresponds TensorFloat32 format limited support StableHLO. f4E2M1FN, f6E2M3FN, f6E3M2FN f8E8M0FNU MX (microscaling) types described OCP Microscaling Formats Specification. Complex types represent complex values real part imaginary part element type. Supported complex types complex<f32> (parts type f32) complex<f64> (parts type f64). Function types represent named anonymous functions. input types (list types left-hand side ->) output types (list types right-hand side ->). many programming languages, function types first class, StableHLO. String type represents sequences bytes. Unlike many programming languages, string type first class StableHLO used specify static metadata program elements.","code":"Type         ::= ValueType | NonValueType ValueType    ::= TensorType | QuantizedTensorType | TokenType | TupleType NonValueType ::= TensorElementType | QuantizedTensorElementType | FunctionType | StringType TensorType ::= 'tensor' '<' Shape TensorElementType '>' Shape ::= {DimensionSize 'x'} DimensionSize ::= digit {digit} | '?' QuantizedTensorType ::= 'tensor' '<' Shape QuantizedTensorElementType '>' QuantizedTensorElementType ::= '!quant.uniform' '<'                   QuantizationStorageType                   ['<' QuantizationStorageMin ':' QuantizationStorageMax '>']                   ':' QuantizationExpressedType                   [':' QuantizationDimension]                   ',' QuantizationParameters '>' QuantizationStorageType ::= IntegerType QuantizationStorageMin ::= IntegerLiteral QuantizationStorageMax ::= IntegerLiteral QuantizationExpressedType ::= FloatType QuantizationDimension ::= IntegerLiteral QuantizationParameters ::= QuantizationParameter                          | '{' QuantizationParameter {',' QuantizationParameter} '}' QuantizationParameter ::= QuantizationScale [':' QuantizationZeroPoint] QuantizationScale ::= FloatLiteral QuantizationZeroPoint ::= IntegerLiteral TokenType ::= 'token' TupleType ::= 'tuple' '<' TupleElementTypes '>' TupleElementTypes ::= [ValueType {',' ValueType}] TensorElementType ::= BooleanType | IntegerType | FloatType | ComplexType BooleanType ::= 'i1' IntegerType ::= SignedIntegerType | UnsignedIntegerType SignedIntegerType ::= 'si2' | 'si4' | 'si8' | 'si16' | 'si32' | 'si64' UnsignedIntegerType ::= 'ui2' | 'ui4' | 'ui8' | 'ui16' | 'ui32' | 'ui64' FloatType ::= 'f4E2M1FN' | 'f6E2M3FN' | 'f6E3M2FN' | 'f8E3M4' | 'f8E4M3'             | 'f8E4M3FN' | 'f8E4M3FNUZ' | 'f8E4M3B11FNUZ' | 'f8E5M2'             | 'f8E5M2FNUZ' | 'f8E8M0FNU' | 'bf16' | 'f16' | 'f32' | 'f64' TensorFloat32 ::= 'tf32' ComplexType ::= 'complex' '<' ComplexElementType '>' ComplexElementType ::= 'f32' | 'f64' FunctionType ::= '(' InputTypes ')' '->' '(' OutputTypes ')' InputTypes ::= [ValueType {',' ValueType}] OutputTypes ::= [ValueType {',' ValueType}] StringType ::= 'string'"},{"path":"/SPEC.html","id":"operations","dir":"","previous_headings":"Programs","what":"Operations","title":"StableHLO Specification","text":"StableHLO operations (also called ops) represent closed set high-level operations machine learning models. discussed , StableHLO syntax heavily inspired MLIR, necessarily ergonomic alternative, arguably best fit StableHLO’s goal creating interoperability ML frameworks ML compilers. StableHLO operations (also called ops) name, inputs/outputs signature. name consists stablehlo. prefix mnemonic uniquely identifies one supported ops. See comprehensive list supported ops. Ops consume inputs produce outputs. Inputs categorized input values (computed execution), input functions (provided statically, StableHLO functions first-class values) input attributes (also provided statically). kind inputs outputs consumed produced op depends mnemonic. example, add op consumes 2 input values produces 1 output value. comparison, select_and_scatter op consumes 3 input values, 2 input functions 3 input attributes. Input functions (also called anonymous functions) similar named functions except : 1) don’t identifier (hence name “anonymous”), 2) don’t declare output types (output types inferred return op within function). syntax input functions includes currently unused part (see Unused production ) compatibility MLIR. MLIR, general concept “regions” can multiple “blocks” ops connected together via jump ops. blocks ids correspond Unused production, can distinguished . StableHLO doesn’t jump ops, corresponding part MLIR syntax unused (still ). Input attributes name value one supported constants. primary way specify static metadata program elements. example, concatenate op uses attribute dimension specify dimension along input values concatenated. Similarly, slice op uses multiple attributes like start_indices limit_indices specify bounds used slice input value. moment, StableHLO programs wild sometimes contain attributes described document. future, planning either absorb attributes StableHLO opset prohibit appearing StableHLO programs. meanwhile, list attributes: layout (#629). mhlo.frontend_attributes (#628). mhlo.sharding (#619). output_operand_aliases (#740). Location metadata (#594). Op signature consists types input values (list types left-hand side ->) types output values (list types right-hand side ->). Strictly speaking, input types redundant, output types almost always redundant well (StableHLO ops, output types can inferred inputs). Nonetheless, op signature deliberately part StableHLO syntax compatibility MLIR. example op whose mnemonic select_and_scatter. consumes 3 input values (%operand, %source %init_value), 2 input functions 3 input attributes (window_dimensions, window_strides padding). Note signature op includes types input values (types input functions attributes provided inline).","code":"Op            ::= [OpOutputs] OpName OpInputs ':' OpSignature OpName        ::= '\"' 'stablehlo' '.' OpMnemonic '\"' OpMnemonic    ::= 'abs' | 'add' | ... OpInputs        ::= OpInputValues OpInputFuncs OpInputAttrs OpInputValues   ::= '(' [OpInputValue {',' OpInputValue}] ')' OpInputValue    ::= ValueId OpInputFuncs    ::= ['(' OpInputFunc {',' OpInputFunc} ')'] OpInputAttrs    ::= ['{' OpInputAttr {',' OpInputAttr} '}'] OpOutputs       ::= [OpOutput {',' OpOutput} '='] OpOutput        ::= ValueId OpInputFunc ::= '{' Unused FuncInputs ':' FuncBody '}' Unused      ::= '^' digit {digit}               | '^' letter {letter | digit} OpInputAttr      ::= OpInputAttrName '=' OpInputAttrValue OpInputAttrName  ::= letter {letter | digit} OpInputAttrValue ::= Constant OpSignature ::= '(' [ValueType {',' ValueType}] ')' '->' '(' [ValueType {',' ValueType}] ')' %result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({   ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):     %0 = \"stablehlo.compare\"(%arg0, %arg1) {       comparison_direction = #stablehlo<comparison_direction GE>     } : (tensor<i32>, tensor<i32>) -> tensor<i1>     \"stablehlo.return\"(%0) : (tensor<i1>) -> () }, {   ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):     %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>     \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) {   window_dimensions = dense<[3, 1]> : tensor<2xi64>,   window_strides = dense<[2, 1]> : tensor<2xi64>,   padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<4x2xi32>, tensor<2x2xi32>, tensor<i32>) -> tensor<4x2xi32>"},{"path":"/SPEC.html","id":"constants","dir":"","previous_headings":"Programs","what":"Constants","title":"StableHLO Specification","text":"StableHLO constants literal type together represent StableHLO value. Generally, type part constant syntax, except ’s unambiguous (e.g. boolean constant unambiguously type i1, whereas integer constant can multiple possible types). Boolean constants represent boolean values true false. Boolean constants type i1. Integer constants represent integer values via strings use decimal hexadecimal notation. bases, e.g. binary octal, supported. Integer constants following constraints: (C1) is_wellformed(integer_literal, integer_type). Floating-point constants represent floating-point values via strings use decimal scientific notation. Additionally, hexadecimal notation can used directly specify underlying bits floating-point format corresponding type. Floating-point constants following constraints: (C1) non-hexadecimal notation used, is_wellformed(float_literal, float_type). (C2) hexadecimal notation used, size(hexadecimal_digits) = num_bits(float_type) / 4. Complex constants represent complex values using lists real part (comes first) imaginary part (comes second). example, (1.0, 0.0) : complex<f32> represents 1.0 + 0.0i, (0.0, 1.0) : complex<f32> represents 0.0 + 1.0i. order parts stored memory implementation-defined. Complex constants following constraints: (C1) is_wellformed(real_part, complex_element_type(complex_type)). (C2) is_wellformed(imaginary_part, complex_element_type(complex_type)). Tensor constants represent tensor values using nested lists specified via NumPy notation. example, dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32> represents tensor value following mapping indices elements: {0, 0} => 1, {0, 1} => 2, {0, 2} => 3, {1, 0} => 4, {1, 1} => 5, {1, 2} => 6. order elements stored memory implementation-defined. Tensor constants following constraints: has_syntax(element_literal: Syntax, element_type: Type) =   is_wellformed(element_literal, type). has_syntax(tensor_literal: List, element_type: Type) =   has_syntax(tensor_literal..., element_type). has_shape(element_literal: Syntax, []) = true. has_shape(tensor_literal: List, shape: List) =   size(tensor_literal) = shape[0]   has_shape(tensor_literal..., shape[1:]). otherwise, false. Quantized tensor constants represent quantized tensor values using notation tensor constants, elements specified constants storage type. Quantized tensor constants following constraints: (C1) has_syntax(quantized_tensor_literal, storage_type(quantized_tensor_type)). (C2) has_shape(quantized_tensor_literal, shape(quantized_tensor_type)). String literals consist bytes specified using ASCII characters escape sequences. encoding-agnostic, interpretation bytes implementation-defined. String literals type string.","code":"Constant ::= BooleanConstant            | IntegerConstant            | FloatConstant            | ComplexConstant            | TensorConstant            | QuantizedTensorConstant            | StringConstant            | EnumConstant BooleanConstant ::= BooleanLiteral BooleanLiteral  ::= 'true' | 'false' IntegerConstant   ::= IntegerLiteral ':' IntegerType IntegerLiteral    ::= ['-' | '+'] DecimalDigits                     | ['-' | '+'] '0x' HexadecimalDigits DecimalDigits     ::= decimalDigit {decimalDigit} HexadecimalDigits ::= hexadecimalDigit {hexadecimalDigit} decimalDigit      ::= '0' | ... | '9' hexadecimalDigit  ::= decimalDigit | 'a' | ... | 'f' | 'A' | ... | 'F' FloatConstant  ::= FloatLiteral ':' FloatType FloatLiteral   ::= SignPart IntegerPart FractionalPart ScientificPart                  | '0x' [HexadecimalDigits] SignPart       ::= ['-' | '+'] IntegerPart    ::= DecimalDigits FractionalPart ::= ['.' [DecimalDigits]] ScientificPart ::= [('e' | 'E') ['-' | '+'] DecimalDigits] ComplexConstant ::= ComplexLiteral ':' ComplexType ComplexLiteral  ::= '(' RealPart ',' ImaginaryPart ')' RealPart        ::= FloatLiteral ImaginaryPart   ::= FloatLiteral TensorConstant ::= TensorLiteral ':' TensorType TensorLiteral  ::= 'dense' '<' (DenseLiteral | ElementLiteral) '>' DenseLiteral   ::= DenseDimension | DenseElements DenseDimension ::= '[' [DenseLiteral {',' DenseLiteral}] ']' DenseElements  ::= [ElementLiteral {',' ElementLiteral}] ElementLiteral ::= BooleanLiteral | IntegerLiteral | FloatLiteral | ComplexLiteral QuantizedTensorConstant ::= QuantizedTensorLiteral ':' QuantizedTensorType QuantizedTensorLiteral  ::= 'dense' '<' (DenseLiteral | ElementLiteral) '>' StringConstant  ::= StringLiteral StringLiteral   ::= '\"' {stringCharacter | escapeSequence} '\"' stringCharacter ::= all ASCII characters except '\\00', '\\01', ... '\\1f' and '\"' escapeSequence  ::= '\\' ('\"' | '\\' | 'n' | 't' | (hexadecimalDigit hexadecimalDigit))"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"semantics","dir":"","previous_headings":"Ops > abs","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise abs operation operand tensor produces result tensor. Depending element type, following: signed integers: integer modulus. floats: abs IEEE-754. complex numbers: complex modulus. quantized types: dequantize_op_quantize(abs, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints","dir":"","previous_headings":"Ops > abs","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(result) = shape(operand). complex_element_type(element_type(operand)) is_complex(operand). baseline_element_type(operand) otherwise.","code":""},{"path":"/SPEC.html","id":"examples","dir":"","previous_headings":"Ops > abs","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [-2, 0, 2] %result = \"stablehlo.abs\"(%operand) : (tensor<3xi32>) -> tensor<3xi32> // %result: [2, 0, 2]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-1","dir":"","previous_headings":"Ops > add","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise addition two tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical . integers: integer addition. floats: addition IEEE-754. complex numbers: complex addition. quantized types: dequantize_op_quantize(add, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-1","dir":"","previous_headings":"Ops > add","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result). (C2) is_quantized(lhs) is_quantized(rhs) is_quantized(result). (C3) storage_type(lhs) = storage_type(rhs) = storage_type(result). (C4) expressed_type(lhs) = expressed_type(rhs) = expressed_type(result). (C5) (is_per_axis_quantized(lhs) is_per_axis_quantized(rhs)) =   is_per_axis_quantized(result). (C6) is_per_axis_quantized(lhs), quantization_dimension(lhs) =   quantization_dimension(result). (C7) is_per_axis_quantized(rhs), quantization_dimension(rhs) =   quantization_dimension(result).","code":""},{"path":"/SPEC.html","id":"examples-1","dir":"","previous_headings":"Ops > add","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.add\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[6, 8], [10, 12]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-2","dir":"","previous_headings":"Ops > after_all","what":"Semantics","title":"StableHLO Specification","text":"Ensures operations producing inputs executed operations depend result. Execution operation nothing, exists establish data dependencies result inputs.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"examples-2","dir":"","previous_headings":"Ops > after_all","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input0: !stablehlo.token // %input1: !stablehlo.token %result = \"stablehlo.after_all\"(%input0, %input1) : (!stablehlo.token, !stablehlo.token) -> !stablehlo.token"},{"path":[]},{"path":"/SPEC.html","id":"semantics-3","dir":"","previous_headings":"Ops > all_gather","what":"Semantics","title":"StableHLO Specification","text":"Within process group StableHLO process grid, concatenates values operands tensors process along all_gather_dim produces results tensors. operation splits StableHLO process grid process_groups defined follows: cross_replica(replica_groups) channel_id <= 0 use_global_device_ids = false. cross_replica_and_partition(replica_groups) channel_id > 0 use_global_device_ids = false. flattened_ids(replica_groups) channel_id > 0 use_global_device_ids = true. Afterwards, within process_group: operands...@receiver = [operand@sender sender process_group] receiver process_group. results...@process = concatenate(operands...@process, all_gather_dim) process process_group.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-2","dir":"","previous_headings":"Ops > all_gather","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= all_gather_dim < rank(operands...). (C2) is_unique(replica_groups). num_replicas cross_replica used. num_replicas cross_replica_and_partition used. num_processes flattened_ids used. (C4) 0 <= replica_groups < size(replica_groups). (C5) use_global_device_ids = true, channel_id > 0. dim(results..., all_gather_dim) =   dim(operands..., all_gather_dim) * dim(process_groups, 1).","code":""},{"path":"/SPEC.html","id":"examples-3","dir":"","previous_headings":"Ops > all_gather","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// num_replicas: 2 // num_partitions: 1 // %operand0@(0, 0): [[1, 2], [3, 4]] // %operand0@(1, 0): [[5, 6], [7, 8]] // %operand1@(0, 0): [[11, 12], [13, 14]] // %operand1@(1, 0): [[15, 16], [17, 18]] %result:2 = \"stablehlo.all_gather\"(%operand0, %operand1) {   all_gather_dim = 1 : i64,   replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,   // channel_id = 0   channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>   // use_global_device_ids = false } : (tensor<2x2xi64>, tensor<2x2xi64>) -> (tensor<2x4xi64>, tensor<2x4xi64>) // %result0@(0, 0): [[1, 2, 5, 6], [3, 4, 7, 8]] // %result0@(1, 0): [[1, 2, 5, 6], [3, 4, 7, 8]] // %result1@(0, 0): [[11, 12, 15, 16], [13, 14, 17, 18]] // %result1@(1, 0): [[11, 12, 15, 16], [13, 14, 17, 18]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-4","dir":"","previous_headings":"Ops > all_reduce","what":"Semantics","title":"StableHLO Specification","text":"Within process group StableHLO process grid, applies reduction function computation values operands tensors process produces results tensors. operation splits StableHLO process grid process_groups defined follows: cross_replica(replica_groups) channel_id <= 0 use_global_device_ids = false. cross_replica_and_partition(replica_groups) channel_id > 0 use_global_device_ids = false. flattened_ids(replica_groups) channel_id > 0 use_global_device_ids = true. Afterwards, within process_group: exec(node) = computation(exec(node.left), exec(node.right)). exec(leaf) = leaf.value. schedule implementation-defined binary tree whose -order traversal to_destination_type(operands...@process_group...[result_index],   type(func_inputs(computation)[0])).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-3","dir":"","previous_headings":"Ops > all_reduce","what":"Constraints","title":"StableHLO Specification","text":"(C1) is_unique(replica_groups). num_replicas cross_replica used. num_replicas cross_replica_and_partition used. num_processes flattened_ids used. (C3) 0 <= replica_groups < size(replica_groups). (C4) use_global_device_ids = true, channel_id > 0. (C5) computation type (tensor<E>, tensor<E>) -> (tensor<E>) is_promotable(element_type(operand), E). (C6) shape(results...) = shape(operands...). (C7) element_type(results...) = E.","code":""},{"path":"/SPEC.html","id":"examples-4","dir":"","previous_headings":"Ops > all_reduce","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// num_replicas: 2 // num_partitions: 1 // %operand0@(0, 0): [1, 2, 3, 4] // %operand0@(1, 0): [5, 6, 7, 8] // %operand1@(0, 0): [9, 10, 11, 12] // %operand1@(1, 0): [13, 14, 15, 16] %result:2 = \"stablehlo.all_reduce\"(%operand0, %operand0) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>     \"stablehlo.return\"(%0) : (tensor<i64>) -> () }) {   replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,   // channel_id = 0   channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>   // use_global_device_ids = false } : (tensor<4xi64>, tensor<4xi64>) -> (tensor<4xi64>, tensor<4xi64>) // %result0@(0, 0): [6, 8, 10, 12] // %result0@(1, 0): [6, 8, 10, 12] // %result1@(0, 0): [22, 24, 26, 28] // %result1@(1, 0): [22, 24, 26, 28]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-5","dir":"","previous_headings":"Ops > all_to_all","what":"Semantics","title":"StableHLO Specification","text":"Within process group StableHLO process grid, splits values operands tensors along split_dimension parts, scatters split parts processes, concatenates scattered parts along concat_dimension produces results tensors. operation splits StableHLO process grid process_groups defined follows: cross_replica(replica_groups) channel_id <= 0. cross_partition(replica_groups) channel_id > 0. Afterwards, within process_group: split_parts...@sender = split(operands...@sender, split_count, split_dimension) sender process_group. scattered_parts...@receiver = [split_parts...@sender[receiver_index]   sender process_group] receiver_index = process_group.index(receiver). results...@process = concatenate(scattered_parts...@process, concat_dimension).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-4","dir":"","previous_headings":"Ops > all_to_all","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= split_dimension < rank(operands...). (C2) dim(operands..., split_dimension) % split_count = 0. (C3) 0 <= concat_dimension < rank(operands...). (C4) 0 < split_count. (C5) is_unique(replica_groups). num_replicas cross_replica used. num_partitions cross_partition used. (C7) 0 <= replica_groups < size(replica_groups). (C8) dim(replica_groups, 1) = split_count. dim(results..., split_dimension) =   dim(operands..., split_dimension) / split_count. dim(results..., concat_dimension) =   dim(operands..., concat_dimension) * split_count.","code":""},{"path":"/SPEC.html","id":"examples-5","dir":"","previous_headings":"Ops > all_to_all","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// num_replicas: 2 // num_partitions: 1 // %operand1@(0, 0): [[1, 2, 3, 4], //                    [5, 6, 7, 8]] // %operand1@(1, 0): [[9, 10, 11, 12], //                    [13, 14, 15, 16]] // %operand2@(0, 0): [[17, 18, 19, 20], //                    [21, 22, 23, 24]] // %operand2@(1, 0): [[25, 26, 27, 28], //                    [29, 30, 31, 32]] %result:2 = \"stablehlo.all_to_all\"(%operand1, %operand2) {   split_dimension = 1 : i64,   concat_dimension = 0 : i64,   split_count = 2 : i64,   replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>   // channel_id = 0 } : (tensor<2x4xi64>, tensor<2x4xi64>) -> (tensor<4x2xi64>, tensor<4x2xi64>) // %result#0@(0, 0): [[1, 2], [5, 6], [9, 10], [13, 14]] // %result#0@(1, 0): [[3, 4], [7, 8], [11, 12], [15, 16]] // %result#1@(0, 0): [[17, 18], [21, 22], [25, 26], [29, 30]] // %result#1@(1, 0): [[19, 20], [23, 24], [27, 28], [31, 32]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-6","dir":"","previous_headings":"Ops > and","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise two tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical . integers: bitwise .","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-5","dir":"","previous_headings":"Ops > and","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-6","dir":"","previous_headings":"Ops > and","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.and\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[1, 2], [3, 0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-7","dir":"","previous_headings":"Ops > atan2","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise atan2 operation lhs rhs tensor produces result tensor. Depending element type, following: floats: atan2 IEEE-754. complex numbers: complex atan2. quantized types: dequantize_op_quantize(atan2, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-6","dir":"","previous_headings":"Ops > atan2","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(lhs) = baseline_type(rhs) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-7","dir":"","previous_headings":"Ops > atan2","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [0.0, 1.0, -1.0] // %rhs: [0.0, 0.0, 0.0] %result = \"stablehlo.atan2\"(%lhs, %rhs) : (tensor<3xf64>, tensor<3xf64>) -> tensor<3xf64> // %result: [0.0, 1.57079637, -1.57079637] // [0.0, pi/2, -pi/2]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-8","dir":"","previous_headings":"Ops > batch_norm_grad","what":"Semantics","title":"StableHLO Specification","text":"Computes gradients several inputs batch_norm_training backpropagating grad_output, produces grad_operand, grad_scale grad_offset tensors. formally, operation can expressed decomposition existing StableHLO operations using Python syntax follows: quantized types, performs dequantize_batch_norm_grad_or_training_quantize(lambda operand, scale, mean, variance, grad_output: batch_norm_grad(operand, scale, mean, variance, grad_output, epsilon, feature_index), operand, scale, mean, variance, grad_output, type(grad_operand), type(grad_scale), type(feature_index)).","code":"def compute_sum(operand, feature_index):   (sum,) = reduce(       inputs=[operand],       init_values=[constant(0, element_type(operand))],       dimensions=[i for i in range(rank(operand)) if i != feature_index],       body=lambda x, y: add(x, y))   return sum  def compute_mean(operand, feature_index):   sum = compute_sum(operand, feature_index)   divisor = constant(size(operand) / dim(operand, feature_index),                      element_type(operand))   divisor_bcast = broadcast_in_dim(divisor, [], type(sum))   return divide(sum, divisor_bcast)  def batch_norm_grad(operand, scale, mean, variance, grad_output, epsilon, feature_index):   # Broadcast inputs to type(operand)   scale_bcast = broadcast_in_dim(scale, [feature_index], type(operand))   mean_bcast = broadcast_in_dim(mean, [feature_index], type(operand))   variance_bcast = broadcast_in_dim(variance, [feature_index], type(operand))   epsilon_bcast = broadcast_in_dim(constant(epsilon, element_type(operand)), [],                                    type(operand))    # Perform normalization using the provided `mean` and `variance`   # Intermediate values will be useful for computing gradients   centered_operand = subtract(operand, mean_bcast)   stddev = sqrt(add(variance_bcast, epsilon_bcast))   normalized_operand = divide(centered_operand, stddev)    # Use the implementation from batchnorm_expander.cc in XLA   # Temporary variables have exactly the same names as in the C++ code   elements_per_feature = broadcast_in_dim(       constant(divide(size(operand), dim(operand, feature_index)),                element_type(grad_output)),       [], type(operand))   i1 = multiply(grad_output, elements_per_feature)   i2 = broadcast_in_dim(       compute_sum(grad_output, feature_index), [feature_index], type(operand))   i3 = broadcast_in_dim(       compute_sum(multiply(grad_output, centered_operand), feature_index),       [feature_index], type(operand))   i4 = multiply(i3, centered_operand)   i5 = divide(i4, add(variance_bcast, epsilon_bcast))   i6 = subtract(subtract(i1, i2), i5)    grad_operand =       multiply(divide(divide(scale_bcast, stddev), elements_per_feature), i6)   grad_scale =       compute_sum(multiply(grad_output, normalized_operand), feature_index)   grad_offset = compute_sum(grad_output, feature_index)    return grad_operand, grad_scale, grad_offset"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-7","dir":"","previous_headings":"Ops > batch_norm_grad","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= feature_index < rank(operand). (C2) operand, scale, mean, variance, grad_output, grad_operand, grad_scale grad_offset baseline_element_type. (C3) operand, grad_output grad_operand shape. (C4) scale, mean, variance, grad_scale grad_offset shape. (C5) size(scale) = dim(operand, feature_index).","code":""},{"path":"/SPEC.html","id":"examples-8","dir":"","previous_headings":"Ops > batch_norm_grad","what":"Examples","title":"StableHLO Specification","text":"","code":"// %operand: [ //            [[1.0, 2.0], [3.0, 4.0]], //            [[3.0, 4.0], [1.0, 2.0]] //           ] // %scale: [1.0, 1.0] // %mean: [2.0, 3.0] // %variance: [1.0, 1.0] // %grad_output: [ //                [[0.1, 0.1], [0.1, 0.1]], //                [[0.1, 0.1], [0.1, 0.1]] //               ] %grad_operand, %grad_scale, %grad_offset = \"stablehlo.batch_norm_grad\"(%operand, %scale, %mean, %variance, %grad_output) {   epsilon = 0.0 : f32,   feature_index = 2 : i64 } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>,      tensor<2x2x2xf64>) -> (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>) // %grad_operand: [ //                 [[0.0, 0.0], [0.0, 0.0]], //                 [[0.0, 0.0], [0.0, 0.0]] //                ] // %grad_scale:  [0.0, 0.0] // %grad_offset: [0.4, 0.4]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-9","dir":"","previous_headings":"Ops > batch_norm_inference","what":"Semantics","title":"StableHLO Specification","text":"Normalizes operand tensor across dimensions except feature_index dimension produces result tensor. formally, operation can expressed decomposition existing StableHLO operations using Python syntax follows: quantized types, performs dequantize_op_quantize(lambda operand, scale, offset, mean, variance: batch_norm_inference(operand, scale, offset, mean, variance, epsilon, feature_index), operand, scale, offset, mean, variance, type(result)).","code":"def batch_norm_inference(operand, scale, offset, mean, variance, epsilon, feature_index):   # Broadcast inputs to shape(operand)   scale_bcast = broadcast_in_dim(scale, [feature_index], type(operand))   offset_bcast = broadcast_in_dim(offset, [feature_index], type(operand))   mean_bcast = broadcast_in_dim(mean, [feature_index], type(operand))   variance_bcast = broadcast_in_dim(variance, [feature_index], type(operand))   epsilon_bcast = broadcast_in_dim(constant(epsilon, element_type(operand)), [],                                    type(operand))    # Perform normalization using the provided `mean` and `variance` instead of   # computing them like `batch_norm_training` does.   centered_operand = subtract(operand, mean_bcast)   stddev = sqrt(add(variance_bcast, epsilon_bcast))   normalized_operand = divide(centered_operand, stddev)   return add(multiply(scale_bcast, normalized_operand), offset_bcast)"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-8","dir":"","previous_headings":"Ops > batch_norm_inference","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= feature_index < rank(operand). (C2) operand, scale, offset, mean, variance result baseline_element_type. (C3) size(scale) = dim(operand, feature_index). (C4) size(offset) = dim(operand, feature_index). (C5) size(mean) = dim(operand, feature_index). (C6) size(variance) = dim(operand, feature_index). (C7) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-9","dir":"","previous_headings":"Ops > batch_norm_inference","what":"Examples","title":"StableHLO Specification","text":"","code":"// %operand: [ //            [[1.0, 2.0], [3.0, 4.0]], //            [[3.0, 4.0], [1.0, 2.0]] //           ] // %scale: [1.0, 1.0] // %offset: [1.0, 1.0] // %mean: [2.0, 3.0] // %variance: [1.0, 1.0] %result = \"stablehlo.batch_norm_inference\"(%operand, %scale, %offset, %mean, %variance) {   epsilon = 0.0 : f32,   feature_index = 2 : i64 } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>) -> tensor<2x2x2xf64> // %result: [ //           [[0.0, 0.0], [2.0, 2.0]], //           [[2.0, 2.0], [0.0, 0.0]] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-10","dir":"","previous_headings":"Ops > batch_norm_training","what":"Semantics","title":"StableHLO Specification","text":"Computes mean variance across dimensions except feature_index dimension normalizes operand tensor producing output, batch_mean batch_var tensors. formally, operation can expressed decomposition existing StableHLO operations using Python syntax follows: quantized types, performs dequantize_batch_norm_grad_or_training_quantize(lambda operand, scale, offset: batch_norm_training(operand, scale, offset, epsilon, feature_index), operand, scale, offset, type(output), type(batch_mean), type(batch_var)).","code":"def compute_mean(operand, feature_index):   (sum,) = reduce(       inputs=[operand],       init_values=[constant(0, element_type(operand))],       dimensions=[i for i in range(rank(operand)) if i != feature_index],       body=lambda x, y: add(x, y))   divisor = constant(size(operand) / dim(operand, feature_index),                      element_type(operand))   divisor_bcast = broadcast_in_dim(divisor, [], type(sum))   return divide(sum, divisor_bcast)  def compute_variance(operand, feature_index):   mean = compute_mean(operand, feature_index)   mean_bcast = broadcast_in_dim(mean, [feature_index], type(operand))   centered_operand = subtract(operand, mean_bcast)   return compute_mean(mul(centered_operand, centered_operand), feature_index)  def batch_norm_training(operand, scale, offset, epsilon, feature_index):   mean = compute_mean(operand, feature_index)   variance = compute_variance(operand, feature_index)   return batch_norm_inference(operand, scale, offset, mean, variance, epsilon,                               feature_index),          mean, variance"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-9","dir":"","previous_headings":"Ops > batch_norm_training","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= feature_index < rank(operand). (C2) operand, scale, offset, batch_mean, batch_var output baseline_element_type. (C3) size(scale) = dim(operand, feature_index). (C4) size(offset) = dim(operand, feature_index). (C5) size(batch_mean) = dim(operand, feature_index). (C6) size(batch_var) = dim(operand, feature_index). (C7) baseline_type(output) = baseline_type(operand).","code":""},{"path":"/SPEC.html","id":"examples-10","dir":"","previous_headings":"Ops > batch_norm_training","what":"Examples","title":"StableHLO Specification","text":"","code":"// %operand: [ //            [[1.0, 2.0], [3.0, 4.0]], //            [[3.0, 4.0], [1.0, 2.0]] //           ] // %scale: [1.0, 1.0] // %offset: [1.0, 1.0] %output, %batch_mean, %batch_var = \"stablehlo.batch_norm_training\"(%operand, %scale, %offset) {   epsilon = 0.0 : f32,   feature_index = 2 : i64 } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>) ->     (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>) // %output: [ //           [[0.0, 0.0], [2.0, 2.0]], //           [[2.0, 2.0], [0.0, 0.0]] //          ] // %batch_mean: [2.0, 3.0] // %batch_var: [1.0, 1.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-11","dir":"","previous_headings":"Ops > bitcast_convert","what":"Semantics","title":"StableHLO Specification","text":"Performs bitcast operation operand tensor produces result tensor bits entire operand tensor reinterpreted using type result tensor. formally, given E = element_type(operand), E' = element_type(result), R = rank(operand): num_bits(E') < num_bits(E), bits(result[i0, ..., iR-1, :]) = bits(operand[i0, ..., iR-1]). num_bits(E') > num_bits(E), bits(result[i0, ..., iR-2]) = bits(operand[i0, ..., iR-2, :]). num_bits(E') = num_bits(E), bits(result[i0, ..., iR-1]) = bits(operand[i0, ..., iR-1]). bits returns -memory representation given value, behavior implementation-defined exact representation tensors implementation-defined, exact representation element types implementation-defined well.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-10","dir":"","previous_headings":"Ops > bitcast_convert","what":"Constraints","title":"StableHLO Specification","text":"num_bits(E') = num_bits(E), shape(result) = shape(operand). rank(result) = R + 1. dim(result, ) = dim(operand, ) 0 <= < R. dim(result, R) * num_bits(E') = num_bits(E). rank(result) = R - 1. dim(result, ) = dim(operand, ) 0 <= < R. dim(operand, R - 1) * num_bits(E) = num_bits(E'). (C2) is_complex(operand) is_complex(result), is_complex(operand) is_complex(result).","code":""},{"path":"/SPEC.html","id":"examples-11","dir":"","previous_headings":"Ops > bitcast_convert","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: 0x0123456789ABCDEF %result = \"stablehlo.bitcast_convert\"(%operand) : (tensor<f64>) -> tensor<4xf16> // %result: [0xCDEF, 0x89AB, 0x4567, 0x0123] // little-endian representation"},{"path":[]},{"path":"/SPEC.html","id":"semantics-12","dir":"","previous_headings":"Ops > broadcast_in_dim","what":"Semantics","title":"StableHLO Specification","text":"Expands dimensions /rank input tensor duplicating data operand tensor produces result tensor. formally, result[result_index] = operand[operand_index] d axes(operand): operand_index[d] = 0 dim(operand, d) = 1. operand_index[d] = result_index[broadcast_dimensions[d]] otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-11","dir":"","previous_headings":"Ops > broadcast_in_dim","what":"Constraints","title":"StableHLO Specification","text":"element_type(operand), !is_per_axis_quantized(operand). element_type(operand) except quantization_dimension(operand), scales(operand), zero_points(operand) may differ quantization_dimension(result), scales(result), zero_points(result) resp., otherwise. (C2) size(broadcast_dimensions) = rank(operand). (C3) 0 <= broadcast_dimensions < rank(result). (C4) is_unique(broadcast_dimensions). dim(operand, d) = 1 dim(operand, d) = dim(result, broadcast_dimensions[d]). quantization_dimension(result) = broadcast_dimensions[quantization_dimension(operand)]. dim(operand, quantization_dimension(operand)) = 1, scales(result)[] = scales(operand)[0] zero_points(result)[] =   zero_points(operand)[0]   range(dim(result, quantization_dimension(result))).","code":""},{"path":"/SPEC.html","id":"examples-12","dir":"","previous_headings":"Ops > broadcast_in_dim","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [1, 2, 3] //           ] %result = \"stablehlo.broadcast_in_dim\"(%operand) {   broadcast_dimensions = array<i64: 2, 1> } : (tensor<1x3xi32>) -> tensor<2x3x2xi32> // %result: [ //            [ //             [1, 1], //             [2, 2], //             [3, 3] //            ], //            [ //             [1, 1], //             [2, 2], //             [3, 3] //            ] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-13","dir":"","previous_headings":"Ops > case","what":"Semantics","title":"StableHLO Specification","text":"Produces output executing exactly one function branches depending value index. formally, result = selected_branch() : selected_branch = branches[index] 0 <= index < size(branches). selected_branch = branches[-1] otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-12","dir":"","previous_headings":"Ops > case","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 < size(branches). (C2) input_types(branches...) = []. (C3) (output_types(branches...)). (C4) type(results...) = output_types(branches[0]).","code":""},{"path":"/SPEC.html","id":"examples-13","dir":"","previous_headings":"Ops > case","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %index: -1 // %result_branch0: [0, 0] // %result_branch1: [1, 1] %result0, %result1 = \"stablehlo.case\"(%index) ({   \"stablehlo.return\"(%result_branch0, %result_branch0) : (tensor<2xi64>, tensor<2xi64>) -> () }, {   \"stablehlo.return\"(%result_branch1, %result_branch1) : (tensor<2xi64>, tensor<2xi64>) -> () }) : (tensor<i32>) -> (tensor<2xi64>, tensor<2xi64>) // %result0: [1, 1] // %result1: [1, 1]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-14","dir":"","previous_headings":"Ops > cbrt","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise cubic root operation operand tensor produces result tensor. Depending element type, following: floats: rootn(x, 3) IEEE-754. complex numbers: complex cubic root. quantized types: dequantize_op_quantize(cbrt, operand, type(result))","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-13","dir":"","previous_headings":"Ops > cbrt","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-14","dir":"","previous_headings":"Ops > cbrt","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [0.0, 1.0, 8.0, 27.0] %result = \"stablehlo.cbrt\"(%operand) : (tensor<4xf64>) -> tensor<4xf64> // %result: [0.0, 1.0, 2.0, 3.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-15","dir":"","previous_headings":"Ops > ceil","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise ceil operand tensor produces result tensor. Implements roundToIntegralTowardPositive operation IEEE-754 specification. quantized types, performs dequantize_op_quantize(ceil, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-14","dir":"","previous_headings":"Ops > ceil","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-15","dir":"","previous_headings":"Ops > ceil","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0] %result = \"stablehlo.ceil\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-0.0, -0.0, 1.0, 1.0, 2.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-16","dir":"","previous_headings":"Ops > cholesky","what":"Semantics","title":"StableHLO Specification","text":"Computes Cholesky decomposition batch matrices. formally, index_space(result), result[i0, ..., iR-3, :, :] Cholesky decomposition [i0, ..., iR-3, :, :], form either lower-triangular (lower true) upper-triangular (lower false) matrix. output values opposite triangle, .e. strict upper triangle strict lower triangle correspondingly, implementation-defined. exists input matrix Hermitian positive-definite matrix, behavior undefined. quantized types, performs dequantize_op_quantize(lambda operand: cholesky(operand, lower), , type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-15","dir":"","previous_headings":"Ops > cholesky","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type() = baseline_type(result). (C2) 2 <= rank(). (C3) dim(, -2) = dim(, -1).","code":""},{"path":"/SPEC.html","id":"examples-16","dir":"","previous_headings":"Ops > cholesky","what":"Examples","title":"StableHLO Specification","text":"","code":"// %a: [ //      [1.0, 2.0, 3.0], //      [2.0, 20.0, 26.0], //      [3.0, 26.0, 70.0] //     ] %result = \"stablehlo.cholesky\"(%a) {   lower = true } : (tensor<3x3xf32>) -> tensor<3x3xf64> // %result: [ //           [1.0, 0.0, 0.0], //           [2.0, 4.0, 0.0], //           [3.0, 5.0, 6.0] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-17","dir":"","previous_headings":"Ops > clamp","what":"Semantics","title":"StableHLO Specification","text":"Clamps every element operand tensor minimum maximum value produces result tensor. formally, result[result_index] = minimum(maximum(operand[result_index], min_element), max_element), min_element = rank(min) = 0 ? min[] : min[result_index], max_element = rank(max) = 0 ? max[] : max[result_index]. quantized types, performs dequantize_op_quantize(clamp, min, operand, max, type(result)). Imposing ordering complex numbers involves surprising semantics, future planning remove support complex numbers operation (#560).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-16","dir":"","previous_headings":"Ops > clamp","what":"Constraints","title":"StableHLO Specification","text":"(C1) rank(min) = 0 shape(min) = shape(operand). (C2) rank(max) = 0 shape(max) = shape(operand). (C3) baseline_element_type(min) = baseline_element_type(operand) = baseline_element_type(max). (C4) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-17","dir":"","previous_headings":"Ops > clamp","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %min: [5, 10, 15] // %operand: [3, 13, 23] // %max: [10, 15, 20] %result = \"stablehlo.clamp\"(%min, %operand, %max) : (tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> tensor<3xi32> // %result: [5, 13, 20]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-18","dir":"","previous_headings":"Ops > collective_broadcast","what":"Semantics","title":"StableHLO Specification","text":"Within process group StableHLO process grid, send value operand tensor source process target processes produce result tensor. operation splits StableHLO process grid process_groups defined follows: cross_replica(replica_groups) channel_id <= 0. cross_partition(replica_groups) channel_id > 0. Afterwards, result@process given : operand@process_groups[, 0] exists process process_groups[]. broadcast_in_dim(constant(is_quantized(result) ? quantize(0,   element_type(result)) : 0, element_type(result)), [], type(result)) otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-17","dir":"","previous_headings":"Ops > collective_broadcast","what":"Constraints","title":"StableHLO Specification","text":"(C1) is_unique(replica_groups). num_replicas cross_replica used. num_partitions cross_partition used. (C3) type(result) = type(operand).","code":""},{"path":"/SPEC.html","id":"examples-18","dir":"","previous_headings":"Ops > collective_broadcast","what":"Examples","title":"StableHLO Specification","text":"","code":"// num_replicas: 4 // num_partitions: 1 // %operand@(0, 0): [[1, 2]] // %operand@(1, 0): [[3, 4]] // %operand@(2, 0): [[5, 6]] // %operand@(3, 0): [[7, 8]] %result = \"stablehlo.collective_broadcast\"(%operand) {   replica_groups = dense<[[2, 1]]> : tensor<1x2xi64>,   channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> } : (tensor1x2xi64>) -> tensor<1x2xi64> // %result@(0, 0): [[0, 0]] // %result@(1, 0): [[5, 6]] // %result@(2, 0): [[5, 6]] // %result@(3, 0): [[0, 0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-19","dir":"","previous_headings":"Ops > collective_permute","what":"Semantics","title":"StableHLO Specification","text":"Within process group StableHLO process grid, sends value operand tensor source process target process produces result tensor. operation splits StableHLO process grid process_groups defined follows: cross_replica(source_target_pairs) channel_id <= 0. cross_partition(source_target_pairs) channel_id > 0. Afterwards, result@process given : operand@process_groups[, 0], exists process_groups[, 1] = process. broadcast_in_dim(constant(is_quantized(result) ? quantize(0,   element_type(result)) : 0, element_type(result)), [], type(result)) otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-18","dir":"","previous_headings":"Ops > collective_permute","what":"Constraints","title":"StableHLO Specification","text":"(C1) dim(source_target_pairs, 1) = 2. (C2) is_unique(source_target_pairs[:, 0]). (C3) is_unique(source_target_pairs[:, 1]). num_replicas cross_replica used. num_partitions cross_partition used. (C5) type(result) = type(operand).","code":""},{"path":"/SPEC.html","id":"examples-19","dir":"","previous_headings":"Ops > collective_permute","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// num_replicas: 3 // num_partitions: 1 // %operand@(0, 0): [[1, 2], [3, 4]] // %operand@(1, 0): [[5, 6], [7, 8]] // %operand@(2, 0): [[9, 10], [11, 12]] %result = \"stablehlo.collective_permute\"(%operand) {   source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>,   channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> } : (tensor<2x2xi64>) -> tensor<2x2xi64> // // %result@(0, 0): [[0, 0], [0, 0]] // %result@(1, 0): [[1, 2], [3, 4]] // %result@(2, 0): [[5, 6], [7, 8]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-20","dir":"","previous_headings":"Ops > compare","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise comparison lhs rhs tensors according comparison_direction compare_type, produces result tensor. values comparison_direction compare_type following semantics: boolean integer element types: EQ: lhs = rhs. NE: lhs != rhs. GE: lhs >= rhs. GT: lhs > rhs. LE: lhs <= rhs. LT: lhs < rhs. floating-point element types compare_type = FLOAT, op implements following IEEE-754 operations: EQ: compareQuietEqual. NE: compareQuietNotEqual. GE: compareQuietGreaterEqual. GT: compareQuietGreater. LE: compareQuietLessEqual. LT: compareQuietLess. floating-point element types compare_type = TOTALORDER, op uses combination totalOrder compareQuietEqual operations IEEE-754. complex element types, lexicographic comparison (real, imag) pairs performed using provided comparison_direction compare_type. Imposing ordering complex numbers involves surprising semantics, future planning remove support complex numbers comparison_direction GE, GT, LE LT (#560). quantized types. performs dequantize_compare(lhs, rhs, comparison_direction).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-19","dir":"","previous_headings":"Ops > compare","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_element_type(lhs) = baseline_element_type(rhs). (C2) shape(lhs) = shape(rhs) = shape(result). SIGNED is_signed_integer(element_type(lhs)). UNSIGNED is_unsigned_integer(element_type(lhs))   is_boolean(element_type(lhs)). FLOAT TOTALORDER is_float(element_type(lhs)). FLOAT is_complex(element_type(lhs)).","code":""},{"path":"/SPEC.html","id":"examples-20","dir":"","previous_headings":"Ops > compare","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [1.0, 3.0] // %rhs: [1.1, 2.9] %result = \"stablehlo.compare\"(%lhs, %rhs) {   comparison_direction = #stablehlo<comparison_direction LT>,   compare_type = #stablehlo<comparison_type FLOAT> } : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xi1> // %result: [true, false]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-21","dir":"","previous_headings":"Ops > complex","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise conversion complex value pair real imaginary values, lhs rhs, produces result tensor.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-20","dir":"","previous_headings":"Ops > complex","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs). (C2) shape(result) = shape(lhs). (C3) element_type(result) type complex<E> E = element_type(lhs).","code":""},{"path":"/SPEC.html","id":"examples-21","dir":"","previous_headings":"Ops > complex","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [1.0, 3.0] // %rhs: [2.0, 4.0] %result = \"stablehlo.complex\"(%lhs, %rhs) : (tensor<2xf64>, tensor<2xf64>) -> tensor<2xcomplex<f64>> // %result: [(1.0, 2.0), (3.0, 4.0)]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-22","dir":"","previous_headings":"Ops > composite","what":"Semantics","title":"StableHLO Specification","text":"Encapsulates operation made (composed) StableHLO operations, taking inputs composite_attributes producing results. semantics op implemented decomposition attribute. composite op can replaced decomposition without changing program semantics. cases inlining decomposition provide op semantics, prefer using custom_call. version field (defaults 0) used denote composite’s semantics change.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-21","dir":"","previous_headings":"Ops > composite","what":"Constraints","title":"StableHLO Specification","text":"(C1) is_namespaced_op_name(name) (C2) is_defined_in_parent_scope(decomposition) (C3) types(inputs...) == input_types(decomposition) (C4) types(results...) == output_types(decomposition)","code":""},{"path":"/SPEC.html","id":"examples-22","dir":"","previous_headings":"Ops > composite","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%results = \"stablehlo.composite\"(%input0, %input1) {   name = \"my_namespace.my_op\",   composite_attributes = {     my_attribute = \"my_value\"   },   decomposition = @my_op,   version = 1 : i32 } : (tensor<f32>, tensor<f32>) -> tensor<f32>"},{"path":[]},{"path":"/SPEC.html","id":"semantics-23","dir":"","previous_headings":"Ops > concatenate","what":"Semantics","title":"StableHLO Specification","text":"Concatenates inputs along dimension dimension order given arguments produces result tensor. formally, result[i0, ..., id, ..., iR-1] = inputs[k][i0, ..., kd, ..., iR-1], : id = d0 + ... + dk-1 + kd. d equal dimension, d0, … dth dimension sizes inputs.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-22","dir":"","previous_headings":"Ops > concatenate","what":"Constraints","title":"StableHLO Specification","text":"(C1) (element_type(inputs...)). (C2) (shape(inputs...)) except dim(inputs..., dimension). (C3) 0 < size(inputs). (C4) 0 <= dimension < rank(inputs[0]). (C5) element_type(result) = element_type(inputs[0]). dim(result, dimension) = dim(inputs[0], dimension) + ....","code":""},{"path":"/SPEC.html","id":"examples-23","dir":"","previous_headings":"Ops > concatenate","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input0: [[1, 2], [3, 4], [5, 6]] // %input1: [[7, 8]] %result = \"stablehlo.concatenate\"(%input0, %input1) {   dimension = 0 : i64 } : (tensor<3x2xi64>, tensor<1x2xi64>) -> tensor<4x2xi64> // %result: [[1, 2], [3, 4], [5, 6], [7, 8]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-24","dir":"","previous_headings":"Ops > constant","what":"Semantics","title":"StableHLO Specification","text":"Produces output tensor constant value.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-23","dir":"","previous_headings":"Ops > constant","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(value) = type(output).","code":""},{"path":"/SPEC.html","id":"examples-24","dir":"","previous_headings":"Ops > constant","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%output = \"stablehlo.constant\"() {   value = dense<[[0.0, 1.0], [2.0, 3.0]]> : tensor<2x2xf32> } : () -> tensor<2x2xf32> // %output: [[0.0, 1.0], [2.0, 3.0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-25","dir":"","previous_headings":"Ops > convert","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise conversion one element type another operand tensor produces result tensor. boolean---supported-type conversions, value false converted zero, value true converted one. -supported-type--boolean conversions, zero value converted false, non-zero values converted true. See work complex types. conversions involving integer--integer, integer--floating-point floating-point--floating-point, source value can exactly represented destination type, result value exact representation. Otherwise, behavior TBD (#180). conversions involving floating-point--integer, fractional part truncated. truncated value represented destination type, behavior TBD (#180). Conversion involving complex--complex follow behavior floating-point--floating-point conversions converting real imaginary parts. complex----type --type--complex conversions, source imaginary value ignored destination imaginary value zeroed, respectively. conversion real part follows floating-point conversions. principle, operation express dequantization (conversion quantized tensors regular tensors), quantization (conversion regular tensors quantized tensors) requantization (conversion quantized tensors), moment dedicated operations - uniform_dequantize first use case uniform_quantize second third use cases. future, two ops may merged convert (#1576).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-24","dir":"","previous_headings":"Ops > convert","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(operand) = shape(result).","code":""},{"path":"/SPEC.html","id":"examples-25","dir":"","previous_headings":"Ops > convert","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [-1, 0, 1] %result = \"stablehlo.convert\"(%operand) : (tensor<3xi64>) -> tensor<3xcomplex<f64>> // %result: [(-1.0, 0.0), (0.0, 0.0), (1.0, 0.0)]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-26","dir":"","previous_headings":"Ops > convolution","what":"Semantics","title":"StableHLO Specification","text":"Computes dot products windows lhs slices rhs produces result. following diagram shows elements result computed lhs rhs using concrete example. formally, consider following reframing inputs terms lhs order able express windows lhs: lhs_window_dimensions = lhs_shape(dim(lhs, input_batch_dimension), dim(rhs, kernel_spatial_dimensions), dim(lhs, input_feature_dimension)). lhs_window_strides = lhs_shape(1, window_strides, 1). lhs_padding = lhs_shape([0, 0], padding, [0, 0]). lhs_base_dilations = lhs_shape(1, lhs_dilation, 1). lhs_window_dilations = lhs_shape(1, rhs_dilation, 1). reframing uses following helper functions: lhs_shape(n, hw, c) = permute([n] + hw + [c], [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension]). result_shape(n1, hw, c1) = permute([n1] + hw + [c1], [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension]). permute([j0, j1, ..., jR-1], permutation) = [i0, i1, ..., iR-1] j[d] = [permutation[d]]. feature_group_count = 1 batch_group_count = 1, output_spatial_index index_space(dim(result, output_spatial_dimensions...)), result[result_shape(:, output_spatial_index, :)] = dot_product : padding_value = constant(0, element_type(lhs)). padded_lhs = pad(lhs, padding_value, lhs_padding[:, 0], lhs_padding[:, 1], lhs_base_dilations - 1). lhs_window_start = lhs_shape(0, output_spatial_index, 0) * lhs_window_strides. lhs_window = slice(padded_lhs, lhs_window_start, lhs_window_start + lhs_window_dimensions, lhs_window_dilations). reversed_lhs_window = reverse(lhs_window, [input_spatial_dimensions[dim] dim range(size(window_reversal)) window_reversal[dim] = true]). feature appears unused, future planning remove (#1181). dot_product = dot_general(reversed_lhs_window, rhs,     lhs_batching_dimensions=[],     lhs_contracting_dimensions=input_spatial_dimensions + [input_feature_dimension],     rhs_batching_dimensions=[],     rhs_contracting_dimensions=kernel_spatial_dimensions + [kernel_input_feature_dimension]). feature_group_count > 1: lhses = split(lhs, feature_group_count, input_feature_dimension). rhses = split(rhs, feature_group_count, kernel_output_feature_dimension). results... = convolution(lhses..., rhses..., ..., feature_group_count=1, ...). result = concatenate(results, output_feature_dimension). batch_group_count > 1: lhses = split(lhs, batch_group_count, input_batch_dimension). rhses = split(rhs, batch_group_count, kernel_output_feature_dimension). results... = convolution(lhses..., rhses..., ..., batch_group_count=1, ...). result = concatenate(results, output_feature_dimension). quantized types, performs dequantize_op_quantize(     lambda lhs, rhs: convolution(lhs, rhs, window_strides, padding,         lhs_dilation, rhs_dilation, window_reversal, input_batch_dimension,         input_feature_dimension, input_spatial_dimensions,         kernel_input_feature_dimension, kernel_output_feature_dimension,         kernel_spatial_dimensions, output_batch_dimension,         output_feature_dimension, output_spatial_dimensions,         feature_group_count, batch_group_count, precision_config), lhs, rhs,         type(result)). hybrid quantized types, performs hybrid_dequantize_then_op(     lambda lhs, rhs: convolution(lhs, rhs, window_strides, padding,         lhs_dilation, rhs_dilation, window_reversal, input_batch_dimension,         input_feature_dimension, input_spatial_dimensions,         kernel_input_feature_dimension, kernel_output_feature_dimension,         kernel_spatial_dimensions, output_batch_dimension,         output_feature_dimension, output_spatial_dimensions,         feature_group_count, batch_group_count, precision_config), lhs, rhs).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-25","dir":"","previous_headings":"Ops > convolution","what":"Constraints","title":"StableHLO Specification","text":"(C1) N = rank(lhs) = rank(rhs). (C2) size(window_strides) = N - 2. (C3) 0 < window_strides. (C4) shape(padding) = [N - 2, 2]. (C5) size(lhs_dilation) = N - 2. (C6) 0 < lhs_dilation. (C7) size(rhs_dilation) = N - 2. (C8) 0 < rhs_dilation. (C9) size(window_reversal) = N - 2. (C10) dim(lhs, input_batch_dimension) % batch_group_count = 0. (C11) dim(lhs, input_feature_dimension) % feature_group_count = 0. (C12) size(input_spatial_dimensions) = N - 2. is_unique(input_dimensions). 0 <= input_dimensions < N. (C14) dim(rhs, kernel_input_feature_dimension) = dim(lhs, input_feature_dimension) / feature_group_count. (C15) dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0. (C16) dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0. (C17) size(kernel_spatial_dimensions) = N - 2. is_unique(kernel_dimensions). 0 <= kernel_dimensions < N. (C19) size(output_spatial_dimensions) = N - 2. is_unique(output_dimensions). 0 <= output_dimensions < N. (C21) 0 < feature_group_count. (C22) 0 < batch_group_count. (C23) feature_group_count = 1 batch_group_count = 1. (C24) size(precision_config) = 2. dim(lhs, input_batch_dimension) / batch_group_count result_dim = output_batch_dimension. dim(rhs, kernel_output_feature_dimension) result_dim = output_feature_dimension. output_spatial_dimensions[spatial_dim] = result_dim. lhs_dim = input_spatial_dimensions[spatial_dim]. rhs_dim = kernel_spatial_dimensions[spatial_dim]. dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) = 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1. padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]. dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) = 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1. is_empty_window[lhs_dim] = padded_input_shape[lhs_dim] = 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]. num_windows = is_empty_window[lhs_dim] ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1. (C26) rank(result) = N. (C27) element_type(lhs) = element_type(rhs) = element_type(result). (C28) is_quantized(lhs) = is_quantized(result) is_quantized(rhs). (C29) is_per_axis_quantized(rhs), quantization_dimension(rhs) = kernel_output_feature_dimension. (C30) is_per_axis_quantized(result), quantization_dimension(result) = output_feature_dimension. (C31) storage_type(lhs) = storage_type(rhs). (C32) expressed_type(lhs) = expressed_type(rhs) = expressed_type(result). (C33) is_per_tensor_quantized(rhs), is_per_tensor_quantized(result). (C34) element_type(lhs) = expressed_type(rhs) = element_type(result).","code":""},{"path":"/SPEC.html","id":"examples-26","dir":"","previous_headings":"Ops > convolution","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[ //        [ //          [1], [2], [5], [6] //        ], //        [ //          [3], [4], [7], [8] //        ], //        [ //          [10], [11], [14], [15] //        ], //        [ //          [12], [13], [16], [17] //        ] //      ]] // // %rhs: [ //        [[[1]], [[1]], [[1]]], //        [[[1]], [[1]], [[1]]], //        [[[1]], [[1]], [[1]]] //       ] %result = \"stablehlo.convolution\"(%lhs, %rhs) {   window_strides = array<i64: 4, 4>,   padding = dense<0> : tensor<2x2xi64>,   lhs_dilation = array<i64: 2, 2>,   rhs_dilation = array<i64: 1, 1>,   window_reversal = array<i1: false, false>,   // In the StableHLO dialect, dimension numbers are encoded via:   // `[<input dimensions>]x[<kernel dimensions>]->[output dimensions]`.   // \"b\" is batch dimension, \"f\" is feature dimension,   // \"i\" is input feature dimension, \"o\" is output feature dimension,   // \"0/1/etc\" are spatial dimensions.   dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,   batch_group_count = 1 : i64,   feature_group_count = 1 : i64,   precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>] } : (tensor<1x4x4x1xi64>, tensor<3x3x1x1xi64>) -> tensor<1x2x2x1xi64> // %result: [[ //            [[10], [26]], //            [[46], [62]] //          ]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-27","dir":"","previous_headings":"Ops > cosine","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise cosine operation operand tensor produces result tensor. Depending element type, following: floats: cos IEEE-754. complex numbers: complex cosine. quantized types: dequantize_op_quantize(cosine, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-26","dir":"","previous_headings":"Ops > cosine","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-27","dir":"","previous_headings":"Ops > cosine","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [0.0, 1.57079632],       // [0, pi/2] //            [3.14159265, 4.71238898] // [pi, 3pi/2] //           ] %result = \"stablehlo.cosine\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 0.0], [-1.0, 0.0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-28","dir":"","previous_headings":"Ops > count_leading_zeros","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise count number leading zero bits operand tensor produces result tensor.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-27","dir":"","previous_headings":"Ops > count_leading_zeros","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(operand) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-28","dir":"","previous_headings":"Ops > count_leading_zeros","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[0, 1], [128, -1]] %result = \"stablehlo.count_leading_zeros\"(%operand) : (tensor<2x2xi64>) -> tensor<2x2xi64> // %result: [[64, 63], [56, 0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-29","dir":"","previous_headings":"Ops > custom_call","what":"Semantics","title":"StableHLO Specification","text":"Encapsulates implementation-defined operation call_target_name takes inputs called_computations produces results. has_side_effect, backend_config api_version may used provide additional implementation-defined metadata. moment, operation contains fairly disorganized collection metadata reflects organic evolution counterpart operation XLA compiler. future, planning unify metadata (#741).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"examples-29","dir":"","previous_headings":"Ops > custom_call","what":"Examples","title":"StableHLO Specification","text":"","code":"%results = \"stablehlo.custom_call\"(%input0) {   call_target_name = \"foo\",   has_side_effect = false,   backend_config = {bar = 42 : i32},   api_version = 4 : i32,   called_computations = [@foo] } : (tensor<f64>) -> tensor<f64>"},{"path":[]},{"path":"/SPEC.html","id":"semantics-30","dir":"","previous_headings":"Ops > divide","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise division dividend lhs divisor rhs tensors produces result tensor. Depending element type, following: integers: integer division produces algebraic quotient fractional part discarded. floats: division IEEE-754. complex numbers: complex division. dequantize_op_quantize(divide, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-28","dir":"","previous_headings":"Ops > divide","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(lhs) = baseline_type(rhs) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-30","dir":"","previous_headings":"Ops > divide","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [17.1, -17.1, 17.1, -17.1] // %rhs: [3.0, 3.0, -3.0, -3.0] %result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32> // %result: [5.66666651, -5.66666651, -5.66666651, 5.66666651]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-31","dir":"","previous_headings":"Ops > dot_general","what":"Semantics","title":"StableHLO Specification","text":"Computes dot products slices lhs slices rhs produces result tensor. formally, result[result_index] = dot_product, : lhs_result_dimensions = [d d axes(lhs) d lhs_batching_dimensions d lhs_contracting_dimensions]. rhs_result_dimensions = [d d axes(rhs) d rhs_batching_dimensions d rhs_contracting_dimensions]. result_batching_index + result_lhs_index + result_rhs_index = result_index size(result_batching_index) = size(lhs_batching_dimensions), size(result_lhs_index) = size(lhs_result_dimensions) size(result_rhs_index) = size(rhs_result_dimensions). transposed_lhs = transpose(lhs, lhs_batching_dimensions + lhs_result_dimensions + lhs_contracting_dimensions). transposed_lhs_slice = slice(transposed_lhs, result_batching_index + result_lhs_index + [:, ..., :]). reshaped_lhs_slice = reshape(transposed_lhs_slice, dims(lhs, lhs_contracting_dimensions)). transposed_rhs = transpose(rhs, rhs_batching_dimensions + rhs_result_dimensions + rhs_contracting_dimensions). transposed_rhs_slice = slice(transposed_rhs, result_batching_index + result_rhs_index + [:, ..., :]). reshaped_rhs_slice = reshape(transposed_rhs_slice, dims(rhs, rhs_contracting_dimensions)). dot_product = reduce(     inputs=[multiply(reshaped_lhs_slice, reshaped_rhs_slice)],     init_values=[constant(0, element_type(result))],     dimensions=range(size(lhs_contracting_dimensions)),     body=lambda x, y: add(x, y)). quantized types, performs dequantize_op_quantize(     lambda lhs, rhs: dot_general(lhs, rhs, lhs_batching_dimensions,         rhs_batching_dimensions, lhs_contracting_dimensions,         rhs_contracting_dimensions, precision_config), lhs, rhs, type(result)). hybrid quantized types, performs hybrid_dequantize_then_op(     lambda lhs, rhs: dot_general(lhs, rhs, lhs_batching_dimensions,         rhs_batching_dimensions, lhs_contracting_dimensions,         rhs_contracting_dimensions, precision_config), lhs, rhs). precision_config controls tradeoff speed accuracy computations accelerator backends. can one following (moment, semantics enum values underspecified, planning address #755): DEFAULT: Fastest calculation, least accurate approximation original number. HIGH: Slower calculation, accurate approximation original number. HIGHEST: Slowest calculation, accurate approximation original number. DotAlgorithm defines main properties algorithm used implement dot operation, also defines precision. algorithm attribute fields set, precision_config must DEFAULT. DotAlgorithms default value, default parameters implementation defined. , dot algorithm fields may set None specify empty dot algorithm, instead use precision_config value. DotAlgorithm fields include: lhs_precision_type rhs_precision_type, precisions LHS RHS operation rounded . Precision types independent storage types inputs output. accumulation_type precision used accumulation. lhs_component_count, rhs_component_count, num_primitive_operations apply algorithm decomposes LHS /RHS multiple components multiple “primitive” dot operations values - usually emulate higher precision (e.g. Leveraging bfloat16 Artificial Intelligence Datatype Higher-Precision Computations: bf16_6x tf32_3x, etc). algorithms decomposition, values set 1. allow_imprecise_accumulation specify accumulation lower precision permitted steps (e.g. CUBLASLT_MATMUL_DESC_FAST_ACCUM). Example DotAlgorithm attributes: implementations decide combinations supported. general, guaranteed algorithm supported accelerator type consumer StableHLO. given algorithm supported, error raised opposed falling back alternative. StableHLO verification provide best effort verification, preventing algorithms known supported hardware. See xla_data.proto > Algorithm supported algorithm values. Ticket #2483 captures plan create centralized doc supported algorithms backend.","code":"// Inputs are casted to tf32, and then accumulated in f32: {lhs_precision_type = tf32,  rhs_precision_type = tf32,  accumulation_type = f32,  lhs_component_count = 1,  rhs_component_count = 1,  num_primitive_operations = 1,  allow_imprecise_accumulation = false}   // bf16_6x: each input is decomposed to 3 bf16 components, then 6 dot operations are done on those components, and the result is accumulated in f32. {lhs_precision_type = bf16,  rhs_precision_type = bf16,  accumulation_type = f32,  lhs_component_count = 3,  rhs_component_count = 3,  num_primitive_operations = 6,  allow_imprecise_accumulation = false}   // Inputs are (casted to) f8e5m2, and we accumulate in f32, but for some steps we may accumulate in lower precision. {lhs_precision_type = f8e5m2,  rhs_precision_type = f8e5m2,  accumulation_type = f32,  lhs_component_count = 1,  rhs_component_count = 1,  num_primitive_operations = 1,  allow_imprecise_accumulation = true}"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-29","dir":"","previous_headings":"Ops > dot_general","what":"Constraints","title":"StableHLO Specification","text":"(C1) size(lhs_batching_dimensions) = size(rhs_batching_dimensions). (C2) size(lhs_contracting_dimensions) =   size(rhs_contracting_dimensions). (C3) is_unique(lhs_batching_dimensions + lhs_contracting_dimensions). (C4) is_unique(rhs_batching_dimensions + rhs_contracting_dimensions). (C5) 0 <= lhs_batching_dimensions < rank(lhs). (C6) 0 <= lhs_contracting_dimensions < rank(lhs). (C7) 0 <= rhs_batching_dimensions < rank(rhs). (C8) 0 <= rhs_contracting_dimensions < rank(rhs). (C9) dim(lhs, lhs_batching_dimensions...) =   dim(rhs, rhs_batching_dimensions...). (C10) dim(lhs, lhs_contracting_dimensions...) =   dim(rhs, rhs_contracting_dimensions...). (C11) size(precision_config) = 2. (C12) shape(result) = dim(lhs, lhs_batching_dimensions) +   dim(lhs, lhs_result_dimensions) + dim(rhs, rhs_result_dimensions). (C13) element_type(lhs) = element_type(rhs). (C14) is_quantized(lhs) = is_quantized(result) is_quantized(rhs). (C15) zero_points(rhs) = 0. (C16) is_per_axis_quantized(rhs), quantization_dimension(rhs) rhs_contracting_dimensions. (C17) storage_type(lhs) = storage_type(rhs). (C18) expressed_type(lhs) = expressed_type(rhs) = expressed_type(result). (C19) is_per_tensor_quantized(rhs), is_per_tensor_quantized(result). (C20) element_type(lhs) = expressed_type(rhs) = element_type(result). (C21) precision_config... = DEFAULT. (C22) 0 < lhs_component_count. (C23) 0 < rhs_component_count. (C24) 0 < num_primitive_operations.","code":""},{"path":"/SPEC.html","id":"examples-31","dir":"","previous_headings":"Ops > dot_general","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [ //        [[1, 2], //         [3, 4]], //        [[5, 6], //         [7, 8]] //       ] // %rhs: [ //        [[1, 0], //         [0, 1]], //        [[1, 0], //         [0, 1]] //       ] %result = \"stablehlo.dot_general\"(%lhs, %rhs) {   dot_dimension_numbers = #stablehlo.dot<     lhs_batching_dimensions = [0],     rhs_batching_dimensions = [0],     lhs_contracting_dimensions = [2],     rhs_contracting_dimensions = [1]   >,   precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>],   algorithm = #stablehlo.dot_algorithm<     lhs_precision_type = tf32,     rhs_precision_type = tf32,     accumulation_type = f32,     lhs_component_count = 1,     rhs_component_count = 1,     num_primitive_operations = 1,     allow_imprecise_accumulation = false   > } : (tensor<2x2x2xi64>, tensor<2x2x2xi64>) -> tensor<2x2x2xi64> // %result: [ //           [[1, 2], //            [3, 4]], //           [[5, 6], //            [7, 8]] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-32","dir":"","previous_headings":"Ops > dynamic_broadcast_in_dim","what":"Semantics","title":"StableHLO Specification","text":"operation functionally identical broadcast_in_dim op, result shape specified dynamically via output_dimensions. operation also accepts optional attributes known_expanding_dimensions, known_nonexpanding_dimensions express static knowledge expanding behavior dimensions. specified, dimensions assumed possibly expanding.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-30","dir":"","previous_headings":"Ops > dynamic_broadcast_in_dim","what":"Constraints","title":"StableHLO Specification","text":"element_type(operand), !is_per_axis_quantized(operand). element_type(operand) except quantization_dimension(operand), scales(operand), zero_points(operand) may differ quantization_dimension(result), scales(result), zero_points(result) resp., otherwise. (C2) size(broadcast_dimensions) = rank(operand). (C3) 0 <= broadcast_dimensions < rank(result). (C4) is_unique(broadcast_dimensions). dim(operand, d) = 1 dim(operand, d) = dim(result, broadcast_dimensions[d]). quantization_dimension(result) = broadcast_dimensions[quantization_dimension(operand)]. dim(operand, quantization_dimension(operand)) = 1, scales(result)[] = scales(operand)[0] zero_points(result)[] =   zero_points(operand)[0]   range(dim(result, quantization_dimension(result))). (C7) size(output_dimensions) = rank(result). (C8) is_unique(known_expanding_dimensions + known_nonexpanding_dimensions). (C9) 0 <= known_expanding_dimensions < rank(operand). (C10) 0 <= known_nonexpanding_dimensions < rank(operand).","code":""},{"path":"/SPEC.html","id":"examples-32","dir":"","previous_headings":"Ops > dynamic_broadcast_in_dim","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [1, 2, 3] //           ] %operand = stablehlo.constant dense<[[1, 2, 3]]> : tensor<1x3xi64> %output_dimensions = stablehlo.constant dense<[2, 3, 2]> : tensor<3xi64> %result = \"stablehlo.dynamic_broadcast_in_dim\"(%operand, %output_dimensions) {   broadcast_dimensions = array<i64: 2, 1>,   known_expanding_dimensions = array<i64: 0>,   known_nonexpanding_dimensions = array<i64: 1> } : (tensor<1x3xi64>, tensor<3xi64>) -> tensor<2x3x2xi64> // %result: [ //            [ //             [1, 1], //             [2, 2], //             [3, 3] //            ], //            [ //             [1, 1], //             [2, 2], //             [3, 3] //            ] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-33","dir":"","previous_headings":"Ops > dynamic_conv","what":"Semantics","title":"StableHLO Specification","text":"operation functionally identical convolution op, padding specified dynamically via padding.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-31","dir":"","previous_headings":"Ops > dynamic_conv","what":"Constraints","title":"StableHLO Specification","text":"(C1) N = rank(lhs) = rank(rhs). (C2) size(window_strides) = N - 2. (C3) 0 < window_strides. (C4) shape(padding) = [N - 2, 2]. (C5) size(lhs_dilation) = N - 2. (C6) 0 < lhs_dilation. (C7) size(rhs_dilation) = N - 2. (C8) 0 < rhs_dilation. (C9) size(window_reversal) = N - 2. (C10) dim(lhs, input_batch_dimension) % batch_group_count = 0. (C11) dim(lhs, input_feature_dimension) % feature_group_count = 0. (C12) size(input_spatial_dimensions) = N - 2. is_unique(input_dimensions). 0 <= input_dimensions < N. (C14) dim(rhs, kernel_input_feature_dimension) = dim(lhs, input_feature_dimension) / feature_group_count. (C15) dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0. (C16) dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0. (C17) size(kernel_spatial_dimensions) = N - 2. is_unique(kernel_dimensions). 0 <= kernel_dimensions < N. (C19) size(output_spatial_dimensions) = N - 2. is_unique(output_dimensions). 0 <= output_dimensions < N. (C21) 0 < feature_group_count. (C22) 0 < batch_group_count. (C23) feature_group_count = 1 batch_group_count = 1. (C24) size(precision_config) = 2. dim(lhs, input_batch_dimension) / batch_group_count result_dim = output_batch_dimension. dim(rhs, kernel_output_feature_dimension) result_dim = output_feature_dimension. output_spatial_dimensions[spatial_dim] = result_dim. lhs_dim = input_spatial_dimensions[spatial_dim]. rhs_dim = kernel_spatial_dimensions[spatial_dim]. dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) = 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1. padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]. dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) = 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1. is_empty_window[lhs_dim] = padded_input_shape[lhs_dim] = 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]. num_windows = is_empty_window[lhs_dim] ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1. (C26) rank(result) = N. (C27) element_type(lhs) = element_type(rhs) = element_type(result). (C28) is_quantized(lhs) = is_quantized(result) is_quantized(rhs). (C29) is_per_axis_quantized(rhs), quantization_dimension(rhs) = kernel_output_feature_dimension. (C30) is_per_axis_quantized(result), quantization_dimension(result) = output_feature_dimension. (C31) storage_type(lhs) = storage_type(rhs). (C32) expressed_type(lhs) = expressed_type(rhs) = expressed_type(result). (C33) is_per_tensor_quantized(rhs), is_per_tensor_quantized(result). (C34) element_type(lhs) = expressed_type(rhs) = element_type(result).","code":""},{"path":"/SPEC.html","id":"examples-33","dir":"","previous_headings":"Ops > dynamic_conv","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[ //        [[1], [2], [5], [6]], //        [[3], [4], [7], [8]], //        [[10], [11], [14], [15]], //        [[12], [13], [16], [17]] //      ]] // // %rhs: [ //         [[[1]], [[1]], [[1]]], //         [[[1]], [[1]], [[1]]], //         [[[1]], [[1]], [[1]]] //        ] // %padding: [[1, 1], //            [1, 1]] %result = \"stablehlo.dynamic_conv\"(%lhs, %rhs, %padding) {   window_strides = array<i64: 4, 4>,   lhs_dilation = array<i64: 2, 2>,   rhs_dilation = array<i64: 1, 1>,   window_reversal = array<i1: false, false>,   dimension_numbers = #stablehlo.conv<raw     input_batch_dimension = 0,     input_feature_dimension = 3,     input_spatial_dimensions = [0, 1],     kernel_input_feature_dimension = 2,     kernel_output_feature_dimension = 3,     kernel_spatial_dimensions = [0, 1],     output_batch_dimension = 0,     output_feature_dimension = 3,     output_spatial_dimensions = [1, 2]   >,   feature_group_count = 1 : i64,   batch_group_count = 1 : i64,   precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>] } : (tensor<1x4x4x1xi64>, tensor<3x3x1x1xi64>, tensor<2x2xi64>) -> tensor<1x2x2x1xi64> // %result: [[ //            [[1], [5]], //            [[10], [14]] //          ]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-34","dir":"","previous_headings":"Ops > dynamic_gather","what":"Semantics","title":"StableHLO Specification","text":"operation functionally identical gather op, slice_sizes specified dynamically value.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-32","dir":"","previous_headings":"Ops > dynamic_gather","what":"Constraints","title":"StableHLO Specification","text":"(C1) rank(operand) = size(offset_dims) + size(collapsed_slice_dims). (C2) 0 <= index_vector_dim <= rank(start_indices). (C3) size(start_index_map) =        index_vector_dim < rank(start_indices) ?        dim(start_indices, index_vector_dim) : 1. (C4) is_unique(offset_dims) is_sorted(offset_dims). (C5) 0 <= offset_dims < rank(result). (C6) is_unique(collapsed_slice_dims) is_sorted(collapsed_slice_dims). (C7) 0 <= collapsed_slice_dims < rank(operand). (C8) slice_sizes[collapsed_slice_dims...] <= 1. (C9) is_unique(start_index_map). (C10) 0 <= start_index_map < rank(operand). (C11) size(slice_sizes) = rank(operand). (C12) 0 <= slice_sizes <= shape(operand). batch_dim_sizes = shape(start_indices) except dimension size start_indices corresponding index_vector_dim included. offset_dim_sizes = shape(slice_sizes) except dimension sizes slice_sizes corresponding collapsed_slice_dims included. combine puts batch_dim_sizes axes corresponding batch_dims offset_dim_sizes axes corresponding offset_dims. (C14) element_type(operand) = element_type(result).","code":""},{"path":"/SPEC.html","id":"examples-34","dir":"","previous_headings":"Ops > dynamic_gather","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [[1, 2], [3, 4], [5, 6], [7, 8]], //            [[9, 10],[11, 12], [13, 14], [15, 16]], //            [[17, 18], [19, 20], [21, 22], [23, 24]] //           ] // %start_indices: [ //                  [[0, 0], [1, 0], [2, 1]], //                  [[0, 1], [1, 1], [0, 2]] //                 ] // %slize_sizes: [1, 2, 2] %result = \"stablehlo.dynamic_gather\"(%operand, %start_indices, %slize_sizes) {   dimension_numbers = #stablehlo.gather<     offset_dims = [2, 3],     collapsed_slice_dims = [0],     start_index_map = [1, 0],     index_vector_dim = 2>,   indices_are_sorted = false } : (tensor<3x4x2xi64>, tensor<2x3x2xi64>, tensor<3xi64>) -> tensor<2x3x2x2xi64> // %result: [ //            [ //              [[1, 2], [3, 4]], //              [[3, 4], [5, 6]], //              [[13, 14], [15, 16]] //            ], //            [ //              [[9, 10], [11, 12]], //              [[11, 12], [13, 14]], //              [[17, 18], [19, 20]] //            ] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-35","dir":"","previous_headings":"Ops > dynamic_iota","what":"Semantics","title":"StableHLO Specification","text":"operation functionally identical iota op, result shape specified dynamically via output_shape.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-33","dir":"","previous_headings":"Ops > dynamic_iota","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= iota_dimension < size(output_shape). (C2) rank(result) = size(output_shape).","code":""},{"path":"/SPEC.html","id":"examples-35","dir":"","previous_headings":"Ops > dynamic_iota","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%output_shape = stablehlo.constant dense<[4, 5]> : tensor<2xi64> %result = \"stablehlo.dynamic_iota\"(%output_shape) {   iota_dimension = 0 : i64 } : (tensor<2xi64>) -> tensor<4x5xi64> // %result: [ //           [0, 0, 0, 0, 0], //           [1, 1, 1, 1, 1], //           [2, 2, 2, 2, 2], //           [3, 3, 3, 3, 3] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-36","dir":"","previous_headings":"Ops > dynamic_pad","what":"Semantics","title":"StableHLO Specification","text":"operation functionally identical pad op, edge_padding_low, edge_padding_high, interior_padding specified dynamically values.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-34","dir":"","previous_headings":"Ops > dynamic_pad","what":"Constraints","title":"StableHLO Specification","text":"(C1) element_type(operand) = element_type(padding_value) =   element_type(result). (C2) size(edge_padding_low) = size(edge_padding_high) =   size(interior_padding) = rank(operand). (C3) 0 <= interior_padding. (C4) shape(result) = shape(operand) + edge_padding_low +   max(shape(operand) - 1, 0) * interior_padding + edge_padding_high.","code":""},{"path":"/SPEC.html","id":"examples-36","dir":"","previous_headings":"Ops > dynamic_pad","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [1, 2, 3], //            [4, 5, 6] //           ] // %padding_value: 0 // %edge_padding_low: [0, 1] // %edge_padding_high: [2, 1] // %interior_padding: [1, 2] %result = \"stablehlo.dynamic_pad\"(%operand, %padding_value,   %edge_padding_low, %edge_padding_high, %interior_padding ) : (tensor<2x3xi64>, tensor<i64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<5x9xi64> // %result: [ //           [0, 1, 0, 0, 2, 0, 0, 3, 0], //           [0, 0, 0, 0, 0, 0, 0, 0, 0], //           [0, 4, 0, 0, 5, 0, 0, 6, 0], //           [0, 0, 0, 0, 0, 0, 0, 0, 0], //           [0, 0, 0, 0, 0, 0, 0, 0, 0] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-37","dir":"","previous_headings":"Ops > dynamic_reshape","what":"Semantics","title":"StableHLO Specification","text":"operation functionally identical reshape op, result shape specified dynamically via output_shape.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-35","dir":"","previous_headings":"Ops > dynamic_reshape","what":"Constraints","title":"StableHLO Specification","text":"element_type(operand), !is_per_axis_quantized(operand). element_type(operand) except quantization_dimension(operand) quantization_dimension(result) may differ, otherwise. (C2) size(operand) = size(result). reduce(dims(operand, [0, 1, ..., quantization_dimension(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result, [0, 1, ..., quantization_dimension(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y). dim(operand, quantization_dimension(operand)) =   dim(result, quantization_dimension(result)). reduce(dims(operand,   [quantization_dimension(operand) + 1, ..., rank(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result,   [quantization_dimension(result) + 1, ..., rank(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y). (C4) size(output_shape) = rank(result).","code":""},{"path":"/SPEC.html","id":"examples-37","dir":"","previous_headings":"Ops > dynamic_reshape","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[1, 2, 3], [4, 5, 6]] // %output_shape: [3, 2] %result = \"stablehlo.dynamic_reshape\"(%operand, %output_shape) : (tensor<2x3xi64>, tensor<2xi64>) -> tensor<3x2xi64> // %result: [[1, 2], [3, 4], [5, 6]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-38","dir":"","previous_headings":"Ops > dynamic_slice","what":"Semantics","title":"StableHLO Specification","text":"Extracts slice operand using dynamically-computed starting indices produces result tensor. start_indices contain starting indices slice dimension subject potential adjustment, slice_sizes contain sizes slice dimension. formally, result[result_index] = operand[operand_index] : adjusted_start_indices = clamp(0, start_indices, shape(operand) -   slice_sizes). operand_index = adjusted_start_indices + result_index.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-36","dir":"","previous_headings":"Ops > dynamic_slice","what":"Constraints","title":"StableHLO Specification","text":"(C1) element_type(operand) = element_type(result). (C2) size(start_indices) = size(slice_sizes) = rank(operand). (C3) (type(start_indices...)). (C4) 0 <= slice_sizes <= shape(operand). (C5) shape(result) = slice_sizes.","code":""},{"path":"/SPEC.html","id":"examples-38","dir":"","previous_headings":"Ops > dynamic_slice","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [0, 0, 1, 1], //            [0, 0, 1, 1], //            [0, 0, 0, 0], //            [0, 0, 0, 0] //           ] // %start_indices0: -1 // %start_indices1: 3 %result = \"stablehlo.dynamic_slice\"(%operand, %start_indices0, %start_indices1) {   slice_sizes = array<i64: 2, 2> } : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32> // %result: [ //           [1, 1], //           [1, 1] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-39","dir":"","previous_headings":"Ops > dynamic_update_slice","what":"Semantics","title":"StableHLO Specification","text":"Produces result tensor equal operand tensor except slice starting start_indices updated values update. formally, result[result_index] defined : adjusted_start_indices = clamp(0, start_indices, shape(operand) -   shape(update)). update_index = result_index - adjusted_start_indices. operand[result_index] otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-37","dir":"","previous_headings":"Ops > dynamic_update_slice","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(operand) = type(result). (C2) element_type(update) = element_type(operand). (C3) rank(update) = rank(operand). (C4) size(start_indices) = rank(operand). (C5) (type(start_indices...)). (C6) 0 <= shape(update) <= shape(operand).","code":""},{"path":"/SPEC.html","id":"examples-39","dir":"","previous_headings":"Ops > dynamic_update_slice","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [1, 1, 0, 0], //            [1, 1, 0, 0], //            [1, 1, 1, 1], //            [1, 1, 1, 1] //           ] // %update: [ //           [1, 1], //           [1, 1] //          ] // %start_indices0: -1 // %start_indices1: 3 %result = \"stablehlo.dynamic_update_slice\"(%operand, %update, %start_indices0, %start_indices1)   : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32> // %result: [ //           [1, 1, 1, 1], //           [1, 1, 1, 1], //           [1, 1, 1, 1], //           [1, 1, 1, 1] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-40","dir":"","previous_headings":"Ops > exponential","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise exponential operation operand tensor produces result tensor. Depending element type, following: floats: exp IEEE-754. complex numbers: complex exponential. quantized types: dequantize_op_quantize(exponential, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-38","dir":"","previous_headings":"Ops > exponential","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-40","dir":"","previous_headings":"Ops > exponential","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[0.0, 1.0], [2.0, 3.0]] %result = \"stablehlo.exponential\"(%operand) : (tensor<2x2xf64>) -> tensor<2x2xf64> // %result: [[1.0, 2.7182818284590451], [7.3890560989306504, 20.085536923187668]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-41","dir":"","previous_headings":"Ops > exponential_minus_one","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise exponential minus one operation operand tensor produces result tensor. Depending element type, following: floats: expm1 IEEE-754. complex numbers: complex exponential minus one. quantized types: dequantize_op_quantize(exponential_minus_one, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-39","dir":"","previous_headings":"Ops > exponential_minus_one","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-41","dir":"","previous_headings":"Ops > exponential_minus_one","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [0.0, 1.0] %result = \"stablehlo.exponential_minus_one\"(%operand) : (tensor<2xf64>) -> tensor<2xf64> // %result: [0.0, 1.71828187]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-42","dir":"","previous_headings":"Ops > fft","what":"Semantics","title":"StableHLO Specification","text":"Performs forward inverse Fourier transforms real complex inputs/outputs. fft_type one following: FFT: Forward complex--complex FFT. IFFT: Inverse complex--complex FFT. RFFT: Forward real--complex FFT. IRFFT: Inverse real--complex FFT (.e. takes complex, returns real). formally, given function fft takes 1-dimensional tensors complex types input, produces 1-dimensional tensors types output computes discrete Fourier transform: fft_type = FFT, result defined final result series L computations L = size(fft_length). example, L = 3: result1[i0, ..., :] = fft(operand[i0, ..., :]). result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1]). result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1]). Furthermore, given function ifft type signature computes inverse fft: fft_type = IFFT, result defined inverse computations fft_type = FFT. example, L = 3: result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1]). result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1]). result[i0, ..., :] = ifft(result2[i0, ..., :]). Furthermore, given function rfft takes 1-dimensional tensors floating-point types, produces 1-dimensional tensors complex types floating-point semantics works follows: rfft(real_operand) = truncated_result complex_operand... = (real_operand..., 0.0). complex_result = fft(complex_operand). truncated_result = complex_result[:(rank(complex_result) / 2 + 1)]. (discrete Fourier transform computed real operands, first N/2 + 1 elements result unambiguously define rest result, result rfft truncated avoid computing redundant elements). fft_type = RFFT, result defined final result series L computations L = size(fft_length). example, L = 3: result1[i0, ..., :] = rfft(operand[i0, ..., :]). result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1]). result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1]). Finally, given function irfft type signature computes inverse rfft: fft_type = IRFFT, result defined inverse computations fft_type = RFFT. example, L = 3: result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1]). result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1]). result[i0, ..., :] = irfft(result2[i0, ..., :]).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-40","dir":"","previous_headings":"Ops > fft","what":"Constraints","title":"StableHLO Specification","text":"(C1) size(fft_length) <= rank(operand). fft_type = FFT, element_type(operand) element_type(result) complex type. fft_type = IFFT, element_type(operand) element_type(result) complex type. fft_type = RFFT, element_type(operand) floating-point type element_type(result) complex type floating-point semantics. fft_type = IRFFT, element_type(operand) complex type element_type(result) floating-point type floating-point semantics. (C3) 1 <= size(fft_length) <= 3. (C4) among operand result, tensor real floating-point type, shape(real)[-size(fft_length):] = fft_length. fft_type = RFFT, dim(result, -1) = dim(operand, -1) = 0 ? 0 : dim(operand, -1) / 2 + 1. fft_type = IRFFT, dim(operand, -1) = dim(result, -1) = 0 ? 0 : dim(result, -1) / 2 + 1.","code":""},{"path":"/SPEC.html","id":"examples-42","dir":"","previous_headings":"Ops > fft","what":"Examples","title":"StableHLO Specification","text":"","code":"// %operand: [(1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)] %result = \"stablehlo.fft\"(%operand) {   fft_type = #stablehlo<fft_type FFT>,   fft_length = array<i64: 4> } : (tensor<4xcomplex<f32>>) -> tensor<4xcomplex<f32>> // %result: [(1.0, 0.0), (1.0, 0.0), (1.0, 0.0), (1.0, 0.0)]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-43","dir":"","previous_headings":"Ops > floor","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise floor operand tensor produces result tensor. Implements roundToIntegralTowardNegative operation IEEE-754 specification. quantized types, performs dequantize_op_quantize(floor, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-41","dir":"","previous_headings":"Ops > floor","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-43","dir":"","previous_headings":"Ops > floor","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0] %result = \"stablehlo.floor\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-1.0, -1.0, 0.0, 0.0, 2.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-44","dir":"","previous_headings":"Ops > gather","what":"Semantics","title":"StableHLO Specification","text":"Gathers slices operand tensor offsets specified start_indices produces result tensor. following diagram shows elements result map elements operand using concrete example. diagram picks example result indices explains detail operand indices correspond . formally, result[result_index] = operand[operand_index] : batch_dims = [d d axes(result) d offset_dims]. batch_index = result_index[batch_dims...]. start_indices[bi0, ..., :, ..., biN] bi individual elements batch_index : inserted index_vector_dim index, index_vector_dim < rank(start_indices). [start_indices[batch_index]] otherwise. full_start_index[d_operand] = clamp(start_index[d_start], 0,   dim(operand, d_operand) - slice_sizes[d_operand]) d_operand = start_index_map[d_start]. full_start_index[d_operand] = 0 otherwise. full_batching_index[d_operand] =   batch_index[d_start - (d_start < index_vector_dim ? 0 : 1)] d_operand = operand_batching_dims[i_batching] d_start = start_indices_batching_dims[i_batching]. full_batching_index[d_operand] = 0 otherwise. offset_index = result_index[offset_dims...]. full_offset_index = [oi0, ..., 0, ..., oiN] oi individual elements offset_index, 0 inserted indices collapsed_slice_dims operand_batching_dims. operand_index = full_start_index + full_batching_index + full_offset_index. indices_are_sorted true implementation can assume start_indices sorted respect start_index_map, otherwise behavior undefined. formally, i1 < i2 indices(result), full_start_index(i1) <= full_start_index(i2).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-42","dir":"","previous_headings":"Ops > gather","what":"Constraints","title":"StableHLO Specification","text":"(C1) rank(operand) = size(offset_dims) + size(collapsed_slice_dims) +        size(operand_batching_dims). (C2) 0 <= index_vector_dim <= rank(start_indices). (C3) size(start_index_map) =        index_vector_dim < rank(start_indices) ?        dim(start_indices, index_vector_dim) : 1. (C4) is_unique(offset_dims) is_sorted(offset_dims). (C5) 0 <= offset_dims < rank(result). (C6) is_unique(concatenate(collapsed_slice_dims, operand_batching_dims)) (C7) is_sorted(collapsed_slice_dims). (C8) 0 <= collapsed_slice_dims < rank(operand). (C9) slice_sizes[collapsed_slice_dims...] <= 1. (C10) is_sorted(operand_batching_dims). (C11) 0 <= operand_batching_dims < rank(operand). (C12) slice_sizes[operand_batching_dims...] <= 1. (C13) is_unique(start_indices_batching_dims). (C14) 0 <= start_indices_batching_dims < rank(start_indices). (C15) index_vector_dim start_indices_batching_dims. (C16) size(operand_batching_dims) == size(start_indices_batching_dims). (C17) dim(operand, operand_batching_dims...) =         dim(start_indices, start_indices_batching_dims...). (C18) is_unique(concatenate(start_index_map, operand_batching_dims)). (C19) 0 <= start_index_map < rank(operand). (C20) size(slice_sizes) = rank(operand). (C21) 0 <= slice_sizes <= shape(operand). batch_dim_sizes = shape(start_indices) except dimension size start_indices corresponding index_vector_dim included. offset_dim_sizes = slice_sizes except dimension sizes slice_sizes corresponding collapsed_slice_dims operand_batching_dims included. combine puts batch_dim_sizes axes corresponding batch_dims offset_dim_sizes axes corresponding offset_dims. (C23) element_type(operand) = element_type(result).","code":""},{"path":"/SPEC.html","id":"examples-44","dir":"","previous_headings":"Ops > gather","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [ //             [[1, 2], [3, 4], [5, 6], [7, 8]], //             [[9, 10],[11, 12], [13, 14], [15, 16]], //             [[17, 18], [19, 20], [21, 22], [23, 24]] //            ], //            [ //             [[25, 26], [27, 28], [29, 30], [31, 32]], //             [[33, 34], [35, 36], [37, 38], [39, 40]], //             [[41, 42], [43, 44], [45, 46], [47, 48]] //            ] //           ] // %start_indices: [ //                  [ //                   [[0, 0], [1, 0], [2, 1]], //                   [[0, 1], [1, 1], [0, 9]] //                  ], //                  [ //                   [[0, 0], [2, 1], [2, 2]], //                   [[1, 2], [0, 1], [1, 0]] //                  ] //                 ] %result = \"stablehlo.gather\"(%operand, %start_indices) {   dimension_numbers = #stablehlo.gather<     offset_dims = [3, 4],     collapsed_slice_dims = [1],     operand_batching_dims = [0],     start_indices_batching_dims = [1],     start_index_map = [2, 1],     index_vector_dim = 3>,   slice_sizes = array<i64: 1, 1, 2, 2>,   indices_are_sorted = false } : (tensor<2x3x4x2xi32>, tensor<2x2x3x2xi64>) -> tensor<2x2x3x2x2xi32> // %result: [ //           [ //            [ //             [[1, 2], [3, 4]], //             [[3, 4], [5, 6]], //             [[13, 14], [15, 16]] //            ], //            [ //             [[33, 34], [35, 36]], //             [[35, 36], [37, 38]], //             [[41, 42], [43, 44]] //            ] //           ], //           [ //            [ //             [[1, 2], [3, 4]], //             [[13, 14], [15, 16]], //             [[21, 22], [23, 24]] //            ], //            [ //             [[43, 44], [45, 46]], //             [[33, 34], [35, 36]], //             [[27, 28], [29, 30]] //            ] //           ] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-45","dir":"","previous_headings":"Ops > get_dimension_size","what":"Semantics","title":"StableHLO Specification","text":"Produces size given dimension operand. formally, result = dim(operand, dimension). Semantics concerns shape component type. element-type anything.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-43","dir":"","previous_headings":"Ops > get_dimension_size","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= dimension < rank(operand).","code":""},{"path":"/SPEC.html","id":"examples-45","dir":"","previous_headings":"Ops > get_dimension_size","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[1, 2, 3], [4, 5, 6]] %result = \"stablehlo.get_dimension_size\"(%operand) {   dimension = 1 : i64 } : (tensor<2x3xi64>) -> tensor<i32> // %result: 3"},{"path":"/SPEC.html","id":"get_tuple_element","dir":"","previous_headings":"Ops","what":"get_tuple_element","title":"StableHLO Specification","text":"Note: Per StableHLO v1.0 Cleanup #2283, op explored deprecation appears unused frameworks compilers. , limited compatibility guarantees (6 months).","code":""},{"path":"/SPEC.html","id":"semantics-46","dir":"","previous_headings":"Ops > get_tuple_element","what":"Semantics","title":"StableHLO Specification","text":"Extracts element index position operand tuple produces result. formally, result = operand[index].","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-44","dir":"","previous_headings":"Ops > get_tuple_element","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= index < size(operand). (C2) type(result) = tuple_element_types(operand)[index].","code":""},{"path":"/SPEC.html","id":"examples-46","dir":"","previous_headings":"Ops > get_tuple_element","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: ([1.0, 2.0], (3)) %result = \"stablehlo.get_tuple_element\"(%operand) <{index = 0 : i32}> : (tuple<tensor<2xf64>, tuple<tensor<i64>>>) -> tensor<2xf64> // %result: [1.0, 2.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-47","dir":"","previous_headings":"Ops > if","what":"Semantics","title":"StableHLO Specification","text":"Produces output executing exactly one function true_branch false_branch depending value pred. formally, result = pred ? true_branch() : false_branch().","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-45","dir":"","previous_headings":"Ops > if","what":"Constraints","title":"StableHLO Specification","text":"(C1) input_types(true_branch) = input_types(false_branch) = []. (C2) output_types(true_branch) = output_types(false_branch). (C3) type(results...) = output_types(true_branch).","code":""},{"path":"/SPEC.html","id":"examples-47","dir":"","previous_headings":"Ops > if","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %result_true_branch: 10 // %result_false_branch: 11 // %pred: true %result = \"stablehlo.if\"(%pred) ({   \"stablehlo.return\"(%result_true_branch) : (tensor<i32>) -> () }, {   \"stablehlo.return\"(%result_false_branch) : (tensor<i32>) -> () }) : (tensor<i1>) -> tensor<i32> // %result: 10"},{"path":[]},{"path":"/SPEC.html","id":"semantics-48","dir":"","previous_headings":"Ops > imag","what":"Semantics","title":"StableHLO Specification","text":"Extracts imaginary part, element-wise, operand produces result tensor. formally, element x: imag(x) = is_complex(x) ? imaginary_part(x) : constant(0, element_type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-46","dir":"","previous_headings":"Ops > imag","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(result) = shape(operand). complex_element_type(element_type(operand)) is_complex(operand). element_type(operand) otherwise.","code":""},{"path":"/SPEC.html","id":"examples-48","dir":"","previous_headings":"Ops > imag","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [(1.0, 2.0), (3.0, 4.0)] %result = \"stablehlo.imag\"(%operand) : (tensor<2xcomplex<f32>>) -> tensor<2xf32> // %result: [2.0, 4.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-49","dir":"","previous_headings":"Ops > infeed","what":"Semantics","title":"StableHLO Specification","text":"Reads data infeed produces results. Semantics infeed_config implementation-defined. results consist payload values come first token comes last. future, planning split payload token two separate outputs improve clarity (#670).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-47","dir":"","previous_headings":"Ops > infeed","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 < size(results). (C2) is_empty(result[:-1]) is_tensor(type(results[:-1])). (C3) is_token(type(results[-1])).","code":""},{"path":"/SPEC.html","id":"examples-49","dir":"","previous_headings":"Ops > infeed","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %token: !stablehlo.token // infeed_queue[0]: [[1, 2], [3, 4]] // infeed_queue[1]: [[5, 6], [7, 8]] %results0:2 = \"stablehlo.infeed\"(%token) {   infeed_config = \"\" } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token) // results0#0: [[1, 2], [3, 4]] %results1:2 = \"stablehlo.infeed\"(%token) {   infeed_config = \"\" } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token) // results1#0: [[5, 6], [7, 8]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-50","dir":"","previous_headings":"Ops > iota","what":"Semantics","title":"StableHLO Specification","text":"Fills output tensor values increasing order starting zero along iota_dimension dimension. formally, output[output_index] = constant(is_quantized(output) ? quantize(output_index[iota_dimension], element_type(output)) : output_index[iota_dimension], element_type(output)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-48","dir":"","previous_headings":"Ops > iota","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 <= iota_dimension < rank(output).","code":""},{"path":"/SPEC.html","id":"examples-50","dir":"","previous_headings":"Ops > iota","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%output = \"stablehlo.iota\"() {   iota_dimension = 0 : i64 } : () -> tensor<4x5xi32> // %output: [ //           [0, 0, 0, 0, 0], //           [1, 1, 1, 1, 1], //           [2, 2, 2, 2, 2], //           [3, 3, 3, 3, 3] //          ]  %output = \"stablehlo.iota\"() {   iota_dimension = 1 : i64 } : () -> tensor<4x5xi32> // %output: [ //           [0, 1, 2, 3, 4], //           [0, 1, 2, 3, 4], //           [0, 1, 2, 3, 4], //           [0, 1, 2, 3, 4] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-51","dir":"","previous_headings":"Ops > is_finite","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise check whether value x finite (.e. neither +Inf, -Inf, NaN) produces y tensor. Implements isFinite operation IEEE-754 specification. quantized types, result always true.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-49","dir":"","previous_headings":"Ops > is_finite","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(x) = shape(y).","code":""},{"path":"/SPEC.html","id":"examples-51","dir":"","previous_headings":"Ops > is_finite","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Logical values: -Inf, +Inf, NaN, ... // %x: [0xFFF0000000000000, 0x7FF0000000000000, 0x7FF8000000000000, -10.0, -0.0, 0.0, 10.0] %y = \"stablehlo.is_finite\"(%x) : (tensor<7xf64) -> tensor<7xi1> // %y: [false, false, false, true, true, true, true]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-52","dir":"","previous_headings":"Ops > log","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise logarithm operation operand tensor produces result tensor. Depending element type, following: floats: log IEEE-754. complex numbers: complex logarithm. quantized types: dequantize_op_quantize(log, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-50","dir":"","previous_headings":"Ops > log","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-52","dir":"","previous_headings":"Ops > log","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[1.0, 2.0], [3.0, 4.0]] %result = \"stablehlo.log\"(%operand) : (tensor<2x2xf64>) -> tensor<2x2xf64> // %result: [[0.0, 0.69314718055994529], [1.0986122886681098, 1.3862943611198906]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-53","dir":"","previous_headings":"Ops > log_plus_one","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise logarithm plus one operation operand tensor produces result tensor. Depending element type, following: floats: logp1 IEEE-754. complex numbers: complex(log(hypot(real(x) + 1, imag(x))), atan2(imag(x), real(x) + 1)) quantized types: dequantize_op_quantize(log_plus_one, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-51","dir":"","previous_headings":"Ops > log_plus_one","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-53","dir":"","previous_headings":"Ops > log_plus_one","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [0.0, -0.999, 7.0, 6.38905621, 15.0] %result = \"stablehlo.log_plus_one\"(%operand) : (tensor<5xf64>) -> tensor<5xf64> // %result: [0.0, -6.90776825, 2.07944155, 2.0, 2.77258873]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-54","dir":"","previous_headings":"Ops > logistic","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise logistic operation operand tensor produces result tensor. Depending element type, following: floats: division(1, addition(1, exp(-x))) IEEE-754. complex numbers: complex logistic. quantized types: dequantize_op_quantize(logistic, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-52","dir":"","previous_headings":"Ops > logistic","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-54","dir":"","previous_headings":"Ops > logistic","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[0.0, 1.0], [2.0, 3.0]] %result = \"stablehlo.logistic\"(%operand) : (tensor<2x2xf64>) -> tensor<2x2xf64> // %result: [[0.5, 0.73105858], [0.88079708, 0.95257413]]"},{"path":"/SPEC.html","id":"map","dir":"","previous_headings":"Ops","what":"map","title":"StableHLO Specification","text":"Note: Per StableHLO v1.0 Cleanup #2283, op explored deprecation appears unused frameworks compilers. , limited compatibility guarantees (6 months).","code":""},{"path":"/SPEC.html","id":"semantics-55","dir":"","previous_headings":"Ops > map","what":"Semantics","title":"StableHLO Specification","text":"Applies map function computation inputs along dimensions produces result tensor. formally, result[result_index] = computation(inputs...[result_index]).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-53","dir":"","previous_headings":"Ops > map","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(inputs...) = shape(result). (C2) 0 < size(inputs) = N. (C3) dimensions = range(rank(inputs[0])). (C4) computation type (tensor<E0>, ..., tensor<EN-1>) -> tensor<E'> Ei = element_type(inputs[]) E' = element_type(result).","code":""},{"path":"/SPEC.html","id":"examples-55","dir":"","previous_headings":"Ops > map","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input0: [[0, 1], [2, 3]] // %input1: [[4, 5], [6, 7]] %result = \"stablehlo.map\"(%input0, %input1) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = stablehlo.multiply %arg0, %arg1 : tensor<i64>     stablehlo.return %0 : tensor<i64> }) {   dimensions = array<i64: 0, 1> } : (tensor<2x2xi64>, tensor<2x2xi64>) -> tensor<2x2xi64> // %result: [[0, 5], [12, 21]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-56","dir":"","previous_headings":"Ops > maximum","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise max operation tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical . integers: integer maximum. floats: maximum IEEE-754. complex numbers: lexicographic maximum (real, imaginary) pair. Imposing ordering complex numbers involves surprising semantics, future planning remove support complex numbers operation (#560). dequantize_op_quantize(maximum, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-54","dir":"","previous_headings":"Ops > maximum","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(lhs) = baseline_type(rhs) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-56","dir":"","previous_headings":"Ops > maximum","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[1, 2], [7, 8]] // %rhs: [[5, 6], [3, 4]] %result = \"stablehlo.maximum\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 6], [7, 8]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-57","dir":"","previous_headings":"Ops > minimum","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise min operation tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical . integers: integer minimum. floats: minimum IEEE-754. complex numbers: lexicographic minimum (real, imaginary) pair. Imposing ordering complex numbers involves surprising semantics, future planning remove support complex numbers operation (#560). dequantize_op_quantize(minimum, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-55","dir":"","previous_headings":"Ops > minimum","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(lhs) = baseline_type(rhs) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-57","dir":"","previous_headings":"Ops > minimum","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[1, 2], [7, 8]] // %rhs: [[5, 6], [3, 4]] %result = \"stablehlo.minimum\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[1, 2], [3, 4]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-58","dir":"","previous_headings":"Ops > multiply","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise product two tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical . integers: integer multiplication. floats: multiplication IEEE-754. complex numbers: complex multiplication. dequantize_op_quantize(multiply, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-56","dir":"","previous_headings":"Ops > multiply","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-58","dir":"","previous_headings":"Ops > multiply","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.multiply\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 12], [21, 32]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-59","dir":"","previous_headings":"Ops > negate","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise negation operand tensor produces result tensor. Depending element type, following: signed integers: integer negation. unsigned integers: bitcast signed integer, integer negation, bitcast back unsigned integer. floats: negate IEEE-754. complex numbers: complex negation. quantized types: dequantize_op_quantize(negate, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-57","dir":"","previous_headings":"Ops > negate","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-59","dir":"","previous_headings":"Ops > negate","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Negation operation with integer Tensors // %operand: [0, -2] %result = \"stablehlo.negate\"(%operand) : (tensor<2xi32>) -> tensor<2xi32> // %result: [0, 2]  // Negation operation with with complex tensors // %operand: (2.5, 0.0) %result = \"stablehlo.negate\"(%operand) : (tensor<1xcomplex<f32>>) -> tensor<1xcomplex<f32>> // %result: [-2.5, -0.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-60","dir":"","previous_headings":"Ops > not","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise tensor operand produces result tensor. Depending element type, following: booleans: logical . integers: bitwise .","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-58","dir":"","previous_headings":"Ops > not","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(operand) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-60","dir":"","previous_headings":"Ops > not","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Bitwise operation with with integer tensors // %operand: [[1, 2], [3, 4]] %result = \"stablehlo.not\"(%operand) : (tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[-2, -3], [-4, -5]]  // Bitwise operation with with boolean tensors // %operand: [true, false] %result = \"stablehlo.not\"(%operand) : (tensor<2xi1>) -> tensor<2xi1> // %result: [false, true]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-61","dir":"","previous_headings":"Ops > optimization_barrier","what":"Semantics","title":"StableHLO Specification","text":"Ensures operations produce operand executed operations depend result prevents compiler transformations moving operations across barrier. , operation identity, .e. result = operand.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-59","dir":"","previous_headings":"Ops > optimization_barrier","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(operand...) = type(result...).","code":""},{"path":"/SPEC.html","id":"examples-61","dir":"","previous_headings":"Ops > optimization_barrier","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand0: 0.0 // %operand1: 1.0 %result0, %result1 = \"stablehlo.optimization_barrier\"(%operand0, %operand1) : (tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>) // %result0: 0.0 // %result1: 1.0"},{"path":[]},{"path":"/SPEC.html","id":"semantics-62","dir":"","previous_headings":"Ops > or","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise two tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical . integers: bitwise .","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-60","dir":"","previous_headings":"Ops > or","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-62","dir":"","previous_headings":"Ops > or","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Bitwise operation with with integer tensors // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.or\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 6], [7, 12]]  // Logical operation with with boolean tensors // %lhs: [[false, false], [true, true]] // %rhs: [[false, true], [false, true]] %result = \"stablehlo.or\"(%lhs, %rhs) : (tensor<2x2xi1>, tensor<2x2xi1>) -> tensor<2x2xi1> // %result: [[false, true], [true, true]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-63","dir":"","previous_headings":"Ops > outfeed","what":"Semantics","title":"StableHLO Specification","text":"Writes inputs outfeed produces result token. Semantics outfeed_config implementation-defined.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"examples-63","dir":"","previous_headings":"Ops > outfeed","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%result = \"stablehlo.outfeed\"(%input0, %token) {   outfeed_config = \"\" } : (tensor<2x2x2xi64>, !stablehlo.token) -> !stablehlo.token"},{"path":[]},{"path":"/SPEC.html","id":"semantics-64","dir":"","previous_headings":"Ops > pad","what":"Semantics","title":"StableHLO Specification","text":"Expands operand padding around tensor well elements tensor given padding_value. edge_padding_low edge_padding_high specify amount padding added low-end (next index 0) high-end (next highest index) dimension respectively. amount padding can negative, absolute value negative padding indicates number elements remove specified dimension. interior_padding specifies amount padding added two elements dimension may negative. Interior padding occurs edge padding negative edge padding remove elements interior-padded operand. formally, result[result_index] defined : operand[operand_index] result_index = edge_padding_low + operand_index * (interior_padding + 1). padding_value otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-61","dir":"","previous_headings":"Ops > pad","what":"Constraints","title":"StableHLO Specification","text":"(C1) element_type(operand) = element_type(padding_value) =   element_type(result). (C2) size(edge_padding_low) = size(edge_padding_high) =   size(interior_padding) = rank(operand). (C3) 0 <= interior_padding. (C4) shape(result) = shape(operand) + edge_padding_low +   max(shape(operand) - 1, 0) * interior_padding + edge_padding_high.","code":""},{"path":"/SPEC.html","id":"examples-64","dir":"","previous_headings":"Ops > pad","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [1, 2, 3], //            [4, 5, 6] //           ] // %padding_value: 0 %result = \"stablehlo.pad\"(%operand, %padding_value) {   edge_padding_low = array<i64: 0, 1>,   edge_padding_high = array<i64: 2, 1>,   interior_padding = array<i64: 1, 2> } : (tensor<2x3xi32>, tensor<i32>) -> tensor<5x9xi32> // %result: [ //           [0, 1, 0, 0, 2, 0, 0, 3, 0], //           [0, 0, 0, 0, 0, 0, 0, 0, 0], //           [0, 4, 0, 0, 5, 0, 0, 6, 0], //           [0, 0, 0, 0, 0, 0, 0, 0, 0], //           [0, 0, 0, 0, 0, 0, 0, 0, 0] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-65","dir":"","previous_headings":"Ops > partition_id","what":"Semantics","title":"StableHLO Specification","text":"Produces partition_id current process.","code":""},{"path":[]},{"path":"/SPEC.html","id":"examples-65","dir":"","previous_headings":"Ops > partition_id","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%result = \"stablehlo.partition_id\"() : () -> tensor<ui32>"},{"path":[]},{"path":"/SPEC.html","id":"semantics-66","dir":"","previous_headings":"Ops > popcnt","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise count number bits set operand tensor produces result tensor.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-62","dir":"","previous_headings":"Ops > popcnt","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(operand) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-66","dir":"","previous_headings":"Ops > popcnt","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [0, 1, 2, 127] %result = \"stablehlo.popcnt\"(%operand) : (tensor<4xi64>) -> tensor<4xi64> // %result: [0, 1, 1, 7]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-67","dir":"","previous_headings":"Ops > power","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise exponentiation lhs tensor rhs tensor produces result tensor. Depending element type, following: integers: integer exponentiation. floats: pow IEEE-754. complex numbers: complex exponentiation. quantized types: dequantize_op_quantize(power, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-63","dir":"","previous_headings":"Ops > power","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-67","dir":"","previous_headings":"Ops > power","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [-2.0, -0.0, -36.0, 5.0, 3.0, 10000.0] // %rhs: [2.0, 2.0, 1.1, 2.0, -1.0, 10.0] %result = \"stablehlo.power\"(%lhs, %rhs) : (tensor<6xf64>, tensor<6xf64>) -> tensor<6xf64> // %result: [4.0, 0.0, -nan, 25.0, 0.333333343, inf]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-68","dir":"","previous_headings":"Ops > real","what":"Semantics","title":"StableHLO Specification","text":"Extracts real part, element-wise, operand produces result tensor. formally, element x: real(x) = is_complex(x) ? real_part(x) : x.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-64","dir":"","previous_headings":"Ops > real","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(result) = shape(operand). complex_element_type(element_type(operand)) is_complex(operand). element_type(operand) otherwise.","code":""},{"path":"/SPEC.html","id":"examples-68","dir":"","previous_headings":"Ops > real","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [(1.0, 2.0), (3.0, 4.0)] %result = \"stablehlo.real\"(%operand) : (tensor<2xcomplex<f32>>) -> tensor<2xf32> // %result: [1.0, 3.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-69","dir":"","previous_headings":"Ops > recv","what":"Semantics","title":"StableHLO Specification","text":"Receives data channel channel_id produces results. is_host_transfer true, operation transfers data host. Otherwise, transfers data another device based values source_target_pairs. flag duplicates information provided channel_type, future planning keep one (#666). is_host_transfer = false source_target_pairs None empty, considered undefined behavior. results consist payload values come first token comes last. future, planning split payload token two separate outputs improve clarity (#670).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-65","dir":"","previous_headings":"Ops > recv","what":"Constraints","title":"StableHLO Specification","text":"(C1) dim(source_target_pairs, 1) = 2. (C2) is_unique(source_target_pairs[:, 0]). (C3) is_unique(source_target_pairs[:, 1]). num_replicas cross_replica used. num_partitions cross_partition used. DEVICE_TO_HOST is_host_transfer = true, DEVICE_TO_DEVICE otherwise.","code":""},{"path":"/SPEC.html","id":"examples-69","dir":"","previous_headings":"Ops > recv","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%results0, %results1 = \"stablehlo.recv\"(%token) {   channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,   is_host_transfer = false,   source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64> } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)"},{"path":[]},{"path":"/SPEC.html","id":"semantics-70","dir":"","previous_headings":"Ops > reduce","what":"Semantics","title":"StableHLO Specification","text":"Applies reduction function body inputs init_values along dimensions produces results tensors. order reductions implementation-defined, means body init_values must form monoid guarantee operation produces results inputs implementations. However, condition doesn’t hold many popular reductions. E.g. floating-point addition body zero init_values don’t actually form monoid floating-point addition associative. formally, results...[j0, ..., jR-1] = reduce(input_slices_converted) : input_slices = inputs...[j0, ..., :, ..., jR-1], : inserted dimensions. input_slices_converted = to_destination_type(input_slices...,   type(func_inputs(body)[:len(func_inputs(body))//2])...). init_values_converted = to_destination_type(init_values...,   type(func_inputs(body)[len(func_inputs(body))//2:])...). exec(node) = body(exec(node.left), exec(node.right)). exec(leaf) = leaf.value. input_slices_converted...[index] values, index index_space(input_slices_converted) ascending lexicographic order index. Interspersed implementation-defined amount init_values_converted implementation-defined positions.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-66","dir":"","previous_headings":"Ops > reduce","what":"Constraints","title":"StableHLO Specification","text":"(C1) (shape(inputs...)). (C2) element_type(inputs...) = element_type(init_values...). (C3) 0 < size(inputs) = size(init_values) = size(results) = N. (C4) 0 <= dimensions < rank(inputs[0]). (C5) is_unique(dimensions). (C6) body type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) is_promotable(element_type(inputs[]), Ei). (C7) shape(results...) = shape(inputs...) except dimension sizes inputs... corresponding dimensions included. (C8) element_type(results[]) = Ei [0,N).","code":""},{"path":"/SPEC.html","id":"examples-70","dir":"","previous_headings":"Ops > reduce","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input = [[0, 1, 2, 3, 4, 5]] // %init_value = 0 %result = \"stablehlo.reduce\"(%input, %init_value) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>     \"stablehlo.return\"(%0) : (tensor<i64>) -> () }) {   dimensions = array<i64: 1> } : (tensor<1x6xi64>, tensor<i64>) -> tensor<1xi64> // %result = [15]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-71","dir":"","previous_headings":"Ops > reduce_precision","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise conversion operand another floating-point type uses exponent_bits mantissa_bits back original floating-point type produces output tensor. formally: mantissa bits original value updated round original value nearest value representable mantissa_bits using roundToIntegralTiesToEven semantics. , mantissa_bits smaller number mantissa bits original value, mantissa bits truncated mantissa_bits. , exponent bits intermediate result don’t fit range provided exponent_bits, intermediate result overflows infinity using original sign underflows zero using original sign. quantized types, performs dequantize_op_quantize(     lambda operand: reduce_precision(operand, exponent_bits, mantissa_bits),     operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-67","dir":"","previous_headings":"Ops > reduce_precision","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(output). (C2) 1 <= exponent_bits. (C3) 0 <= mantissa_bits.","code":""},{"path":"/SPEC.html","id":"examples-71","dir":"","previous_headings":"Ops > reduce_precision","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Logical values: +Inf, NaN, +Denormal, 0.0, 65519.0, 65520.0 // %operand: [0x7FF0000000000000, 0x7FFFFFFFFFFFFFFF, 0x0000000000000001, 0.0, 65519.0, 65520.0] %output = \"stablehlo.reduce_precision\"(%operand) {   exponent_bits = 5 : i32,   mantissa_bits = 10 : i32 } : (tensor<6xf64>) -> tensor<6xf64> // Logical values: +Inf, NaN, 0.0, 0.0, 65504.0, +Inf // %output: [0x7FF0000000000000, 0x7FFFFFFFFFFFFFFF, 0.0, 0.0, 65504.0, 0x7FF0000000000000]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-72","dir":"","previous_headings":"Ops > reduce_scatter","what":"Semantics","title":"StableHLO Specification","text":"Within process group StableHLO process grid, performs reduction, using computations, values operand tensor process, splits reduction result along scatter_dimension parts, scatters split parts processes produce result. operation splits StableHLO process grid process_groups defined follows: cross_replica(replica_groups) channel_id <= 0 use_global_device_ids = false. cross_replica_and_partition(replica_groups) channel_id > 0 use_global_device_ids = false. flattened_ids(replica_groups) channel_id > 0 use_global_device_ids = true. Afterwards, within process_group: reduced_value = all_reduce(operand, replica_groups, channel_id,   use_global_device_ids, computation). parts@sender = split(reduced_value@sender, dim(process_groups, 1),   scatter_dimension). result@receiver = parts@sender[receiver_index] sender process_group, receiver_index = process_group.index(receiver).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-68","dir":"","previous_headings":"Ops > reduce_scatter","what":"Constraints","title":"StableHLO Specification","text":"(C1) dim(operand, scatter_dimension) % dim(process_groups, 1) = 0. (C2) 0 <= scatter_dimension < rank(operand). (C3) is_unique(replica_groups). num_replicas cross_replica used. num_replicas cross_replica_and_partition used. num_processes flattened_ids used. (C5) 0 <= replica_groups < size(replica_groups). (C6) use_global_device_ids = true, channel_id > 0. (C7) computation type (tensor<E>, tensor<E>) -> (tensor<E>) is_promotable(element_type(operand), E). dim(result, scatter_dimension) = dim(operand, scatter_dimension) /   dim(process_groups, 1). (C9) element_type(result) = E.","code":""},{"path":"/SPEC.html","id":"examples-72","dir":"","previous_headings":"Ops > reduce_scatter","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [[1, 2, 3, 4], //                   [5, 6, 7, 8]] // %operand@(1, 0): [[9, 10, 11, 12], //                   [13, 14, 15, 16]] %result = \"stablehlo.reduce_scatter\"(%operand) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):   %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>   \"stablehlo.return\"(%0) : (tensor<i64>) -> () }) {   scatter_dimension = 1 : i64,   replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,   channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> } : (tensor<2x4xi64>) -> tensor<2x2xi64> // // %result@(0, 0): [[10, 12], //                  [18, 20]] // %result@(1, 0): [[14, 16], //                  [22, 24]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-73","dir":"","previous_headings":"Ops > reduce_window","what":"Semantics","title":"StableHLO Specification","text":"Applies reduction function body windows inputs init_values produces results. following diagram shows elements results... computed inputs... using concrete example. formally, results...[result_index] = reduce(windows, init_values, axes(inputs...), body) (see reduce) : padded_inputs = pad(inputs..., init_values..., padding[:, 0], padding[:, 1],   base_dilations - 1). window_start = result_index * window_strides. window_end = window_start + (window_dimensions - 1) * window_dilations + 1. windows = slice(padded_inputs..., window_start, window_end,   window_dilations).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-69","dir":"","previous_headings":"Ops > reduce_window","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 < size(inputs) = size(init_values) = size(results) = N. (C2) (shape(inputs...)). (C3) element_type(inputs...) = element_type(init_values...). (C4) size(window_dimensions) = rank(inputs[0]). (C5) 0 < window_dimensions. (C6) size(window_strides) = rank(inputs[0]). (C7) 0 < window_strides. (C8) size(base_dilations) = rank(inputs[0]). (C9) 0 < base_dilations. (C10) size(window_dilations) = rank(inputs[0]). (C11) 0 < window_dilations. (C12) shape(padding) = [rank(inputs[0]), 2]. (C13) body type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) is_promotable(element_type(inputs[]), Ei). (C14) (shape(results...)). dilated_input_shape = shape(inputs[0]) = 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1. padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1]. dilated_window_shape = (window_dimensions - 1) * window_dilations + 1. is_empty_window = padded_input_shape = 0 || dilated_window_shape > padded_input_shape. num_windows = is_empty_window ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1. (C16) element_type(results[]) = Ei [0,N).","code":""},{"path":"/SPEC.html","id":"examples-73","dir":"","previous_headings":"Ops > reduce_window","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input = [[1, 2], [3, 4], [5, 6]] // %init_value = 0 %result = \"stablehlo.reduce_window\"(%input, %init_value) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>     \"stablehlo.return\"(%0) : (tensor<i64>) -> () }) {   window_dimensions = array<i64: 2, 1>,   window_strides = array<i64: 4, 1>,   base_dilations = array<i64: 2, 1>,   window_dilations = array<i64: 3, 1>,   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<3x2xi64>, tensor<i64>) -> tensor<2x2xi64> // %result = [[0, 0], [3, 4]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-74","dir":"","previous_headings":"Ops > remainder","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise remainder dividend lhs divisor rhs tensors produces result tensor. formally, sign result taken dividend, absolute value result always less divisor’s absolute value. remainder calculated lhs - d * rhs, d given : integers: stablehlo.divide(lhs, rhs). floats: division(lhs, rhs) IEEE-754 rounding attribute roundTowardZero. complex numbers: TBD (#997). dequantize_op_quantize(remainder, lhs, rhs, type(result)). floating-point element types, operation contrast remainder operation IEEE-754 specification d integral value nearest exact value lhs/rhs ties even.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-70","dir":"","previous_headings":"Ops > remainder","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-74","dir":"","previous_headings":"Ops > remainder","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [17, -17, 17, -17] // %rhs: [3, 3, -3, -3] %result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4xi64> // %result: [2, -2, 2, -2]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-75","dir":"","previous_headings":"Ops > replica_id","what":"Semantics","title":"StableHLO Specification","text":"Produces replica_id current process.","code":""},{"path":[]},{"path":"/SPEC.html","id":"examples-75","dir":"","previous_headings":"Ops > replica_id","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%result = \"stablehlo.replica_id\"() : () -> tensor<ui32>"},{"path":[]},{"path":"/SPEC.html","id":"semantics-76","dir":"","previous_headings":"Ops > reshape","what":"Semantics","title":"StableHLO Specification","text":"Performs reshape operand tensor result tensor. Conceptually, amounts keeping canonical representation potentially changing shape, e.g. tensor<2x3xf32> tensor<3x2xf32> tensor<6xf32>. formally, result[result_index] = operand[operand_index] result_index operand_index position lexicographic ordering index_space(result) index_space(operand).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-71","dir":"","previous_headings":"Ops > reshape","what":"Constraints","title":"StableHLO Specification","text":"element_type(operand), !is_per_axis_quantized(operand). element_type(operand) except quantization_dimension(operand) quantization_dimension(result) may differ, otherwise. (C2) size(operand) = size(result). reduce(dims(operand, [0, 1, ..., quantization_dimension(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result, [0, 1, ..., quantization_dimension(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y). dim(operand, quantization_dimension(operand)) =   dim(result, quantization_dimension(result)). reduce(dims(operand,   [quantization_dimension(operand) + 1, ..., rank(operand) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y) =   reduce(dims(result,   [quantization_dimension(result) + 1, ..., rank(result) - 1]),   init_values=1, dimensions=[0], body=lambda x, y: x * y).","code":""},{"path":"/SPEC.html","id":"examples-76","dir":"","previous_headings":"Ops > reshape","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[1, 2, 3], [4, 5, 6]] %result = \"stablehlo.reshape\"(%operand) : (tensor<2x3xi32>) -> tensor<3x2xi32> // %result: [[1, 2], [3, 4], [5, 6]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-77","dir":"","previous_headings":"Ops > reverse","what":"Semantics","title":"StableHLO Specification","text":"Reverses order elements operand along specified dimensions produces result tensor. formally, result[result_index] = operand[operand_index] : operand_index[d] = dim(result, d) - result_index[d] - 1 d dimensions. operand_index[d] = result_index[d] otherwise.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-72","dir":"","previous_headings":"Ops > reverse","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(operand) = type(result). (C2) is_unique(dimensions). (C3) 0 <= dimensions < rank(result).","code":""},{"path":"/SPEC.html","id":"examples-77","dir":"","previous_headings":"Ops > reverse","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand = [[1, 2], [3, 4], [5, 6]] %result = \"stablehlo.reverse\"(%operand) {   dimensions = array<i64: 1> } : (tensor<3x2xi32>) -> tensor<3x2xi32> // %result: [[2, 1], [4, 3], [6, 5]]"},{"path":"/SPEC.html","id":"rng","dir":"","previous_headings":"Ops","what":"rng","title":"StableHLO Specification","text":"Note: Per StableHLO v1.0 Cleanup #2283, op explored deprecation appears unused frameworks compilers. , limited compatibility guarantees (6 months).","code":""},{"path":"/SPEC.html","id":"semantics-78","dir":"","previous_headings":"Ops > rng","what":"Semantics","title":"StableHLO Specification","text":"Generates random numbers using rng_distribution algorithm produces result tensor given shape shape. rng_distribution = UNIFORM, random numbers generated following uniform distribution interval [, b). >= b, behavior undefined. rng_distribution = NORMAL, random numbers generated following normal distribution mean = standard deviation = b. b < 0, behavior undefined. exact way random numbers generated implementation-defined. example, may may deterministic, may may use hidden state. conversations many stakeholders, op come effectively deprecated, future planning explore removing (#597).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-73","dir":"","previous_headings":"Ops > rng","what":"Constraints","title":"StableHLO Specification","text":"(C1) element_type() = element_type(b) = element_type(result). (C2) rng_distribution = NORMAL, is_float(). (C3) shape(result) = shape.","code":""},{"path":"/SPEC.html","id":"examples-78","dir":"","previous_headings":"Ops > rng","what":"Examples","title":"StableHLO Specification","text":"","code":"// %a = 0 // %b = 2 // %shape = [3, 3] %result = \"stablehlo.rng\"(%a, %b, %shape) {   rng_distribution = #stablehlo<rng_distribution UNIFORM> } : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32> // %result: [ //           [1, 0, 1], //           [1, 1, 1], //           [0, 0, 0] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-79","dir":"","previous_headings":"Ops > rng_bit_generator","what":"Semantics","title":"StableHLO Specification","text":"Returns output filled uniform random bits updated output state output_state using pseudorandom number generator algorithm rng_algorithm given initial state initial_state. output guaranteed deterministic function initial_state, guaranteed deterministic implementations. rng_algorithm one following: DEFAULT: Implementation-defined algorithm. THREE_FRY: Implementation-defined variant Threefry algorithm.* PHILOX: Implementation-defined variant Philox algorithm.* * See: Salmon et al. SC 2011. Parallel random numbers: easy 1, 2, 3.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-74","dir":"","previous_headings":"Ops > rng_bit_generator","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(initial_state) = type(output_state). implementation-defined rng_algorithm = DEFAULT. 2 rng_algorithm = THREE_FRY. 2 3 rng_algorithm = PHILOX.","code":""},{"path":"/SPEC.html","id":"examples-79","dir":"","previous_headings":"Ops > rng_bit_generator","what":"Examples","title":"StableHLO Specification","text":"","code":"// %initial_state: [1, 2] %output_state, %output = \"stablehlo.rng_bit_generator\"(%initial_state) {   rng_algorithm = #stablehlo<rng_algorithm THREE_FRY> } : (tensor<2xui64>) -> (tensor<2xui64>, tensor<2x2xui64>) // %output_state: [1, 6] // %output: [ //           [9236835810183407956, 16087790271692313299], //           [18212823393184779219, 2658481902456610144] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-80","dir":"","previous_headings":"Ops > round_nearest_afz","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise rounding towards nearest integer, breaking ties away zero, operand tensor produces result tensor. Implements roundToIntegralTiesToAway operation IEEE-754 specification. quantized types, performs dequantize_op_quantize(round_nearest_afz, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-75","dir":"","previous_headings":"Ops > round_nearest_afz","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-80","dir":"","previous_headings":"Ops > round_nearest_afz","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5] %result = \"stablehlo.round_nearest_afz\"(%operand) : (tensor<5xf64>) -> tensor<5xf64> // %result: [-3.0, 0.0, 1.0, 1.0, 3.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-81","dir":"","previous_headings":"Ops > round_nearest_even","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise rounding towards nearest integer, breaking ties towards even integer, operand tensor produces result tensor. Implements roundToIntegralTiesToEven operation IEEE-754 specification. quantized types, performs dequantize_op_quantize(round_nearest_even, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-76","dir":"","previous_headings":"Ops > round_nearest_even","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-81","dir":"","previous_headings":"Ops > round_nearest_even","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5] %result = \"stablehlo.round_nearest_even\"(%operand) : (tensor<5xf64>) -> tensor<5xf64> // %result: [-2.0, 0.0, 0.0, 1.0, 2.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-82","dir":"","previous_headings":"Ops > rsqrt","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise reciprocal square root operation operand tensor produces result tensor. Depending element type, following: floats: rSqrt IEEE-754. complex numbers: complex reciprocal square root. quantized types: dequantize_op_quantize(rsqrt, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-77","dir":"","previous_headings":"Ops > rsqrt","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-82","dir":"","previous_headings":"Ops > rsqrt","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[1.0, 4.0], [9.0, 25.0]] %result = \"stablehlo.rsqrt\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 0.5], [0.33333343, 0.2]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-83","dir":"","previous_headings":"Ops > scatter","what":"Semantics","title":"StableHLO Specification","text":"Produces results tensors equal inputs tensors except several slices specified scatter_indices updated values updates using update_computation. following diagram shows elements updates... map elements results... using concrete example. diagram picks example updates... indices explains detail results... indices correspond . formally, update_index index_space(updates[0]): update_scatter_dims = [d d axes(updates[0]) d   update_window_dims]. update_scatter_index = update_index[update_scatter_dims...]. scatter_indices[si0, ..., :, ..., siN] si individual elements update_scatter_index : inserted index_vector_dim index, index_vector_dim < rank(scatter_indices). [scatter_indices[update_scatter_index]] otherwise. full_start_index[d_input] = start_index[d_start] d_input = scatter_dims_to_operand_dims[d_start]. full_start_index[d_input] = 0 otherwise. full_batching_index[d_input] =   update_scatter_index[d_start - (d_start < index_vector_dim ? 0 : 1)] d_input = input_batching_dims[i_batching] d_start = scatter_indices_batching_dims[i_batching]. full_batching_index[d_input] = 0 otherwise. update_window_index = update_index[update_window_dims...]. full_window_index = [wi0, ..., 0, ..., wiN] wi individual elements update_window_index, 0 inserted indices inserted_window_dims input_batching_dims. result_index = full_start_index + full_batching_index + full_window_index. Given , results = exec(schedule, inputs), : schedule implementation-defined permutation index_space(updates[0]). updates_converted = to_destination_type(   updates...[update_index], type(func_inputs(update_computation)   [len(func_inputs(update_computation))//2:])... ) updated_values = update_computation(results...[result_index],   updates_converted) updated_results copy results results...[result_index] set updated_values.... updated_results = results. exec([], results) = results. indices_are_sorted true implementation can assume scatter_indices sorted respect scatter_dims_to_operand_dims, otherwise behavior undefined. formally, i1 < i2 indices(result), full_start_index(i1) <= full_start_index(i2). unique_indices true implementation can assume result_index indices scattered unique. unique_indices true indices scattered unique behavior undefined.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-78","dir":"","previous_headings":"Ops > scatter","what":"Constraints","title":"StableHLO Specification","text":"(C1) (shape(inputs...)). (C2) rank(inputs[0]) = size(update_window_dims) + size(inserted_window_dims) + size(input_batching_dims). (C3) (shape(updates...)). update_scatter_dim_sizes = shape(scatter_indices) except dimension size scatter_indices corresponding index_vector_dim included. update_window_dim_sizes <= shape(inputs[0]) except dimension sizes inputs[0] corresponding inserted_window_dims input_batching_dims included. combine puts update_scatter_dim_sizes axes corresponding update_scatter_dims update_window_dim_sizes axes corresponding update_window_dims. (C5) 0 < size(inputs) = size(updates) = N. (C6) element_type(updates...) = element_type(inputs...). (C7) is_unique(update_window_dims) is_sorted(update_window_dims). (C8) 0 <= update_window_dims < rank(updates[0]). (C9) is_unique(concatenate(inserted_window_dims, input_batching_dims)) (C10) is_sorted(inserted_window_dims). (C11) 0 <= inserted_window_dims < rank(inputs[0]). (C12) is_sorted(input_batching_dims). (C13) 0 <= input_batching_dims < rank(inputs[0])). (C14) is_unique(scatter_indices_batching_dims). (C15) 0 <= scatter_indices_batching_dims < rank(scatter_indices). (C16) index_vector_dim scatter_indices_batching_dims. (C17) size(input_batching_dims) == size(scatter_indices_batching_dims). (C18) dim(inputs[0], input_batching_dims...) =         dim(scatter_indices, scatter_indices_batching_dims...). (C19) size(scatter_dims_to_operand_dims) =         index_vector_dim < rank(scatter_indices) ?         dim(scatter_indices, index_vector_dim) : 1. (C20) is_unique(concatenate(scatter_dims_to_operand_dims,         input_batching_dims)). (C21) 0 <= scatter_dims_to_operand_dims < rank(inputs[0]). (C22) 0 <= index_vector_dim <= rank(scatter_indices). (C23) update_computation type (tensor<E0>, ..., tensor<EN-1>,   tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>), is_promotable(element_type(inputs[]), Ei). (C24) shape(inputs...) = shape(results...). (C25) element_type(results[]) = Ei [0,N).","code":""},{"path":"/SPEC.html","id":"examples-83","dir":"","previous_headings":"Ops > scatter","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input: [ //          [ //           [[1, 2], [3, 4], [5, 6], [7, 8]], //           [[9, 10],[11, 12], [13, 14], [15, 16]], //           [[17, 18], [19, 20], [21, 22], [23, 24]] //          ], //          [ //           [[25, 26], [27, 28], [29, 30], [31, 32]], //           [[33, 34], [35, 36], [37, 38], [39, 40]], //           [[41, 42], [43, 44], [45, 46], [47, 48]] //          ] //         ] // %scatter_indices: [ //                    [ //                     [[0, 0], [1, 0], [2, 1]], //                     [[0, 1], [1, 1], [0, 9]] //                    ], //                    [ //                     [[0, 0], [2, 1], [2, 2]], //                     [[1, 2], [0, 1], [1, 0]] //                    ] //                   ] // %update: [ //           [ //            [[1, 1], [1, 1], [1, 1]], //            [[1, 1], [1, 1], [1, 1]] //           ], //           [ //            [[1, 1], [1, 1], [1, 1]], //            [[1, 1], [1, 1], [1, 1]] //           ] //          ] %result = \"stablehlo.scatter\"(%input, %scatter_indices, %update) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>     \"stablehlo.return\"(%0) : (tensor<i64>) -> () }) {   scatter_dimension_numbers = #stablehlo.scatter<     update_window_dims = [3, 4],     inserted_window_dims = [1],     input_batching_dims = [0],     scatter_indices_batching_dims = [1],     scatter_dims_to_operand_dims = [2, 1],     index_vector_dim = 3>,   indices_are_sorted = false,   unique_indices = false } : (tensor<2x3x4x2xi64>, tensor<2x2x3x2xi64>, tensor<2x2x3x2x2xi64>) -> tensor<2x3x4x2xi64> // %result: [ //           [ //            [[3, 4], [6, 7], [6, 7], [7, 8]], //            [[9, 10],[11, 12], [15, 16], [17, 18]], //            [[17, 18], [19, 20], [22, 23], [24, 25]] //           ], //           [ //            [[25, 26], [28, 29], [30, 31], [31, 32]], //            [[35, 36], [38, 39], [38, 39], [39, 40]], //            [[41, 42], [44, 45], [46, 47], [47, 48]] //           ] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-84","dir":"","previous_headings":"Ops > select","what":"Semantics","title":"StableHLO Specification","text":"Produces result tensor element selected on_true on_false tensor based value corresponding element pred. formally, result[result_index] = pred_element ? on_true[result_index] : on_false[result_index], pred_element = rank(pred) = 0 ? pred[] : pred[result_index]. quantized types, performs dequantize_select_quantize(pred, on_true, on_false, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-79","dir":"","previous_headings":"Ops > select","what":"Constraints","title":"StableHLO Specification","text":"(C1) rank(pred) = 0 shape(pred) = shape(on_true). (C2) baseline_type(on_true) = baseline_type(on_false) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-84","dir":"","previous_headings":"Ops > select","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %pred: [[false, true], [true, false]] // %on_true: [[1, 2], [3, 4]] // %on_false: [[5, 6], [7, 8]] %result = \"stablehlo.select\"(%pred, %on_true, %on_false) : (tensor<2x2xi1>, tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 2], [3, 8]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-85","dir":"","previous_headings":"Ops > select_and_scatter","what":"Semantics","title":"StableHLO Specification","text":"Scatters values source tensor using scatter based outcome reduce_window input tensor using select produces result tensor. following diagram shows elements result computed operand source using concrete example. formally: inputs = [operand]. window_dimensions, window_strides, padding used . base_dilations = windows_dilations = 1. body defined : source_values = [source[source_index] source_index  source_indices]. selected_index(source_index) = operand_index selected_values[source_index] operand element operand_index. source_indices = [source_index source_index  indices(source) selected_index(source_index) = result_index].","code":"def body(arg0: tensor<E>, arg1: tensor<E>) -> tensor<E>:   return select(arg0, arg1) ? arg0 : arg1;"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-80","dir":"","previous_headings":"Ops > select_and_scatter","what":"Constraints","title":"StableHLO Specification","text":"(C1) element_type(operand) = element_type(source). padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1]. is_empty_window = padded_operand_shape = 0 || window_dimensions > padded_operand_shape. num_windows = is_empty_window ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1. (C3) element_type(init_value) = element_type(operand). (C4) size(window_dimensions) = rank(operand). (C5) 0 < window_dimensions. (C6) size(window_strides) = rank(operand). (C7) 0 < window_strides. (C8) shape(padding) = [rank(operand), 2]. (C9) select type (tensor<E>, tensor<E>) -> tensor<i1> E = element_type(operand). (C10) scatter type (tensor<E>, tensor<E>) -> tensor<E> is_promotable(element_type(operand), E). (C11) shape(operand) = shape(result). (C12) element_type(result) = E.","code":""},{"path":"/SPEC.html","id":"examples-85","dir":"","previous_headings":"Ops > select_and_scatter","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[1, 5], [2, 5], [3, 6], [4, 4]] // %source: [[5, 6], [7, 8]] // %init_value: 0 %result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = \"stablehlo.compare\"(%arg0, %arg1) {       comparison_direction = #stablehlo<comparison_direction GE>     } : (tensor<i64>, tensor<i64>) -> tensor<i1>     \"stablehlo.return\"(%0) : (tensor<i1>) -> () }, {   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>     \"stablehlo.return\"(%0) : (tensor<i64>) -> () }) {   window_dimensions = array<i64: 3, 1>,   window_strides = array<i64: 2, 1>,   padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<4x2xi64>, tensor<2x2xi64>, tensor<i64>) -> tensor<4x2xi64> // %result: [[0, 0], [0, 0], [5, 14], [7, 0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-86","dir":"","previous_headings":"Ops > send","what":"Semantics","title":"StableHLO Specification","text":"Sends inputs channel channel_id. Inputs sent devices order specified source_target_pairs. operation produces result token. is_host_transfer true, operation transfers data host. Otherwise, transfers data another device based values source_target_pairs. flag duplicates information provided channel_type, future planning keep one (#666). is_host_transfer = false source_target_pairs None empty, considered undefined behavior.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-81","dir":"","previous_headings":"Ops > send","what":"Constraints","title":"StableHLO Specification","text":"(C1) dim(source_target_pairs, 1) = 2. (C2) is_unique(source_target_pairs[:, 0]). (C3) is_unique(source_target_pairs[:, 1]). num_replicas cross_replica used. num_partitions cross_partition used. DEVICE_TO_HOST is_host_transfer = true, DEVICE_TO_DEVICE otherwise.","code":""},{"path":"/SPEC.html","id":"examples-86","dir":"","previous_headings":"Ops > send","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"%result = \"stablehlo.send\"(%operand, %token) {   channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,   is_host_transfer = false,   source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64> } : (tensor<2x2xi64>, !stablehlo.token) -> !stablehlo.token"},{"path":[]},{"path":"/SPEC.html","id":"semantics-87","dir":"","previous_headings":"Ops > shift_left","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise left-shift operation lhs tensor rhs number bits produces result tensor.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-82","dir":"","previous_headings":"Ops > shift_left","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-87","dir":"","previous_headings":"Ops > shift_left","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [-1, 0, 1] // %rhs: [1, 2, 3] %result = \"stablehlo.shift_left\"(%lhs, %rhs): (tensor<3xi64>, tensor<3xi64>) -> tensor<3xi64> // %result: [-2, 0, 8]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-88","dir":"","previous_headings":"Ops > shift_right_arithmetic","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise arithmetic right-shift operation lhs tensor rhs number bits produces result tensor.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-83","dir":"","previous_headings":"Ops > shift_right_arithmetic","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-88","dir":"","previous_headings":"Ops > shift_right_arithmetic","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [-1, 0, 8] // %rhs: [1, 2, 3] %result = \"stablehlo.shift_right_arithmetic\"(%lhs, %rhs): (tensor<3xi64>, tensor<3xi64>) -> tensor<3xi64> // %result: [-1, 0, 1]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-89","dir":"","previous_headings":"Ops > shift_right_logical","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise logical right-shift operation lhs tensor rhs number bits produces result tensor.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-84","dir":"","previous_headings":"Ops > shift_right_logical","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-89","dir":"","previous_headings":"Ops > shift_right_logical","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [-1, 0, 8] // %rhs: [1, 2, 3] %result = \"stablehlo.shift_right_logical\"(%lhs, %rhs): (tensor<3xi64>, tensor<3xi64>) -> tensor<3xi64> // %result: [9223372036854775807, 0, 1]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-90","dir":"","previous_headings":"Ops > sign","what":"Semantics","title":"StableHLO Specification","text":"Returns sign operand element-wise produces result tensor. formally, element x, semantics can expressed using Python syntax follows: quantized types, performs dequantize_op_quantize(sign, operand, type(result)).","code":"def sign(x):   if is_integer(x):     if compare(x, 0, LT, SIGNED): return -1     if compare(x, 0, EQ, SIGNED): return 0     return 1   elif is_float(x):     if is_nan(x): return NaN     if compare(x, -0.0, EQ, FLOAT): return -0.0     if compare(x, +0.0, EQ, FLOAT): return +0.0     if compare(x, 0.0, LT, FLOAT): return -1.0     return 1.0   elif is_complex(x):     if is_nan(real(x)) or is_nan(imag(x)): return (NaN, NaN)     if compare(x, (0.0, 0.0), EQ, FLOAT): return (0.0, 0.0)     return divide(x, convert(abs(x), type(x)))"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-85","dir":"","previous_headings":"Ops > sign","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-90","dir":"","previous_headings":"Ops > sign","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Logical values: +NaN, -1.0, -0.0, +0.0, 1.0 // operand: [0x7FFFFFFFFFFFFFFF, -1.0, -0.0, 0.0, 1.0] %result = \"stablehlo.sign\"(%operand) : (tensor<5xf64>) -> tensor<5xf64> // Logical values: +NaN, -1.0, -0.0, +0.0, 1.0 // %result: [0x7FFFFFFFFFFFFFFF, -1.0, -0.0, 0.0, 1.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-91","dir":"","previous_headings":"Ops > sine","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise sine operation operand tensor produces result tensor. Depending element type, following: floats: sin IEEE-754. complex numbers: complex sine. quantized types: dequantize_op_quantize(sine, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-86","dir":"","previous_headings":"Ops > sine","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-91","dir":"","previous_headings":"Ops > sine","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [0.0, 1.57079632],       // [0, pi/2] //            [3.14159265, 4.71238898] // [pi, 3pi/2] //           ] %result = \"stablehlo.sine\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 1.0], [0.0, -1.0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-92","dir":"","previous_headings":"Ops > slice","what":"Semantics","title":"StableHLO Specification","text":"Extracts slice operand using statically-computed starting indices produces result tensor. start_indices contain starting indices slice dimension, limit_indices contain ending indices (exclusive) slice dimension, strides contain strides dimension. formally, result[result_index] = operand[operand_index] operand_index = start_indices + result_index * strides.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-87","dir":"","previous_headings":"Ops > slice","what":"Constraints","title":"StableHLO Specification","text":"(C1) element_type(operand) = element_type(result). (C2) size(start_indices) = size(limit_indices) = size(strides) =   rank(operand). (C3) 0 <= start_indices <= limit_indices <= shape(operand). (C4) 0 < strides. (C5) shape(result) = ceil((limit_indices - start_indices) / strides).","code":""},{"path":"/SPEC.html","id":"examples-92","dir":"","previous_headings":"Ops > slice","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [0, 0, 0, 0], //            [0, 0, 1, 1], //            [0, 0, 1, 1] //           ] %result = \"stablehlo.slice\"(%operand) {   start_indices = array<i64: 1, 2>,   limit_indices = array<i64: 3, 4>,   strides = array<i64: 1, 1> } : (tensor<3x4xi64>) -> tensor<2x2xi64> // % result: [ //            [1, 1], //            [1, 1] //           ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-93","dir":"","previous_headings":"Ops > sort","what":"Semantics","title":"StableHLO Specification","text":"Sorts 1-dimensional slices inputs along dimension dimension together, according comparator produces results. Unlike similar inputs operations, dimension allows negative values, semantics described . future, may disallowed consistency reasons (#1377). is_stable true, sorting stable, , relative order elements considered equal comparator preserved. case single input, two elements e1 e2 considered equal comparator comparator(e1, e2) = comparator(e2, e1) = false. See formalization generalizes multiple inputs. formally, result_index index_space(results[0]): adjusted_dimension = dimension >= 0 ? dimension : rank(inputs[0]) + dimension. result_slice = [ri0, ..., :, ..., riR-1] riN individual elements result_index, : inserted adjusted_dimension. inputs_together = (inputs[0]..., ..., inputs[N-1]...). results_together[result_slice] = sort(inputs_together[result_slice], comparator_together). sort sorts 1-dimensional slice non-descending order expecting comparator_together returns true left-hand side argument less right-hand second argument.  (results[0]..., ..., results[N-1]...) = results_together.","code":"def comparator_together(lhs_together, rhs_together):   args = []   for (lhs_el, rhs_el) in zip(lhs_together, rhs_together):     args.append(lhs_el)     args.append(rhs_el)   return comparator(*args)"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-88","dir":"","previous_headings":"Ops > sort","what":"Constraints","title":"StableHLO Specification","text":"(C1) 0 < size(inputs). (C2) type(inputs...) = type(results...). (C3) (shape(inputs...) + shape(results...)). (C4) -R <= dimension < R, R = rank(inputs[0]). (C5) comparator type (tensor<E1>, tensor<E1>, ..., tensor<EN-1>, tensor<EN-1>) -> tensor<i1>, Ei = element_type(inputs[]).","code":""},{"path":"/SPEC.html","id":"examples-93","dir":"","previous_headings":"Ops > sort","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %input0 = [[1, 2, 3], [3, 2, 1]] // %input1 = [[3, 2, 1], [1, 2, 3]] %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>, %arg2: tensor<i64>, %arg3: tensor<i64>):     %predicate = \"stablehlo.compare\"(%arg0, %arg1) {       comparison_direction = #stablehlo<comparison_direction GT>     } : (tensor<i64>, tensor<i64>) -> tensor<i1>     \"stablehlo.return\"(%predicate) : (tensor<i1>) -> () }) {   dimension = 0 : i64,   is_stable = true } : (tensor<2x3xi64>, tensor<2x3xi64>) -> (tensor<2x3xi64>, tensor<2x3xi64>) // %result0 = [[3, 2, 3], [1, 2, 1]] // %result1 = [[1, 2, 1], [3, 2, 3]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-94","dir":"","previous_headings":"Ops > sqrt","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise square root operation operand tensor produces result tensor. Depending element type, following: floats: squareRoot IEEE-754. complex numbers: complex square root. quantized types: dequantize_op_quantize(sqrt, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-89","dir":"","previous_headings":"Ops > sqrt","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-94","dir":"","previous_headings":"Ops > sqrt","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [[0.0, 1.0], [4.0, 9.0]] %result = \"stablehlo.sqrt\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 1.0], [2.0, 3.0]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-95","dir":"","previous_headings":"Ops > subtract","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise subtraction two tensors lhs rhs produces result tensor. Depending element type, following: integers: integer subtraction. floats: subtraction IEEE-754. complex numbers: complex subtraction. dequantize_op_quantize(subtract, lhs, rhs, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-90","dir":"","previous_headings":"Ops > subtract","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(lhs) = baseline_type(rhs) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-95","dir":"","previous_headings":"Ops > subtract","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %lhs: [[6, 8], [10, 12]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.subtract\"(%lhs, %rhs) : (tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2xf32>) // %result: [[1, 2], [3, 4]]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-96","dir":"","previous_headings":"Ops > tan","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise tangent operation operand tensor produces result tensor. Depending element type, following: floats: tan IEEE-754. complex numbers: complex tangent. quantized types: dequantize_op_quantize(tan, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-91","dir":"","previous_headings":"Ops > tan","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-96","dir":"","previous_headings":"Ops > tan","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [0.0, 1.57079632],       // [0, pi/2] //            [3.14159265, 4.71238898] // [pi, 3pi/2] //           ] %result = \"stablehlo.tan\"(%operand) : (tensor<2x2xf64>) -> tensor<2x2xf64> // %result: [ //           [0.0, 1.63312e+16], //           [0.0, 5.44375e+15] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-97","dir":"","previous_headings":"Ops > tanh","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise hyperbolic tangent operation operand tensor produces result tensor. Depending element type, following: floats: tanh IEEE-754. complex numbers: complex hyperbolic tangent. dequantize_op_quantize(tanh, operand, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-92","dir":"","previous_headings":"Ops > tanh","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_type(operand) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-97","dir":"","previous_headings":"Ops > tanh","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [-1.0, 0.0, 1.0] %result = \"stablehlo.tanh\"(%operand) : (tensor<3xf32>) -> tensor<3xf32> // %result: [-0.76159416, 0.0, 0.76159416]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-98","dir":"","previous_headings":"Ops > transpose","what":"Semantics","title":"StableHLO Specification","text":"Permutes dimensions operand tensor using permutation produces result tensor. formally, result[result_index] = operand[operand_index] result_index[d] = operand_index[permutation[d]].","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-93","dir":"","previous_headings":"Ops > transpose","what":"Constraints","title":"StableHLO Specification","text":"element_type(operand), !is_per_axis_quantized(operand). element_type(operand) except quantization_dimension(operand) quantization_dimension(result) may differ, otherwise. (C2) permutation permutation range(rank(operand)). (C3) shape(result) = dim(operand, permutation...). (C4) is_per_axis_quantized(result), quantization_dimension(operand) =   permutation(quantization_dimension(result)).","code":""},{"path":"/SPEC.html","id":"examples-98","dir":"","previous_headings":"Ops > transpose","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %operand: [ //            [[1,2], [3,4], [5,6]], //            [[7,8], [9,10], [11,12]] //           ] %result = \"stablehlo.transpose\"(%operand) {   permutation = array<i64: 2, 1, 0> } : (tensor<2x3x2xi32>) -> tensor<2x3x2xi32> // %result: [ //           [[1,7], [3,9], [5,11]], //           [[2,8], [4,10], [6,12]] //          ]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-99","dir":"","previous_headings":"Ops > triangular_solve","what":"Semantics","title":"StableHLO Specification","text":"Solves batches systems linear equations lower upper triangular coefficient matrices. formally, given b, result[i0, ..., iR-3, :, :] solution op([i0, ..., iR-3, :, :]) * x = b[i0, ..., iR-3, :, :] left_side true x * op([i0, ..., iR-3, :, :]) = b[i0, ..., iR-3, :, :] left_side false, solving variable x op() determined transpose_a, can one following: NO_TRANSPOSE: Perform operation using -. TRANSPOSE: Perform operation transpose . ADJOINT: Perform operation conjugate transpose . Input data read lower triangle , lower true upper triangle , otherwise. Output data returned triangle; values triangle implementation-defined. unit_diagonal true, implementation can assume diagonal elements equal 1, otherwise behavior undefined. quantized types, performs dequantize_op_quantize(lambda x, y: triangular_solve(x, y, left_side, lower, unit_diagonal, transpose_a), , b, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-94","dir":"","previous_headings":"Ops > triangular_solve","what":"Constraints","title":"StableHLO Specification","text":"(C1) baseline_element_type() = baseline_element_type(b). (C2) 2 <= rank() = rank(b) = R. shape()[:-3] = shape(b)[:-3]. dim(, -2) = dim(, -1) = dim(b, left_side ? -2 : -1). (C4) baseline_type(b) = baseline_type(result).","code":""},{"path":"/SPEC.html","id":"examples-99","dir":"","previous_headings":"Ops > triangular_solve","what":"Examples","title":"StableHLO Specification","text":"","code":"// %a = [ //       [1.0, 0.0, 0.0], //       [2.0, 4.0, 0.0], //       [3.0, 5.0, 6.0] //      ] // %b = [ //       [2.0, 0.0, 0.0], //       [4.0, 8.0, 0.0], //       [6.0, 10.0, 12.0] //      ] %result = \"stablehlo.triangular_solve\"(%a, %b) {   left_side = true,   lower = true,   unit_diagonal = false,   transpose_a = #stablehlo<transpose NO_TRANSPOSE> } : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<3x3xf32> // %result: [ //           [2.0, 0.0, 0.0], //           [0.0, 2.0, 0.0], //           [0.0, 0.0, 2.0] //          ]"},{"path":"/SPEC.html","id":"tuple","dir":"","previous_headings":"Ops","what":"tuple","title":"StableHLO Specification","text":"Note: Per StableHLO v1.0 Cleanup #2283, op explored deprecation appears unused frameworks compilers. , limited compatibility guarantees (6 months).","code":""},{"path":"/SPEC.html","id":"semantics-100","dir":"","previous_headings":"Ops > tuple","what":"Semantics","title":"StableHLO Specification","text":"Produces result tuple values val.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-95","dir":"","previous_headings":"Ops > tuple","what":"Constraints","title":"StableHLO Specification","text":"(C1) result type tuple<E0, ..., EN-1> Ei = type(val[]).","code":""},{"path":"/SPEC.html","id":"examples-100","dir":"","previous_headings":"Ops > tuple","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %val0: [1.0, 2.0] // %val1: (3) %result = \"stablehlo.tuple\"(%val0, %val1) : (tensor<2xf32>, tuple<tensor<i32>>) -> tuple<tensor<2xf32>, tuple<tensor<i32>>> // %result: ([1.0, 2.0], (3))"},{"path":[]},{"path":"/SPEC.html","id":"semantics-101","dir":"","previous_headings":"Ops > uniform_dequantize","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise conversion quantized tensor operand floating-point tensor result according quantization parameters defined operand type. formally, result = dequantize(operand).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-96","dir":"","previous_headings":"Ops > uniform_dequantize","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(operand) = shape(result). (C2) element_type(result) = expressed_type(operand).","code":""},{"path":"/SPEC.html","id":"examples-101","dir":"","previous_headings":"Ops > uniform_dequantize","what":"Examples","title":"StableHLO Specification","text":"","code":"// %operand: [10, 10] %result = \"stablehlo.uniform_dequantize\"(%operand) : (tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>>) -> tensor<2xf32> // %result: [4.0, 15.0]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-102","dir":"","previous_headings":"Ops > uniform_quantize","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise conversion floating-point tensor quantized tensor operand quantized tensor result according quantization parameters defined result type. formally, result = quantize(operand, type(result)). float_result = dequantize(operand). result = quantize(float_result, type(result)).","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-97","dir":"","previous_headings":"Ops > uniform_quantize","what":"Constraints","title":"StableHLO Specification","text":"(C1) shape(operand) = shape(result). (C2) expressed_type(result) = is_float(operand) ? element_type(operand) :   expressed_type(operand).","code":""},{"path":"/SPEC.html","id":"examples-102","dir":"","previous_headings":"Ops > uniform_quantize","what":"Examples","title":"StableHLO Specification","text":"","code":"// %operand: [4.0, 15.0] %result = \"stablehlo.uniform_quantize\"(%operand) : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>> // %result: [10, 10]  // %operand: [10, 10] %result = \"stablehlo.uniform_quantize\"(%operand) : (tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>>) -> tensor<2x!quant.uniform<i8:f32:0, {0.1:-20,0.2:-30}>> // %result: [20, 45]"},{"path":[]},{"path":"/SPEC.html","id":"semantics-103","dir":"","previous_headings":"Ops > while","what":"Semantics","title":"StableHLO Specification","text":"Produces output executing body function 0 times cond function outputs true. formally, semantics can expressed using Python syntax follows: behavior infinite loop TBD (#383).","code":"internal_state = operand while cond(*internal_state):   internal_state = body(*internal_state) results = internal_state"},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-98","dir":"","previous_headings":"Ops > while","what":"Constraints","title":"StableHLO Specification","text":"(C1) cond type (T0, ..., TN-1) -> tensor<i1>, Ti = type(operand[]). (C2) body type (T0, ..., TN-1) -> (T0, ..., TN-1), Ti = type(operand[]). (C3) type(results...) = type(operand...).","code":""},{"path":"/SPEC.html","id":"examples-103","dir":"","previous_headings":"Ops > while","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// %init_i: 1 // %init_sum: 0 // %one: 1 // %ten: 10 %results0, %results1 = \"stablehlo.while\"(%init_i, %init_sum) ({   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %cond = \"stablehlo.compare\"(%arg0, %ten) {       comparison_direction = #stablehlo<comparison_direction LT>     } : (tensor<i64>, tensor<i64>) -> tensor<i1>     stablehlo.return %cond : tensor<i1>   }, {   ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):     %new_sum = stablehlo.add %arg1, %one : tensor<i64>     %new_i = stablehlo.add %arg0, %one : tensor<i64>     stablehlo.return %new_i, %new_sum : tensor<i64>, tensor<i64> }) : (tensor<i64>, tensor<i64>) -> (tensor<i64>, tensor<i64>) // %results0: 10 // %results1: 10"},{"path":[]},{"path":"/SPEC.html","id":"semantics-104","dir":"","previous_headings":"Ops > xor","what":"Semantics","title":"StableHLO Specification","text":"Performs element-wise XOR two tensors lhs rhs produces result tensor. Depending element type, following: booleans: logical XOR. integers: bitwise XOR.","code":""},{"path":[]},{"path":[]},{"path":"/SPEC.html","id":"constraints-99","dir":"","previous_headings":"Ops > xor","what":"Constraints","title":"StableHLO Specification","text":"(C1) type(lhs) = type(rhs) = type(result).","code":""},{"path":"/SPEC.html","id":"examples-104","dir":"","previous_headings":"Ops > xor","what":"Examples","title":"StableHLO Specification","text":" Examples","code":"// Bitwise operation with with integer tensors // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[4, 4], [4, 12]]  // Logical operation with with boolean tensors // %lhs: [[false, false], [true, true]] // %rhs: [[false, true], [false, true]] %result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor<2x2xi1>, tensor<2x2xi1>) -> tensor<2x2xi1> // %result: [[false, true], [true, false]]"},{"path":"/SPEC.html","id":"dialect-interop","dir":"","previous_headings":"","what":"Dialect Interop","title":"StableHLO Specification","text":"moment, StableHLO programs wild sometimes contain operations defined StableHLO.","code":""},{"path":"/SPEC.html","id":"module-function-call-and-return","dir":"","previous_headings":"Dialect Interop","what":"Module, Function, Call and Return","title":"StableHLO Specification","text":"StableHLO uses upstream MLIR operations ModuleOp, FuncOp, CallOp, ReturnOp. done better interop existing MLIR machinery, many useful passes written targeting FuncOp ModuleOp, many compilation pipelines expect ops present. Full compatibility guarantees applied ops. anything ever changes ops incompatible way (.e. removal), StableHLO equivalents added preserve compatibility.","code":""},{"path":"/SPEC.html","id":"chlo","dir":"","previous_headings":"Dialect Interop","what":"CHLO","title":"StableHLO Specification","text":"CHLO opset contains higher level operations decompose StableHLO. Currently compatibility guarantees CHLO. compatibility guarantees, chlo-legalize--stablehlo pass must used prior serialization.","code":""},{"path":"/SPEC.html","id":"shape-operations","dir":"","previous_headings":"Dialect Interop","what":"Shape Operations","title":"StableHLO Specification","text":"common use case community use certain operations core MLIR dialects dynamic StableHLO programs perform shape computations. commonly, include shape dialect ops like shape_of num_elements, tensor dialect ops like dim from_elements, builtin index type. Dynamism RFC > O2 denotes scope, however support index types included interop purposes. compatibility guarantees ops types. shape-legalize--stablehlo pass can used convert operations fully supported StableHLO ops.","code":""},{"path":"/SPEC.html","id":"deprecated-operations","dir":"","previous_headings":"","what":"Deprecated Operations","title":"StableHLO Specification","text":"several StableHLO operations inherited MHLO deprecated way StableHLO. full details removals can found StableHLO v1.0 Cleanup #2283. tracker issue deprecations #2340. operations fall categories: “HLO” category StableHLO operations - initially part StableHLO opset later deemed fit well: broadcast, create_token, cross-replica-sum, dot, einsum, torch_index_select, unary_einsum (#3). Unused ops - operations may useful point, ops either underdeveloped, pipelines using ops refactored require anymore. includes map, tuple (#598), get_tuple_element, rng, complex comparisons #560, convolution window_reversal (#1181). ops can removed easily given can expressed using existing ops (broadcast, create_token, cross-replica-sum, dot, unary_einsum) removed existing compatibilty window passes (6 months). Others still explored removal (einsum, get_tuple_element, map, rng torch_index_select, tuple, complex comparisons, window_reversal). Pending community feedback, ops either removed, added spec full support. ops futures known, guaranteed 6 months compatibility.","code":""},{"path":[]},{"path":"/SPEC.html","id":"sequential-execution","dir":"","previous_headings":"Execution","what":"Sequential execution","title":"StableHLO Specification","text":"StableHLO program executed providing input values main function computing output values. Output values function computed executing graph ops rooted corresponding return op. execution order implementation-defined long aligned dataflow, .e. ops executed uses. StableHLO, side-effecting ops consume one token produce one token (multiple tokens can multiplexed one token via after_all), execution order side effects also aligned dataflow. example, program two possible execution orders: %0 → %1 → %2 → return %1 → %0 → %2 → return. formally, StableHLO process combination : 1) StableHLO program, 2) operation statuses (executed yet, already executed), 3) intermediate values process working . process starts input values main function, progresses graph ops updating operation statuses intermediate values finishes output values. formalization TBD (#484).","code":"func.func @main() -> tensor<f64> {   %0 = stablehlo.constant dense<1.0> : tensor<f64>   %1 = stablehlo.constant dense<2.0> : tensor<f64>   %2 = stablehlo.add %0, %1 : tensor<f64>   return %2 : tensor<f64> }"},{"path":"/SPEC.html","id":"parallel-execution","dir":"","previous_headings":"Execution","what":"Parallel execution","title":"StableHLO Specification","text":"StableHLO programs can executed parallel, organized 2D process grid num_replicas num_partitions type ui32. StableHLO process grid, num_replicas * num_partitions StableHLO processes executing time. process unique process_id = (replica_id, partition_id), replica_id replica_ids = range(num_replicas) partition_id partition_ids = range(num_partitions) type ui32. size process grid known statically every program (future, planning make explicit part StableHLO programs #650), position within process grid known statically every process. process access position within process grid via replica_id partition_id ops. Within process grid, programs can (“Single Program, Multiple Data” style), can different (“Multiple Program, Multiple Data” style) something . future, planning introduce support idioms defining parallel StableHLO programs, including GSPMD (#619). Within process grid, processes mostly independent - separate operation statuses, separate input/intermediate/output values ops executed separately processes, exception small number collective ops described . Given execution ops using values process, usually unambiguous refer values names. However, describing semantics collective ops, insufficient, gives rise notation name@process_id refer value name within particular process. (perspective, unqualified name can viewed shorthand name@(replica_id(), partition_id())). execution order across processes implementation-defined, except synchronization introduced point--point communication collective ops described .","code":""},{"path":"/SPEC.html","id":"point-to-point-communication","dir":"","previous_headings":"Execution","what":"Point-to-point communication","title":"StableHLO Specification","text":"StableHLO processes can communicate StableHLO channels. channel represented positive id type si64. various ops, possible send values channels receive channels. formalization, e.g. channel ids coming , processes programs become aware kind synchronization introduced , TBD (#484).","code":""},{"path":"/SPEC.html","id":"streaming-communication","dir":"","previous_headings":"Execution","what":"Streaming communication","title":"StableHLO Specification","text":"Every StableHLO process access two streaming interfaces: Infeed can read . Outfeed can written . Unlike channels, used communicate processes therefore processes ends, infeeds outfeeds end implementation-defined. formalization, e.g. streaming communication influences execution order kind synchronization introduced , TBD (#484).","code":""},{"path":"/SPEC.html","id":"collective-ops","dir":"","previous_headings":"Execution","what":"Collective ops","title":"StableHLO Specification","text":"six collective ops StableHLO: all_gather, all_reduce, all_to_all, collective_broadcast, collective_permute, reduce_scatter. ops split processes StableHLO process grid StableHLO process groups execute joint computation within process group, independently process groups. Within process group, collective ops may introduce synchronization barrier. formalization, e.g. elaborating exactly synchronization happens, exactly processes arrive barrier, happens don’t, TBD (#484). process group involves cross-partition communication, .e. processes process group whose partition ids different, execution collective op needs channel, collective op must provide positive channel_id type si64. Cross-replica communication doesn’t need channels. computations performed collective ops specific individual ops described individual op sections . However, strategies process grid split process groups shared ops described section. formally, StableHLO supports following four strategies.","code":""},{"path":"/SPEC.html","id":"cross_replica","dir":"","previous_headings":"Execution > Collective ops","what":"cross_replica","title":"StableHLO Specification","text":"cross-replica communications happen within process group. strategy takes replica_groups - list lists replica ids - computes Cartesian product replica_groups partition_ids. replica_groups must unique elements cover replica_ids. formally, using Python syntax: example, replica_groups = [[0, 1], [2, 3]] num_partitions = 2, cross_replica produce [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(2, 0), (3, 0)], [(2, 1), (3, 1)]].","code":"def cross_replica(replica_groups: List[List[ReplicaId]]) -> List[List[ProcessId]]:   for replica_group in replica_groups:     for partition_id in partition_ids:       process_group = []       for replica_id in replica_group:         process_group.append((replica_id, partition_id))       yield process_group"},{"path":"/SPEC.html","id":"cross_partition","dir":"","previous_headings":"Execution > Collective ops","what":"cross_partition","title":"StableHLO Specification","text":"cross-partition communications happen within process group. strategy takes partition_groups - list lists partition ids - computes Cartesian product partition_groups replica_ids. partition_groups must unique elements cover partition_ids. formally, using Python syntax: example, partition_groups = [[0, 1]] num_replicas = 4, cross_partition produce [[(0, 0), (0, 1)], [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)]].","code":"def cross_partition(partition_groups: List[List[PartitionId]]) -> List[List[ProcessId]]:   for partition_group in partition_groups:     for replica_id in replica_ids:       process_group = []       for partition_id in partition_group:         process_group.append((replica_id, partition_id))       yield process_group"},{"path":"/SPEC.html","id":"cross_replica_and_partition","dir":"","previous_headings":"Execution > Collective ops","what":"cross_replica_and_partition","title":"StableHLO Specification","text":"cross-replica cross-partition communications may happen within process group. strategy takes replica_groups - list lists replica ids - computes Cartesian products replica_group partition_ids. replica_groups must unique elements cover replica_ids. formally, using Python syntax: example, replica_groups = [[0, 1], [2, 3]] num_partitions = 2, cross_replica_and_partition produce [[(0, 0), (1, 0), (0, 1), (1, 1)], [(2, 0), (3, 0), (2, 1), (3, 1)]].","code":"def cross_replica_and_partition(replica_groups: List[List[ReplicaId]]) -> List[List[ProcessId]]:   for replica_group in replica_groups:     process_group = []     for partition_id in partition_ids:       for replica_id in replica_group:         process_group.append((replica_id, partition_id))     yield process_group"},{"path":"/SPEC.html","id":"flattened_ids","dir":"","previous_headings":"Execution > Collective ops","what":"flattened_ids","title":"StableHLO Specification","text":"strategy takes flattened_id_groups - list lists “flattened” process ids form replica_id * num_partitions + partition_id - turns process ids. flattened_id_groups must unique elements cover process_ids. formally, using Python syntax: example, flattened_id_groups = [[0, 1, 2, 3], [4, 5, 6, 7]], num_replicas = 4 num_partitions = 2, flattened_ids produce [[(0, 0), (0, 1), (1, 0), (1, 1)], [(2, 0), (2, 1), (3, 0), (3, 1)]].","code":"def flattened_ids(flattened_id_groups: List[List[ui32]]) -> List[List[ProcessId]]:   for flattened_id_group in flattened_id_groups:     process_group = []     for flattened_id in flattened_id_group:       replica_id = flattened_id // num_partitions       partition_id = flattened_id % num_partitions       process_group.append((replica_id, partition_id))     yield process_group"},{"path":"/SPEC.html","id":"accuracy","dir":"","previous_headings":"Execution","what":"Accuracy","title":"StableHLO Specification","text":"moment, StableHLO provide guarantees numerical accuracy, may change future (#1156).","code":""},{"path":"/SPEC.html","id":"execution-semantics-of-quantized-operation","dir":"","previous_headings":"Execution","what":"Execution semantics of quantized operation","title":"StableHLO Specification","text":"interpretation quantized StableHLO operations may vary depending hardware requirements capabilities. instance, hardware may opt interpret quantized operations using “dequantize, perform floating-point operation, finally quantize” strategy. Others may perform entire computation integer arithmetic. Consequently, interpretation quantized StableHLO operations exclusively determined specific implementation. interpretation hybrid quantization (#1575) based ’s semantics prescribed specification (via 1792).","code":""},{"path":"/SPEC.html","id":"errors","dir":"","previous_headings":"Execution","what":"Errors","title":"StableHLO Specification","text":"StableHLO programs validated extensive set constraints individual ops, rules many classes errors prior run time. However, error conditions still possible, e.g. integer overflows, --bounds accesses, etc. Unless explicitly called , errors result implementation-defined behavior, may change future (#1157).","code":""},{"path":"/SPEC.html","id":"floating-point-exceptions","dir":"","previous_headings":"Execution > Errors","what":"Floating-point exceptions","title":"StableHLO Specification","text":"exception rule, floating-point exceptions StableHLO programs well-defined behavior. Operations result exceptions defined IEEE-754 standard (invalid operation, division--zero, overflow, underflow, inexact exceptions) produce default results (defined standard) continue execution without raising corresponding status flag; similar raiseNoFlag exception handling standard. Exceptions nonstandard operations (e.g. complex arithmetic certain transcendental functions) implementation-defined.","code":""},{"path":"/SPEC.html","id":"shape-mismatches","dir":"","previous_headings":"Execution > Errors","what":"Shape mismatches","title":"StableHLO Specification","text":"StableHLO supports dynamically-shaped tensors. However, shapes agree runtime, otherwise behavior undefined. StableHLO explicitly provide op can assert tensor given shape runtime. Generating correct code responsibility producer. specific example, program valid. However, runtime, exact shapes %arg0 %arg1 , otherwise behavior program undefined:","code":"func.func @foo(%arg0: tensor<?xi32>, %arg1: tensor<?xi32>) -> tensor<?xi32> {     %0 = stablehlo.add %arg0, %arg1 : tensor<?xi32>     return %0 : tensor<?xi32> }"},{"path":"/SPEC.html","id":"notation","dir":"","previous_headings":"","what":"Notation","title":"StableHLO Specification","text":"describing syntax, document using modified ISO flavor EBNF syntax (ISO/IEC 14977:1996, Wikipedia), two modifications: 1) rules defined using ::= rather =, 2) concatenation expressed using juxtaposition rather ,. describing semantics (.e. within “Types”, “Constants” “Ops” sections), using formulas based Python syntax extended support concisely expressing array operations described . works well small snippets code, rare cases larger snippets code needed, use vanilla Python syntax always introduced explicitly.","code":""},{"path":"/SPEC.html","id":"formulas","dir":"","previous_headings":"Notation","what":"Formulas","title":"StableHLO Specification","text":"Let’s explore formulas work based example dot_general specification. One constraints operation looks follows: dim(lhs, lhs_batching_dimensions...) = dim(rhs, rhs_batching_dimensions...). names used formula come two sources: 1) global functions, .e. dim, 2) member definitions corresponding program element, .e. lhs, lhs_batching_dimensions, rhs rhs_batching_dimensions inputs defined “Inputs” section dot_general. mentioned , syntax formula Python-based conciseness-oriented extensions. make sense formula, let’s transform vanilla Python syntax. formulas, using = represent equality, first step towards obtaining Python syntax replacing = ==, follows: dim(lhs, lhs_batching_dimensions...) == dim(rhs, rhs_batching_dimensions...). Also, formulas support ellipses (...) turn scalar expressions tensor expressions. nutshell, f(xs...) roughly means “scalar x tensor xs, compute scalar f(x) return scalar results together tensor result”. vanilla Python syntax, example formula turns : [dim(lhs, dim1) dim1 lhs_batching_dimensions] == [dim(rhs, dim2) dim2 rhs_batching_dimensions]. Thanks ellipses, often possible avoid working level individual scalars. However, tricky cases, lower-level semi-informal syntax may used like start_indices[bi0, ..., :, ..., biN] formula gather specification. service conciseness, don’t provide exact formalism translating syntax vanilla Python, hopes still intuitively understandable case--case basis. Please let us know specific formulas look opaque, ’ll try improve . Also, notice formulas use ellipses expand sorts lists, including tensors, lists tensors (e.g. can arise variadic number tensors), etc. another area don’t provide exact formalism (e.g. lists even part StableHLO type system) instead rely intuitive understandability. final noteworthy notational vehicle employ implicit broadcasting. StableHLO opset doesn’t support implicit broadcasting, formulas , also service conciseness. nutshell, scalar used context tensor expected, scalar broadcasted expected shape. continue dot_general example, ’s another constraint: 0 <= lhs_batching_dimensions < rank(lhs). defined dot_general specification, lhs_batching_dimensions tensor, however 0 rank(lhs) scalars. apply implicit broadcasting, formula become [0, ..., 0] <= lhs_batching_dimensions < [rank(lhs), ..., rank(lhs)]. applied particular dot_general operation, formula evaluate tensor booleans. formulas used constraints, constraint holds formula evaluates either true tensor true elements.","code":""},{"path":"/SPEC.html","id":"names","dir":"","previous_headings":"Notation","what":"Names","title":"StableHLO Specification","text":"formulas, lexical scope includes: 1) global functions, 2) member definitions, 3) local definitions. list global functions provided . list element definitions depends program element notation applied : operations, member definitions include names introduced “Inputs” “Outputs” sections. everything else, member definitions include structural parts program element, named corresponding EBNF non-terminals. time, names structural parts obtained converting names non-terminals snake case (e.g. IntegerLiteral => integer_literal), sometimes names get abbreviated process (e.g. QuantizationStorageType => storage_type) case names introduced explicitly similarly “Inputs” / “Outputs” sections operation specifications. Additionally, member definitions always include self refer corresponding program element.","code":""},{"path":"/SPEC.html","id":"values","dir":"","previous_headings":"Notation","what":"Values","title":"StableHLO Specification","text":"formulas evaluated, work following types values: 1) Value (actual values, e.g. dense<[[1, 2], [3, 4]]> : tensor<2x2xi32>; always know types), 2) Placeholder (future values, e.g. lhs, rhs result; actual values known yet, types known), 3) Type (types defined “Types” section), 4) Function (global functions defined “Functions” section). Depending context, names may referring different values. specifically, “Semantics” section ops (equivalents program elements) defines runtime logic, inputs available Value. contrast, “Constraints” section ops (equivalents) defines “compile-time” logic, .e. something typically executed runtime, constant inputs available Value inputs available Placeholder. Let’s consider example transpose operation: operation, permutation constant, ’s available Value semantics constraints. contrast, operand result available Value semantics Placeholder constraints.","code":"%result = \"stablehlo.transpose\"(%operand) {   permutation = dense<[2, 1, 0]> : tensor<3xi64> } : (tensor<2x3x2xi32>) -> tensor<2x3x2xi32>"},{"path":[]},{"path":"/SPEC.html","id":"construction-of-types","dir":"","previous_headings":"Notation > Functions","what":"Construction of types","title":"StableHLO Specification","text":"functions can used construct types. Instead, directly use type syntax ’s typically concise. E.g. (tensor<E>, tensor<E>) -> (tensor<E>) rather function_type( [tensor_type([], E), tensor_type([], E)], [tensor_type([], E)]).","code":""},{"path":"/SPEC.html","id":"functions-on-types","dir":"","previous_headings":"Notation > Functions","what":"Functions on types","title":"StableHLO Specification","text":"element_type defined tensor types quantized tensor types returns, respectively, TensorElementType QuantizedTensorElementType part corresponding TensorType QuantizedTensorType. is_per_axis_quantized(x: Value | Placeholder | Type) -> Value shortcut is_quantized(x) quantization_dimension(x) None. is_per_tensor_quantized(x: Value | Placeholder | Type) -> Value shortcut is_quantized(x) quantization_dimension(x) None. is_promotable(x: Type, y: Type) -> bool checks type x can promoted type y. x y QuantizedTensorElementTypes, promotion applied storage_type. specific version promotion currently used context reduction computation (refer RFC details). is_quantized(x: Value | Placeholder | Type) -> Value shortcut is_quantized_tensor_element_type(x). is_type_name(x: Value | Placeholder | Type) -> Value. Available types. example, is_float(x) returns true x FloatType. x value placeholder, function shortcut is_type_name(type(x)). max_value(x: Type) -> Value returns maximum value TensorElementType. x TensorElementType, returns None. min_value(x: Type) -> Value returns minimum possible value TensorElementType. x TensorElementType, returns None. member_name(x: Value | Placeholder | Type) -> . Available member definitions member_name types. example, tensor_element_type(x) returns TensorElementType part corresponding TensorType. x value placeholder, function shortcut member_name(type(x)). x type appropriate member, value placeholder type, returns None. is_empty_algorithm(*args: Type) checks dot algorithm fields set None. needed since dot algorithms implementation defined default behaviors, specifying default value incorrect.","code":"def element_type(x: Value | Placeholder | Type):  if type(x) == TensorType:     return tensor_element_type(x)   if type(x) == QuantizedTensorType:     return quantized_tensor_element_type(x)   if type(x) is not Type:     return element_type(type(x)) def is_promotable(x: Type, y: Type) -> Value:   is_same_type = (is_bool(x) and is_bool(y)) or     (is_integer(x) and is_integer(y)) or (is_float(x) and is_float(y)) or     (is_complex(x) and is_complex(y)) or     (is_quantized(x) and is_quantized(y) and expressed_type(x) = expressed_type(y))    if is_same_type == False:     return False    if is_integer(x) or is_float(x):     return bitwidth(x) <= bitwidth(y)    if is_complex(x):     return bitwidth(element_type(x)) <= bitwidth(element_type(y))    if is_quantized(x):     return bitwidth(storage_type(x)) <= bitwidth(storage_type(y))    return false"},{"path":"/SPEC.html","id":"construction-of-values","dir":"","previous_headings":"Notation > Functions","what":"Construction of values","title":"StableHLO Specification","text":"operation_name(*xs: Value | Type) -> Value. Available operations. example, add(lhs, rhs) takes two tensor values lhs rhs returns output evaluating add operation inputs. operations e.g. broadcast_in_dim, types outputs “load-bearing”, .e. needed evaluate operation. case, function takes types arguments.","code":""},{"path":"/SPEC.html","id":"functions-on-values","dir":"","previous_headings":"Notation > Functions","what":"Functions on values","title":"StableHLO Specification","text":"Python’s operators functions available. E.g. subscription slicing notations Python available index tensors, quantized tensors tuples. to_destination_type(x: Value, destination_type: Type) -> Value defined tensors returns converted value x based type(x) destination_type follows: early discussion merging convert, uniform_quantize uniform_dequantize operations (#1576). merge need function can use operation name convert instead. is_nan(x: Value) -> Value defined tensors returns true elements x NaN false otherwise. x tensor, returns None. is_sorted(x: Value) -> Value defined tensors returns true elements x sorted ascending order respect ascending lexicographical order indices false otherwise. x tensor, returns None. is_unique(x: Value) -> Value defined tensors returns true x doesn’t duplicate elements false otherwise. x tensor, returns None. member_name(x: Value) -> defined member definitions member_name values. example, real_part(x) returns RealPart part corresponding ComplexConstant. x value appropriate member, returns None. (x: Value) -> Value defined tensors returns true elements x equal false otherwise. tensor doesn’t elements, counts “equal ”, .e. function returns true. x tensor, returns None. split(x: Value, num_results: Value, axis: Value) -> Value defined tensors returns num_results slices x along axis axis. x tensor dim(x, axis) % num_results != 0, returns None. is_defined_in_parent_scope(x: Value) -> Value defined strings returns true x name function defined scope parent function relevant op. is_namespaced_op_name(x: Value) -> Value defined strings returns true x valid op name, respects following regular expression: [-zA-Z][-zA-Z0-9_]*([.][-zA-Z0-9_$]+)+","code":"def to_destination_type(x: Value, destination_type: Type) -> Value:   if type(x) == destination_type:     return x    if is_quantized(destination_type):     if is_quantized(type(x)):       return quantize(x, destination_type)     assert is_float(type(x))     return quantize(x, destination_type)    if is_quantized(type(x)):     assert destination_type = expressed_type(type(x))     return dequantize(type(x))    return convert(x, destination_type)"},{"path":"/SPEC.html","id":"shape-computations","dir":"","previous_headings":"Notation > Functions","what":"Shape computations","title":"StableHLO Specification","text":"axes(x: Value | Placeholder | Type) -> Value shortcut range(rank(x)). dim(x: Value | Placeholder | Type, axis: Value) -> Value shortcut shape(x)[axis]. dims(x: Value | Placeholder | Type, axes: List) -> List shortcut list(map(lambda axis: dim(x, axis), axes)). index_space(x: Value | Placeholder | Type) -> Value defined tensors returns size(x) indices corresponding TensorType sorted ascending lexicographical order, .e. [0, ..., 0], [0, ..., 1], …, shape(x) - 1. x tensor type, quantized tensor type, value placeholder one types, returns None. rank(x: Value | Placeholder | Type) -> Value shortcut size(shape(x)). shape(x: Value | Placeholder | Type) -> Value defined “Functions types” section via member_name. size(x: Value | Placeholder | Type) -> Value shortcut reduce(lambda x, y: x * y, shape(x)).","code":""},{"path":"/SPEC.html","id":"quantization-computations","dir":"","previous_headings":"Notation > Functions","what":"Quantization computations","title":"StableHLO Specification","text":"def baseline_element_type(x: Value | Placeholder | Type) -> Type shortcut element_type(baseline_type(x)). baseline_type defined tensor types quantized tensor types transforms “baseline”, .e. type shape quantization parameters element type reset default values. used handy trick compare tensor quantized tensor types uniformly, needed quite often. quantized types, enables comparing types ignoring quantization parameters, , shape, storage_type, expressed_type, storage_min, storage_max, quantization_dimension (per-axis quantized type) must match, scales zero points may differ. dequantize defined quantized tensor types turns floating-point tensor types. happens via converting quantized elements represent integer values storage type corresponding floating-point values expressed type using zero point scale associated quantized element type. quantize defined floating-point tensor types turns quantized tensor types. happens via converting floating-point values expressed type corresponding integer values storage type using zero point scale associated quantized element type. dequantize_op_quantize used specify element-wise computations quantized tensors. dequantizes, .e. turns quantized elements expressed types, performs operation, quantizes, .e. turns results back storage types. moment, function works per-tensor quantization. Per-axis quantization work progress (#1574). hybrid_dequantize_then_op used specify weight-quantization hybrid op accepts lhs floating-point rhs quantized types. dequantizes quantized inputs expressed types performs computation float. Element type float lhs tensor expressed type quantized rhs tensor identical.","code":"def baseline_type(x: Value | Placeholder | Type) -> Type:   if type(x) == TensorType:     return x   if type(x) == QuantizedTensorType:     element_type = quantized_tensor_element_type(x)     baseline_element_type = QuantizedTensorElementType(       storage_type = storage_type(element_type),       storage_min = storage_min(element_type),       storage_max = storage_max(element_type),       expressed_type = expressed_type(element_type),       quantization_dimension = quantization_dimension(element_type),       scales = [constant(1.0, expressed_type(element_type))] * dim(x, quantization_dimension(element_type)),       zero_points = [constant(0, storage_type(element_type))] * dim(x, quantization_dimension(element_type)))     return QuantizedTensorType(shape(x), baseline_element_type)   if type(x) is not Type:     return baseline_element_type(type(x)) def compute_zero_points(quantized_type, result_type):   if is_per_tensor_quantized(quantized_type):     return broadcast_in_dim(constant(zero_point(quantized_type), storage_type(quantized_type)), [], result_type)   if is_per_axis_quantized(quantized_type):     for i in index_space(result_type):       d = quantization_dimension(quantized_type)       zero_points[i] = zero_points(quantized_type)[i[d]]     return zero_points  def compute_scales(quantized_type, result_type):   if is_per_tensor_quantized(quantized_type):     return broadcast_in_dim(constant(scale(quantized_type), expressed_type(quantized_type)), [],             type(result_type))   if is_per_axis_quantized(quantized_type):     for i in index_space(result_type):       d = quantization_dimension(quantized_type)       scales[i] = scales(quantized_type)[i[d]]     return scales  def dequantize(x: Value) -> Value:   assert is_quantized(x)   x_storage = bitcast_convert(x, storage_type(x))   x_storage_sub = x_storage - compute_zero_points(type(x), type(x_storage))   x_expressed_sub = convert(x_storage_sub, expressed_type(x))   return x_expressed_sub * compute_scales(type(x), type(x_expressed_sub)) def quantize(x: Value, result_type: Type) -> Value:   assert is_float(x) and is_quantized(result_type)   zero_points = compute_zero_points(result_type, TensorType(shape(x), storage_type(result_type)))   converted_zero_points = convert(zero_points, expressed_type(result_type))   converted_min = convert(storage_min(result_type), expressed_type(result_type))   converted_max = convert(storage_max(result_type), expressed_type(result_type))    x_scaled = x / compute_scales(result_type, type(x))   x_scaled_add_zp = x_scaled + converted_zero_points   x_clamped = clamp(converted_min, x_scaled_add_zp, converted_max)   x_rounded = round_nearest_even(x_clamped)   return convert(x_rounded, result_type) def dequantize_op_quantize(op, *inputs_and_output_type):   inputs = inputs_and_output_type[:-1]   output_type = inputs_and_output_type[-1]    float_inputs = map(dequantize, inputs)   float_result = op(*float_inputs)   return quantize(float_result, output_type)  def dequantize_batch_norm_grad_or_training_quantize(op, *inputs_and_output_types):   inputs = inputs_and_output_type[:-3]   float_inputs = map(dequantize, inputs)   float_results = op(*float_inputs)   return map(quantize, float_results, inputs_and_output_type[-3:])  def dequantize_compare(lhs, rhs, comparison_direction):   float_lhs = dequantize(lhs)   float_rhs = dequantize(rhs)   return compare(float_lhs, float_rhs, comparison_direction, FLOAT)  def dequantize_select_quantize(pred, on_true, on_false, output_type):   float_on_true = dequantize(on_true)   float_on_false = dequantize(on_false)   float_result = select(pred, float_on_true, float_on_false)   return quantize(float_result, output_type) def hybrid_dequantize_then_op(op, lhs, rhs):   assert(is_float(lhs) and is_quantized(rhs) and element_type(lhs) == expressed_type(rhs))   return op(lhs, dequantize(rhs))"},{"path":"/SPEC.html","id":"grid-computations","dir":"","previous_headings":"Notation > Functions","what":"Grid computations","title":"StableHLO Specification","text":"cross_partition(replica_groups: Value) -> Value. See “cross_replica” section . cross_replica(replica_groups: Value) -> Value. See “cross_replica” section . cross_replica_and_partition(replica_groups: Value) -> Value. See “cross_replica_and_partition” section . flattened_ids(replica_groups: Value) -> Value. See “flattened_ids” section .","code":""},{"path":"/SPEC.html","id":"dynamism","dir":"","previous_headings":"","what":"Dynamism","title":"StableHLO Specification","text":"StableHLO values can dynamic dimension sizes, e.g. tensor<?xi64>. However, StableHLO values dynamic number dimensions (unranked dynamism, e.g. tensor<*xi64>). Operands results allowed use dynamic dimension sizes, even constraints sizes. Constraints verified statically possible, otherwise deferred runtime mismatches result undefined behavior. See examples.","code":""},{"path":"/SPEC.html","id":"shape-mismatches-for-unary-elementwise-operations","dir":"","previous_headings":"Dynamism","what":"Shape mismatches for unary elementwise operations","title":"StableHLO Specification","text":"Consider following toy program: program unusual, common know shape result shape input. Nonetheless, valid StableHLO program. possible statically validate abs operation program, exact shape operand unknown. However, shapes certainly compatible, can checked statically: ? turn 2 runtime, issue. However, ? also turn integer, case behavior undefined. Note dimension size dynamic result, undefined behavior. Indeed, “expected” size, mismatch.","code":"func.func @foo(%arg0: tensor<?xf64>) {   %0 = stablehlo.abs %arg0 : (tensor<?xf64>) -> tensor<2xf64>   return }"},{"path":"/SPEC.html","id":"shape-mismatches-for-binary-elementwise-operations","dir":"","previous_headings":"Dynamism","what":"Shape mismatches for binary elementwise operations","title":"StableHLO Specification","text":"Consider following toy program: comes binary elementwise operations, shapes inputs result must agree runtime. compile time, static dimensions must equal, otherwise merely need compatible. dimension dynamic inputs, undefined behavior runtime, dynamic size may match corresponding size operand (static dynamic). inputs static, whether result dynamic matter: statically known dimensions checked statically, dynamic dimensions impose constraints.","code":"func.func @foo(%arg0: tensor<?xf64>, %arg1: tensor<?xf64>) {   %0 = stablehlo.add %arg0, %arg0 : (tensor<?xf64>, tensor<?xf64>) -> tensor<?xf64>   return }"},{"path":"/SPEC.html","id":"shape-mismatches-for-ops-that-take-their-output-shape-as-an-operand","dir":"","previous_headings":"Dynamism","what":"Shape mismatches for ops that take their output shape as an operand","title":"StableHLO Specification","text":"Consider following toy program: values shape operand runtime must match shape result, otherwise behavior undefined. , runtime %arg0 must value dense<[3, 4]> : tensor<2xi32>. shape operand constant, can verified statically. result shape fully dynamic, mismatch.","code":"func.func @foo(%arg0: tensor<2xi32>) {   %0 = stablehlo.dynamic_iota %arg0, dim = 0 : (tensor<2xi32>) -> tensor<3x4xi64>   return }"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Falbel. Author, maintainer. Sebastian Fischer. Author. Nikolai German. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Falbel D, Fischer S, German N (2025). stablehlo: Create stableHLO programs. R package version 0.0.0.9000, https://r-xla.github.io/stablehlo/.","code":"@Manual{,   title = {stablehlo: Create stableHLO programs},   author = {Daniel Falbel and Sebastian Fischer and Nikolai German},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://r-xla.github.io/stablehlo/}, }"},{"path":"/index.html","id":"stablehlo","dir":"","previous_headings":"","what":"Create stableHLO programs","title":"Create stableHLO programs","text":"{stablehlo} R package provides functional API create stableHLO programs. programs can executed using R package pjrt.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Create stableHLO programs","text":"","code":"pak::pak(\"r-xla/stablehlo\")"},{"path":"/index.html","id":"quickstart","dir":"","previous_headings":"","what":"Quickstart","title":"Create stableHLO programs","text":", create function takes two input arguments x y type f32 shape (2, 2) adds . Passing func hlo_input() optional, automatically use last function created hlo_func().","code":"library(stablehlo) func <- hlo_func(\"myfn\") func #> func.func @myfn () ->  { #>  #> } x <- hlo_input(\"x\", \"f32\", shape = c(2, 2), func = func) x #> Variable %x in: #> func.func @myfn (%x: tensor<2x2xf32>) ->  { #>  #> } y <- hlo_input(\"y\", \"f32\", shape = c(2, 2), func = func) y #> Variable %y in: #> func.func @myfn (%x: tensor<2x2xf32>, %y: tensor<2x2xf32>) ->  { #>  #> } z <- hlo_add(x, y) z #> Variable %0 in: #> func.func @myfn (%x: tensor<2x2xf32>, %y: tensor<2x2xf32>) ->  { #> %0 = \"stablehlo.add\" (%x, %y): (tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2xf32>) #> } f <- hlo_return(z) identical(f, func) #> [1] TRUE f #> func.func @myfn (%x: tensor<2x2xf32>, %y: tensor<2x2xf32>) -> tensor<2x2xf32> { #> %0 = \"stablehlo.add\" (%x, %y): (tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2xf32>) #> \"func.return\"(%0): (tensor<2x2xf32>) -> () #> }"},{"path":"/index.html","id":"restrictions","dir":"","previous_headings":"","what":"Restrictions","title":"Create stableHLO programs","text":"R package considered partial implementation stableHLO specification. least initially, : support subset available operations, see issue overview. support datatypes, e.g. quantized types complex numbers supported.","code":""},{"path":"/reference/BooleanType.html","id":null,"dir":"Reference","previous_headings":"","what":"BooleanType — BooleanType","title":"BooleanType — BooleanType","text":"Represents boolean type.","code":""},{"path":"/reference/BooleanType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BooleanType — BooleanType","text":"","code":"BooleanType()"},{"path":"/reference/BooleanType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BooleanType — BooleanType","text":"BooleanType","code":""},{"path":"/reference/FloatType.html","id":null,"dir":"Reference","previous_headings":"","what":"FloatType — FloatType","title":"FloatType — FloatType","text":"Represents float type.","code":""},{"path":"/reference/FloatType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FloatType — FloatType","text":"","code":"FloatType(value)"},{"path":"/reference/FloatType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"FloatType — FloatType","text":"value (character(1))","code":""},{"path":"/reference/FloatType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"FloatType — FloatType","text":"FloatType","code":""},{"path":"/reference/Func.html","id":null,"dir":"Reference","previous_headings":"","what":"Func — Func","title":"Func — Func","text":"represents function.","code":""},{"path":"/reference/Func.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Func — Func","text":"","code":"Func(   id = FuncId(),   inputs = FuncInputs(),   outputs = FuncOutputs(),   body = FuncBody() )"},{"path":"/reference/Func.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Func — Func","text":"id (FuncId id function. inputs (FuncInputs inputs function. outputs (FuncOutputs outputs function. body (FuncBody body function.","code":""},{"path":"/reference/Func.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Func — Func","text":"Func object.","code":""},{"path":"/reference/FuncId.html","id":null,"dir":"Reference","previous_headings":"","what":"FuncId — FuncId","title":"FuncId — FuncId","text":"represents id function.","code":""},{"path":"/reference/FuncId.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FuncId — FuncId","text":"","code":"FuncId(id = character(0))"},{"path":"/reference/FuncId.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"FuncId — FuncId","text":"id id function.","code":""},{"path":"/reference/FuncVariable.html","id":null,"dir":"Reference","previous_headings":"","what":"FuncVariable — FuncVariable","title":"FuncVariable — FuncVariable","text":"represents variable within function.","code":""},{"path":"/reference/FuncVariable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FuncVariable — FuncVariable","text":"","code":"FuncVariable(value_id = ValueId(), value_type = ValueType(), func = Func())"},{"path":"/reference/FuncVariable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"FuncVariable — FuncVariable","text":"value_id name variable. value_type type variable. func function variable belongs .","code":""},{"path":"/reference/IntegerType.html","id":null,"dir":"Reference","previous_headings":"","what":"IntegerType — IntegerType","title":"IntegerType — IntegerType","text":"Represents integer type.","code":""},{"path":"/reference/IntegerType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"IntegerType — IntegerType","text":"","code":"IntegerType(value)"},{"path":"/reference/IntegerType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"IntegerType — IntegerType","text":"value (character(1))","code":""},{"path":"/reference/IntegerType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"IntegerType — IntegerType","text":"IntegerType","code":""},{"path":"/reference/Shape.html","id":null,"dir":"Reference","previous_headings":"","what":"Shape — Shape","title":"Shape — Shape","text":"Represents shape tensor.","code":""},{"path":"/reference/Shape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shape — Shape","text":"","code":"Shape(dims)"},{"path":"/reference/Shape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shape — Shape","text":"dims (integer())","code":""},{"path":"/reference/Shape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shape — Shape","text":"Shape","code":""},{"path":"/reference/TensorElementType.html","id":null,"dir":"Reference","previous_headings":"","what":"TensorElementType — TensorElementType","title":"TensorElementType — TensorElementType","text":"Type union possible element types.","code":""},{"path":"/reference/TensorElementType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TensorElementType — TensorElementType","text":"","code":"TensorElementType"},{"path":"/reference/TensorElementType.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"TensorElementType — TensorElementType","text":"object class S7_union length 1.","code":""},{"path":"/reference/TensorType.html","id":null,"dir":"Reference","previous_headings":"","what":"TensorType — TensorType","title":"TensorType — TensorType","text":"Represents tensor type specific data type shape.","code":""},{"path":"/reference/TensorType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TensorType — TensorType","text":"","code":"TensorType(dtype = BooleanType(), shape = Shape())"},{"path":"/reference/TensorType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TensorType — TensorType","text":"dtype (TensorElementType) shape (Shape)","code":""},{"path":"/reference/TensorType.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"TensorType — TensorType","text":"TensorType","code":""},{"path":"/reference/ValueType.html","id":null,"dir":"Reference","previous_headings":"","what":"ValueType — ValueType","title":"ValueType — ValueType","text":"represents type value.","code":""},{"path":"/reference/ValueType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ValueType — ValueType","text":"","code":"ValueType(type, shape = NULL)"},{"path":"/reference/ValueType.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ValueType — ValueType","text":"type type value. shape shape value.","code":""},{"path":"/reference/dot-current_fn.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the last function created — .current_fn","title":"Get the last function created — .current_fn","text":"Get last function created (either via hlo_func local_func), returned yet.","code":""},{"path":"/reference/dot-current_fn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the last function created — .current_fn","text":"","code":".current_fn()"},{"path":"/reference/dot-current_fn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the last function created — .current_fn","text":"Func object.","code":""},{"path":"/reference/format_double.html","id":null,"dir":"Reference","previous_headings":"","what":"Format Double Array with Scientific Notation — format_double","title":"Format Double Array with Scientific Notation — format_double","text":"Formats double array using scientific notation 16 digits precision, similar formatC(x, digits = 16, format = \"e\"). used embed floating point constants stableHLO programs.","code":""},{"path":"/reference/format_double.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format Double Array with Scientific Notation — format_double","text":"","code":"format_double(x, precision = 64)"},{"path":"/reference/format_double.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format Double Array with Scientific Notation — format_double","text":"x (double()) Vector format. precision (integer(1)) Currently supports 32 64 bit precisions.","code":""},{"path":"/reference/format_double.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format Double Array with Scientific Notation — format_double","text":"character()","code":""},{"path":"/reference/format_double.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format Double Array with Scientific Notation — format_double","text":"","code":"format_double(1.23, 32) #> [1] \"1.23000000e+00\" format_double(1.23, 64) #> [1] \"1.2300000000000000e+00\""},{"path":"/reference/hlo_abs.html","id":null,"dir":"Reference","previous_headings":"","what":"Abs Operator — hlo_abs","title":"Abs Operator — hlo_abs","text":"See https://openxla.org/stablehlo/spec#abs details.","code":""},{"path":"/reference/hlo_abs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Abs Operator — hlo_abs","text":"","code":"hlo_abs(operand)"},{"path":"/reference/hlo_abs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abs Operator — hlo_abs","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_abs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Abs Operator — hlo_abs","text":"FuncVariable","code":""},{"path":"/reference/hlo_add.html","id":null,"dir":"Reference","previous_headings":"","what":"Add Operator — hlo_add","title":"Add Operator — hlo_add","text":"See https://openxla.org/stablehlo/spec#add details.","code":""},{"path":"/reference/hlo_add.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add Operator — hlo_add","text":"","code":"hlo_add(lhs, rhs)"},{"path":"/reference/hlo_add.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add Operator — hlo_add","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_add.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add Operator — hlo_add","text":"FuncVariable","code":""},{"path":"/reference/hlo_after_all.html","id":null,"dir":"Reference","previous_headings":"","what":"AfterAll Operator — hlo_after_all","title":"AfterAll Operator — hlo_after_all","text":"See https://openxla.org/stablehlo/spec#after_all details.","code":""},{"path":"/reference/hlo_after_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AfterAll Operator — hlo_after_all","text":"","code":"hlo_after_all(...)"},{"path":"/reference/hlo_after_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AfterAll Operator — hlo_after_all","text":"... (FuncVariable)","code":""},{"path":"/reference/hlo_after_all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AfterAll Operator — hlo_after_all","text":"FuncVariable","code":""},{"path":"/reference/hlo_and.html","id":null,"dir":"Reference","previous_headings":"","what":"And Operator — hlo_and","title":"And Operator — hlo_and","text":"See https://openxla.org/stablehlo/spec#details.","code":""},{"path":"/reference/hlo_and.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"And Operator — hlo_and","text":"","code":"hlo_and(lhs, rhs)"},{"path":"/reference/hlo_and.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"And Operator — hlo_and","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_and.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"And Operator — hlo_and","text":"FuncVariable","code":""},{"path":"/reference/hlo_atan2.html","id":null,"dir":"Reference","previous_headings":"","what":"Atan2 Operator — hlo_atan2","title":"Atan2 Operator — hlo_atan2","text":"See https://openxla.org/stablehlo/spec#atan2 details.","code":""},{"path":"/reference/hlo_atan2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Atan2 Operator — hlo_atan2","text":"","code":"hlo_atan2(lhs, rhs)"},{"path":"/reference/hlo_atan2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Atan2 Operator — hlo_atan2","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_atan2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Atan2 Operator — hlo_atan2","text":"FuncVariable","code":""},{"path":"/reference/hlo_broadcast_in_dim.html","id":null,"dir":"Reference","previous_headings":"","what":"BroadcastInDim Operator — hlo_broadcast_in_dim","title":"BroadcastInDim Operator — hlo_broadcast_in_dim","text":"See https://openxla.org/stablehlo/spec#broadcast_in_dim details.","code":""},{"path":"/reference/hlo_broadcast_in_dim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BroadcastInDim Operator — hlo_broadcast_in_dim","text":"","code":"hlo_broadcast_in_dim(operand, broadcast_dimensions, shape_out)"},{"path":"/reference/hlo_broadcast_in_dim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BroadcastInDim Operator — hlo_broadcast_in_dim","text":"operand, broadcast_dimensions, shape_out (FuncVariable)","code":""},{"path":"/reference/hlo_broadcast_in_dim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BroadcastInDim Operator — hlo_broadcast_in_dim","text":"FuncVariable","code":""},{"path":"/reference/hlo_cbrt.html","id":null,"dir":"Reference","previous_headings":"","what":"Cbrt Operator — hlo_cbrt","title":"Cbrt Operator — hlo_cbrt","text":"See https://openxla.org/stablehlo/spec#cbrt details.","code":""},{"path":"/reference/hlo_cbrt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cbrt Operator — hlo_cbrt","text":"","code":"hlo_cbrt(operand)"},{"path":"/reference/hlo_cbrt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cbrt Operator — hlo_cbrt","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_cbrt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cbrt Operator — hlo_cbrt","text":"FuncVariable","code":""},{"path":"/reference/hlo_ceil.html","id":null,"dir":"Reference","previous_headings":"","what":"Ceil Operator — hlo_ceil","title":"Ceil Operator — hlo_ceil","text":"See https://openxla.org/stablehlo/spec#ceil details.","code":""},{"path":"/reference/hlo_ceil.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ceil Operator — hlo_ceil","text":"","code":"hlo_ceil(operand)"},{"path":"/reference/hlo_ceil.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ceil Operator — hlo_ceil","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_ceil.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ceil Operator — hlo_ceil","text":"FuncVariable","code":""},{"path":"/reference/hlo_clamp.html","id":null,"dir":"Reference","previous_headings":"","what":"Clamp Operator — hlo_clamp","title":"Clamp Operator — hlo_clamp","text":"See https://openxla.org/stablehlo/spec#clamp details.","code":""},{"path":"/reference/hlo_clamp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clamp Operator — hlo_clamp","text":"","code":"hlo_clamp(Min, operand, Max)"},{"path":"/reference/hlo_clamp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clamp Operator — hlo_clamp","text":"Min, operand, Max (FuncVariable)","code":""},{"path":"/reference/hlo_clamp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clamp Operator — hlo_clamp","text":"FuncVariable","code":""},{"path":"/reference/hlo_closure.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Closure — hlo_closure","title":"Create a Closure — hlo_closure","text":"Creates new function without arguments captures provided variables.","code":""},{"path":"/reference/hlo_closure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Closure — hlo_closure","text":"","code":"hlo_closure(...)"},{"path":"/reference/hlo_closure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Closure — hlo_closure","text":"... (FuncVariable) variables capture.","code":""},{"path":"/reference/hlo_closure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Closure — hlo_closure","text":"(list() FuncVariable)","code":""},{"path":"/reference/hlo_closure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Closure — hlo_closure","text":"","code":"func <- local_func() x <- hlo_input(\"x\", \"f32\", shape = c(2, 2)) #> Error in globals[[\"CURRENT_FN\"]] %??% stop(\"No function is currently being built\"): No function is currently being built y <- hlo_input(\"y\", \"f32\", shape = c(2, 2)) #> Error in globals[[\"CURRENT_FN\"]] %??% stop(\"No function is currently being built\"): No function is currently being built f <- hlo_closure(x, y) #> Error: object 'x' not found print(f) #> Error: object 'f' not found"},{"path":"/reference/hlo_constant.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Constant — hlo_constant","title":"Create a Constant — hlo_constant","text":"Create either scalar tensor constant. Note strictly speaking, stableHLO 'scalars' simply tensors 0 dimensions.","code":""},{"path":"/reference/hlo_constant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Constant — hlo_constant","text":"","code":"hlo_scalar(value, ..., func = .current_fn())  # S3 method for class 'logical' hlo_scalar(value, ..., func = .current_fn())  # S3 method for class 'double' hlo_scalar(value, ..., func = .current_fn())  # S3 method for class 'integer' hlo_scalar(value, ..., func = .current_fn())  hlo_tensor(value, ..., func = .current_fn())  # S3 method for class 'array' hlo_tensor(value, ..., func = .current_fn())  # S3 method for class 'integer' hlo_tensor(value, ..., func = .current_fn())  # S3 method for class 'logical' hlo_tensor(value, ..., func = .current_fn())  # S3 method for class 'double' hlo_tensor(value, ..., func = .current_fn())  # S3 method for class 'PJRTBuffer' hlo_tensor(value, ..., func = .current_fn())"},{"path":"/reference/hlo_constant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Constant — hlo_constant","text":"value () Value create constant. ... () Additional arguments including: dtype (character(1)): String element type. Can one f64, f32, u8, u16, u32, u64, i8, i16, i32, i64, pred. shape (integer()): Shape tensor (hlo_tensor ). specified, shape inferred data. func (Func) function add constant . Per default, uses last function created hlo_func local_func.","code":""},{"path":"/reference/hlo_constant.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Constant — hlo_constant","text":"","code":"hlo_scalar(1L, dtype = \"i32\") #> Error in globals[[\"CURRENT_FN\"]] %??% stop(\"No function is currently being built\"): No function is currently being built hlo_scalar(1, dtype = \"f32\") #> Error in globals[[\"CURRENT_FN\"]] %??% stop(\"No function is currently being built\"): No function is currently being built hlo_scalar(TRUE) #> Error in globals[[\"CURRENT_FN\"]] %??% stop(\"No function is currently being built\"): No function is currently being built hlo_tensor(array(c(1, 2, 3, 4), dim = c(1, 4)), dtype = \"f32\") #> Error in globals[[\"CURRENT_FN\"]] %??% stop(\"No function is currently being built\"): No function is currently being built"},{"path":"/reference/hlo_cosine.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine Operator — hlo_cosine","title":"Cosine Operator — hlo_cosine","text":"See https://openxla.org/stablehlo/spec#cosine details.","code":""},{"path":"/reference/hlo_cosine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine Operator — hlo_cosine","text":"","code":"hlo_cosine(operand)"},{"path":"/reference/hlo_cosine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine Operator — hlo_cosine","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_cosine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine Operator — hlo_cosine","text":"FuncVariable","code":""},{"path":"/reference/hlo_divide.html","id":null,"dir":"Reference","previous_headings":"","what":"Divide Operator — hlo_divide","title":"Divide Operator — hlo_divide","text":"See https://openxla.org/stablehlo/spec#divide details.","code":""},{"path":"/reference/hlo_divide.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Divide Operator — hlo_divide","text":"","code":"hlo_divide(lhs, rhs)"},{"path":"/reference/hlo_divide.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Divide Operator — hlo_divide","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_divide.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Divide Operator — hlo_divide","text":"FuncVariable","code":""},{"path":"/reference/hlo_dot_general.html","id":null,"dir":"Reference","previous_headings":"","what":"DotGeneral Operator — hlo_dot_general","title":"DotGeneral Operator — hlo_dot_general","text":"See https://openxla.org/stablehlo/spec#dot_general details.","code":""},{"path":"/reference/hlo_dot_general.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DotGeneral Operator — hlo_dot_general","text":"","code":"hlo_dot_general(lhs, rhs, contracting_dims, batching_dims = NULL)"},{"path":"/reference/hlo_dot_general.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DotGeneral Operator — hlo_dot_general","text":"lhs, rhs, contracting_dims, batching_dims (FuncVariable)","code":""},{"path":"/reference/hlo_dot_general.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DotGeneral Operator — hlo_dot_general","text":"FuncVariable","code":""},{"path":"/reference/hlo_exponential.html","id":null,"dir":"Reference","previous_headings":"","what":"Exponential Operator — hlo_exponential","title":"Exponential Operator — hlo_exponential","text":"See https://openxla.org/stablehlo/spec#exponential details.","code":""},{"path":"/reference/hlo_exponential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exponential Operator — hlo_exponential","text":"","code":"hlo_exponential(operand)"},{"path":"/reference/hlo_exponential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exponential Operator — hlo_exponential","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_exponential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exponential Operator — hlo_exponential","text":"FuncVariable","code":""},{"path":"/reference/hlo_exponential_minus_one.html","id":null,"dir":"Reference","previous_headings":"","what":"ExponentialMinusOne Operator — hlo_exponential_minus_one","title":"ExponentialMinusOne Operator — hlo_exponential_minus_one","text":"See https://openxla.org/stablehlo/spec#exponential_minus_one details.","code":""},{"path":"/reference/hlo_exponential_minus_one.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ExponentialMinusOne Operator — hlo_exponential_minus_one","text":"","code":"hlo_exponential_minus_one(operand)"},{"path":"/reference/hlo_exponential_minus_one.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ExponentialMinusOne Operator — hlo_exponential_minus_one","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_exponential_minus_one.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ExponentialMinusOne Operator — hlo_exponential_minus_one","text":"FuncVariable","code":""},{"path":"/reference/hlo_floor.html","id":null,"dir":"Reference","previous_headings":"","what":"Floor Operator — hlo_floor","title":"Floor Operator — hlo_floor","text":"See https://openxla.org/stablehlo/spec#floor details.","code":""},{"path":"/reference/hlo_floor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Floor Operator — hlo_floor","text":"","code":"hlo_floor(operand)"},{"path":"/reference/hlo_floor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Floor Operator — hlo_floor","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_floor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Floor Operator — hlo_floor","text":"FuncVariable","code":""},{"path":"/reference/hlo_func.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a function — hlo_func","title":"Create a function — hlo_func","text":"Create function given id. local_func removes function exiting current scope, whereas hlo_func . calling function, created function stored global variable accessible via .current_fn. Functions receiving Func argument usually use .current_fn() default.","code":""},{"path":"/reference/hlo_func.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a function — hlo_func","text":"","code":"hlo_func(id = \"main\")  local_func(id = \"main\")"},{"path":"/reference/hlo_func.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a function — hlo_func","text":"id (character(1) id function.","code":""},{"path":"/reference/hlo_func.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a function — hlo_func","text":"Func object.","code":""},{"path":"/reference/hlo_if.html","id":null,"dir":"Reference","previous_headings":"","what":"If Operator — hlo_if","title":"If Operator — hlo_if","text":"See https://openxla.org/stablehlo/spec#details.","code":""},{"path":"/reference/hlo_if.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"If Operator — hlo_if","text":"","code":"hlo_if(pred, true_branch, false_branch)"},{"path":"/reference/hlo_if.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"If Operator — hlo_if","text":"pred, true_branch, false_branch (FuncVariable)","code":""},{"path":"/reference/hlo_if.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"If Operator — hlo_if","text":"FuncVariable","code":""},{"path":"/reference/hlo_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a input to a function — hlo_input","title":"Create a input to a function — hlo_input","text":"Create input function","code":""},{"path":"/reference/hlo_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a input to a function — hlo_input","text":"","code":"hlo_input(name, dtype, shape = integer(), func = .current_fn())"},{"path":"/reference/hlo_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a input to a function — hlo_input","text":"name (character(1)) name parameter. dtype (ValueType) element type parameter. Can contain digits, letters underscores. starts digit, can contain digits. Otherwise must start letter. shape (integer()) shape parameter. Use integer() scalars. func (Func) function id parameter. Per default, uses last function created hlo_func.","code":""},{"path":"/reference/hlo_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a input to a function — hlo_input","text":"","code":"func <- hlo_func() x <- hlo_input(\"x\", \"f32\", shape = c(2, 2)) print(x) #> Variable %x in: #> func.func @main (%x: tensor<2x2xf32>) ->  { #>  #> }  # You can combine multiple inputs as follows: c(   hlo_input(\"x\", \"f32\", shape = c(2, 2)),   hlo_input(\"y\", \"f32\", shape = c(2, 2)) ) #> [[1]] #> Variable %x in: #> func.func @main (%x: tensor<2x2xf32>, %x: tensor<2x2xf32>, %y: tensor<2x2xf32>) ->  { #>  #> } #>  #> [[2]] #> Variable %y in: #> func.func @main (%x: tensor<2x2xf32>, %x: tensor<2x2xf32>, %y: tensor<2x2xf32>) ->  { #>  #> } #>"},{"path":"/reference/hlo_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Log Operator — hlo_log","title":"Log Operator — hlo_log","text":"See https://openxla.org/stablehlo/spec#log details.","code":""},{"path":"/reference/hlo_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log Operator — hlo_log","text":"","code":"hlo_log(operand)"},{"path":"/reference/hlo_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log Operator — hlo_log","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log Operator — hlo_log","text":"FuncVariable","code":""},{"path":"/reference/hlo_log_plus_one.html","id":null,"dir":"Reference","previous_headings":"","what":"LogPlusOne Operator — hlo_log_plus_one","title":"LogPlusOne Operator — hlo_log_plus_one","text":"See https://openxla.org/stablehlo/spec#log_plus_one details.","code":""},{"path":"/reference/hlo_log_plus_one.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LogPlusOne Operator — hlo_log_plus_one","text":"","code":"hlo_log_plus_one(operand)"},{"path":"/reference/hlo_log_plus_one.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LogPlusOne Operator — hlo_log_plus_one","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_log_plus_one.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LogPlusOne Operator — hlo_log_plus_one","text":"FuncVariable","code":""},{"path":"/reference/hlo_logistic.html","id":null,"dir":"Reference","previous_headings":"","what":"Logistic Operator — hlo_logistic","title":"Logistic Operator — hlo_logistic","text":"See https://openxla.org/stablehlo/spec#logistic details.","code":""},{"path":"/reference/hlo_logistic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logistic Operator — hlo_logistic","text":"","code":"hlo_logistic(operand)"},{"path":"/reference/hlo_logistic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logistic Operator — hlo_logistic","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_logistic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logistic Operator — hlo_logistic","text":"FuncVariable","code":""},{"path":"/reference/hlo_maximum.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum Operator — hlo_maximum","title":"Maximum Operator — hlo_maximum","text":"See https://openxla.org/stablehlo/spec#maximum details.","code":""},{"path":"/reference/hlo_maximum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum Operator — hlo_maximum","text":"","code":"hlo_maximum(lhs, rhs)"},{"path":"/reference/hlo_maximum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum Operator — hlo_maximum","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_maximum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum Operator — hlo_maximum","text":"FuncVariable","code":""},{"path":"/reference/hlo_minimum.html","id":null,"dir":"Reference","previous_headings":"","what":"Minimum Operator — hlo_minimum","title":"Minimum Operator — hlo_minimum","text":"See https://openxla.org/stablehlo/spec#minimum details.","code":""},{"path":"/reference/hlo_minimum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Minimum Operator — hlo_minimum","text":"","code":"hlo_minimum(lhs, rhs)"},{"path":"/reference/hlo_minimum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minimum Operator — hlo_minimum","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_minimum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minimum Operator — hlo_minimum","text":"FuncVariable","code":""},{"path":"/reference/hlo_multiply.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiply Operator — hlo_multiply","title":"Multiply Operator — hlo_multiply","text":"See https://openxla.org/stablehlo/spec#multiply details.","code":""},{"path":"/reference/hlo_multiply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiply Operator — hlo_multiply","text":"","code":"hlo_multiply(lhs, rhs)"},{"path":"/reference/hlo_multiply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiply Operator — hlo_multiply","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_multiply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiply Operator — hlo_multiply","text":"FuncVariable","code":""},{"path":"/reference/hlo_negate.html","id":null,"dir":"Reference","previous_headings":"","what":"Negate Operator — hlo_negate","title":"Negate Operator — hlo_negate","text":"See https://openxla.org/stablehlo/spec#negate details.","code":""},{"path":"/reference/hlo_negate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Negate Operator — hlo_negate","text":"","code":"hlo_negate(operand)"},{"path":"/reference/hlo_negate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Negate Operator — hlo_negate","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_negate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Negate Operator — hlo_negate","text":"FuncVariable","code":""},{"path":"/reference/hlo_or.html","id":null,"dir":"Reference","previous_headings":"","what":"Or Operator — hlo_or","title":"Or Operator — hlo_or","text":"See https://openxla.org/stablehlo/spec#details.","code":""},{"path":"/reference/hlo_or.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Or Operator — hlo_or","text":"","code":"hlo_or(lhs, rhs)"},{"path":"/reference/hlo_or.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Or Operator — hlo_or","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_or.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Or Operator — hlo_or","text":"FuncVariable","code":""},{"path":"/reference/hlo_power.html","id":null,"dir":"Reference","previous_headings":"","what":"Power Operator — hlo_power","title":"Power Operator — hlo_power","text":"See https://openxla.org/stablehlo/spec#power details.","code":""},{"path":"/reference/hlo_power.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Power Operator — hlo_power","text":"","code":"hlo_power(lhs, rhs)"},{"path":"/reference/hlo_power.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Power Operator — hlo_power","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_power.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Power Operator — hlo_power","text":"FuncVariable","code":""},{"path":"/reference/hlo_remainder.html","id":null,"dir":"Reference","previous_headings":"","what":"Remainder Operator — hlo_remainder","title":"Remainder Operator — hlo_remainder","text":"See https://openxla.org/stablehlo/spec#remainder details.","code":""},{"path":"/reference/hlo_remainder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remainder Operator — hlo_remainder","text":"","code":"hlo_remainder(lhs, rhs)"},{"path":"/reference/hlo_remainder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remainder Operator — hlo_remainder","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_remainder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remainder Operator — hlo_remainder","text":"FuncVariable","code":""},{"path":"/reference/hlo_return.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a return operation — hlo_return","title":"Create a return operation — hlo_return","text":"Create return operation","code":""},{"path":"/reference/hlo_return.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a return operation — hlo_return","text":"","code":"hlo_return(...)"},{"path":"/reference/hlo_return.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a return operation — hlo_return","text":"... (FuncVariable)","code":""},{"path":"/reference/hlo_round_nearest_even.html","id":null,"dir":"Reference","previous_headings":"","what":"RoundNearestEven Operator — hlo_round_nearest_even","title":"RoundNearestEven Operator — hlo_round_nearest_even","text":"See https://openxla.org/stablehlo/spec#round_nearest_even details.","code":""},{"path":"/reference/hlo_round_nearest_even.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RoundNearestEven Operator — hlo_round_nearest_even","text":"","code":"hlo_round_nearest_even(operand)"},{"path":"/reference/hlo_round_nearest_even.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"RoundNearestEven Operator — hlo_round_nearest_even","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_round_nearest_even.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"RoundNearestEven Operator — hlo_round_nearest_even","text":"FuncVariable","code":""},{"path":"/reference/hlo_rsqrt.html","id":null,"dir":"Reference","previous_headings":"","what":"Rsqrt Operator — hlo_rsqrt","title":"Rsqrt Operator — hlo_rsqrt","text":"See https://openxla.org/stablehlo/spec#rsqrt details.","code":""},{"path":"/reference/hlo_rsqrt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rsqrt Operator — hlo_rsqrt","text":"","code":"hlo_rsqrt(operand)"},{"path":"/reference/hlo_rsqrt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rsqrt Operator — hlo_rsqrt","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_rsqrt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rsqrt Operator — hlo_rsqrt","text":"FuncVariable","code":""},{"path":"/reference/hlo_sign.html","id":null,"dir":"Reference","previous_headings":"","what":"Sign Operator — hlo_sign","title":"Sign Operator — hlo_sign","text":"See https://openxla.org/stablehlo/spec#sign details.","code":""},{"path":"/reference/hlo_sign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sign Operator — hlo_sign","text":"","code":"hlo_sign(operand)"},{"path":"/reference/hlo_sign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sign Operator — hlo_sign","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_sign.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sign Operator — hlo_sign","text":"FuncVariable","code":""},{"path":"/reference/hlo_sine.html","id":null,"dir":"Reference","previous_headings":"","what":"Sine Operator — hlo_sine","title":"Sine Operator — hlo_sine","text":"See https://openxla.org/stablehlo/spec#sine details.","code":""},{"path":"/reference/hlo_sine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sine Operator — hlo_sine","text":"","code":"hlo_sine(operand)"},{"path":"/reference/hlo_sine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sine Operator — hlo_sine","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_sine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sine Operator — hlo_sine","text":"FuncVariable","code":""},{"path":"/reference/hlo_sqrt.html","id":null,"dir":"Reference","previous_headings":"","what":"Sqrt Operator — hlo_sqrt","title":"Sqrt Operator — hlo_sqrt","text":"See https://openxla.org/stablehlo/spec#sqrt details.","code":""},{"path":"/reference/hlo_sqrt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sqrt Operator — hlo_sqrt","text":"","code":"hlo_sqrt(operand)"},{"path":"/reference/hlo_sqrt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sqrt Operator — hlo_sqrt","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_sqrt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sqrt Operator — hlo_sqrt","text":"FuncVariable","code":""},{"path":"/reference/hlo_subtract.html","id":null,"dir":"Reference","previous_headings":"","what":"Subtract Operator — hlo_subtract","title":"Subtract Operator — hlo_subtract","text":"See https://openxla.org/stablehlo/spec#subtract details.","code":""},{"path":"/reference/hlo_subtract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subtract Operator — hlo_subtract","text":"","code":"hlo_subtract(lhs, rhs)"},{"path":"/reference/hlo_subtract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subtract Operator — hlo_subtract","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_subtract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subtract Operator — hlo_subtract","text":"FuncVariable","code":""},{"path":"/reference/hlo_tan.html","id":null,"dir":"Reference","previous_headings":"","what":"Tan Operator — hlo_tan","title":"Tan Operator — hlo_tan","text":"See https://openxla.org/stablehlo/spec#tan details.","code":""},{"path":"/reference/hlo_tan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tan Operator — hlo_tan","text":"","code":"hlo_tan(operand)"},{"path":"/reference/hlo_tan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tan Operator — hlo_tan","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_tan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tan Operator — hlo_tan","text":"FuncVariable","code":""},{"path":"/reference/hlo_tanh.html","id":null,"dir":"Reference","previous_headings":"","what":"Tanh Operator — hlo_tanh","title":"Tanh Operator — hlo_tanh","text":"See https://openxla.org/stablehlo/spec#tanh details.","code":""},{"path":"/reference/hlo_tanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tanh Operator — hlo_tanh","text":"","code":"hlo_tanh(operand)"},{"path":"/reference/hlo_tanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tanh Operator — hlo_tanh","text":"operand (FuncVariable)","code":""},{"path":"/reference/hlo_tanh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tanh Operator — hlo_tanh","text":"FuncVariable","code":""},{"path":"/reference/hlo_xor.html","id":null,"dir":"Reference","previous_headings":"","what":"Xor Operator — hlo_xor","title":"Xor Operator — hlo_xor","text":"See https://openxla.org/stablehlo/spec#xor details.","code":""},{"path":"/reference/hlo_xor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Xor Operator — hlo_xor","text":"","code":"hlo_xor(lhs, rhs)"},{"path":"/reference/hlo_xor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Xor Operator — hlo_xor","text":"lhs, rhs (FuncVariable)","code":""},{"path":"/reference/hlo_xor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Xor Operator — hlo_xor","text":"FuncVariable","code":""},{"path":"/reference/infer_types_boolean_biv.html","id":null,"dir":"Reference","previous_headings":"","what":"Infer types for boolean binary operations — infer_types_boolean_biv","title":"Infer types for boolean binary operations — infer_types_boolean_biv","text":"Infer types boolean binary operations.","code":""},{"path":"/reference/infer_types_boolean_biv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Infer types for boolean binary operations — infer_types_boolean_biv","text":"","code":"infer_types_boolean_biv(lhs, rhs)"},{"path":"/reference/infer_types_boolean_biv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Infer types for boolean binary operations — infer_types_boolean_biv","text":"lhs (ValueType) left-hand side operand. rhs (ValueType) right-hand side operand.","code":""},{"path":"/reference/infer_types_boolean_biv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Infer types for boolean binary operations — infer_types_boolean_biv","text":"(ValueType) inferred type.","code":""},{"path":"/reference/infer_types_generic_biv.html","id":null,"dir":"Reference","previous_headings":"","what":"Infer types for binary operations — infer_types_generic_biv","title":"Infer types for binary operations — infer_types_generic_biv","text":"Infer types binary operations.","code":""},{"path":"/reference/infer_types_generic_biv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Infer types for binary operations — infer_types_generic_biv","text":"","code":"infer_types_generic_biv(lhs, rhs)"},{"path":"/reference/infer_types_generic_biv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Infer types for binary operations — infer_types_generic_biv","text":"lhs (ValueType) left-hand side operand. rhs (ValueType) right-hand side operand.","code":""},{"path":"/reference/infer_types_generic_biv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Infer types for binary operations — infer_types_generic_biv","text":"(ValueType) inferred type.","code":""},{"path":"/reference/infer_types_generic_uni.html","id":null,"dir":"Reference","previous_headings":"","what":"Infer types for unary operations — infer_types_generic_uni","title":"Infer types for unary operations — infer_types_generic_uni","text":"Infer types unary operations.","code":""},{"path":"/reference/infer_types_generic_uni.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Infer types for unary operations — infer_types_generic_uni","text":"","code":"infer_types_generic_uni(operand)"},{"path":"/reference/infer_types_generic_uni.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Infer types for unary operations — infer_types_generic_uni","text":"operand (ValueType) operand.","code":""},{"path":"/reference/infer_types_generic_uni.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Infer types for unary operations — infer_types_generic_uni","text":"(ValueType) inferred type.","code":""},{"path":"/reference/r_to_stablehlo_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Represent an R value in stableHLO — r_to_stablehlo_string","title":"Represent an R value in stableHLO — r_to_stablehlo_string","text":"Convert R value StableHLO string representation","code":""},{"path":"/reference/r_to_stablehlo_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Represent an R value in stableHLO — r_to_stablehlo_string","text":"","code":"r_to_stablehlo_string(value, dtype)"},{"path":"/reference/r_to_stablehlo_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Represent an R value in stableHLO — r_to_stablehlo_string","text":"value () R value convert dtype (character(1)) element type use.","code":""},{"path":"/reference/r_to_stablehlo_string.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Represent an R value in stableHLO — r_to_stablehlo_string","text":"character(1)","code":""},{"path":"/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. tengen dtype, shape","code":""},{"path":"/reference/repr.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate string representation for object — repr","title":"Generate string representation for object — repr","text":"function generates string representation object. package, primarily used convert Func stableHLO string representation.","code":""},{"path":"/reference/repr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate string representation for object — repr","text":"","code":"repr(x, ...)"},{"path":"/reference/repr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate string representation for object — repr","text":"x object generate string representation . ... Additional arguments passed method.","code":""},{"path":"/reference/repr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate string representation for object — repr","text":"character(1)","code":""},{"path":"/reference/repr_env2name.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert environment to name — repr_env2name","title":"Convert environment to name — repr_env2name","text":"function converts environment name. Can called within repr call.","code":""},{"path":"/reference/repr_env2name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert environment to name — repr_env2name","text":"","code":"repr_env2name(x)"},{"path":"/reference/repr_env2name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert environment to name — repr_env2name","text":"x environment convert name.","code":""},{"path":"/reference/repr_env2name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert environment to name — repr_env2name","text":"character(1)","code":""}]
